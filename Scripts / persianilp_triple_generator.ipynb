{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzeoasjo5b6YIoJ3uW4soa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install Library"
      ],
      "metadata": {
        "id": "6bBm4o4CwZYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import transformers"
      ],
      "metadata": {
        "id": "_Si4O0Zgwemr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extrac Zip File"
      ],
      "metadata": {
        "id": "z9s7YrCewy7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/FarsiBaseTriple.zip\"\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "excel_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "merged_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "1FH5oVcDw8OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Incomplete Triple"
      ],
      "metadata": {
        "id": "f55aPBUvxKRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "output_path = \"/content/FarsiBase\"\n",
        "os.makedirs(output_path, exist_ok=True)  # create output folder if it does not exist\n",
        "\n",
        "csv_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.csv')]\n",
        "print(f\"üîé Number of CSV files found: {len(csv_files)}\")\n",
        "\n",
        "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "merged_df.to_csv('/content/mergeData.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"‚úÖ All files merged successfully.\\n\")\n",
        "\n",
        "print(\"üìä Full report of merged DataFrame:\")\n",
        "print(\"=================================\")\n",
        "print(f\"‚û° Total number of rows: {len(merged_df):,}\")\n",
        "print(f\"‚û° Total number of columns: {len(merged_df.columns)}\")\n",
        "print(\"\\nüîπ Number of null values in each column:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "def simplify_uri(uri):\n",
        "    if isinstance(uri, str):\n",
        "        return uri.strip().split(\"/\")[-1].split(\"#\")[-1]  # improvement for handling different URIs\n",
        "    return uri\n",
        "\n",
        "cols_to_simplify = [\"subjectLabel\", \"predicateLabel\", \"objectLabel\"]\n",
        "simplified_df = merged_df.copy()\n",
        "for col in cols_to_simplify:\n",
        "    simplified_df[col] = simplified_df[col].apply(simplify_uri)\n",
        "\n",
        "simplified_df.drop_duplicates(inplace=True)\n",
        "simplified_df = simplified_df.dropna(how='all')\n",
        "print(f\"\\n‚ôª Number of rows after removing duplicates: {len(simplified_df):,}\")\n",
        "\n",
        "simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n",
        "\n",
        "complete_df = simplified_df[simplified_df[\"filled_count\"] == 3].drop(columns=[\"filled_count\"])\n",
        "complete_path = os.path.join(output_path, \"complete_triples.csv\")\n",
        "complete_df.to_csv(complete_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\nüíæ Complete triples file ({len(complete_df):,} rows) saved at {complete_path}\")\n",
        "\n",
        "two_filled_df = simplified_df[simplified_df[\"filled_count\"] == 2].drop(columns=[\"filled_count\"])\n",
        "two_path = os.path.join(output_path, \"triples_with_two_values.csv\")\n",
        "two_filled_df.to_csv(two_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"üíæ Triples with two values file ({len(two_filled_df):,} rows) saved at {two_path}\")\n",
        "\n",
        "one_filled_df = simplified_df[simplified_df[\"filled_count\"] == 1].drop(columns=[\"filled_count\"])\n",
        "one_path = os.path.join(output_path, \"triples_with_one_value.csv\")\n",
        "one_filled_df.to_csv(one_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"üíæ Triples with one value file ({len(one_filled_df):,} rows) saved at {one_path}\")\n",
        "\n",
        "print(\"\\nüéâ Processing completed successfully!\")\n",
        "print(f\"\\nüìù Final report:\\n{simplified_df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ID5JYfvxA_b",
        "outputId": "f43178bd-c79c-42cb-e101-37e117871105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Number of CSV files found: 231\n",
            "‚úÖ All files merged successfully.\n",
            "\n",
            "üìä Full report of merged DataFrame:\n",
            "=================================\n",
            "‚û° Total number of rows: 2,306,413\n",
            "‚û° Total number of columns: 3\n",
            "\n",
            "üîπ Number of null values in each column:\n",
            "subjectLabel      1920360\n",
            "predicateLabel     245845\n",
            "objectLabel        182278\n",
            "dtype: int64\n",
            "\n",
            "‚ôª Number of rows after removing duplicates: 251,600\n",
            "\n",
            "üíæ Complete triples file (228,927 rows) saved at /content/FarsiBase/complete_triples.csv\n",
            "üíæ Triples with two values file (21,492 rows) saved at /content/FarsiBase/triples_with_two_values.csv\n",
            "üíæ Triples with one value file (1,181 rows) saved at /content/FarsiBase/triples_with_one_value.csv\n",
            "\n",
            "üéâ Processing completed successfully!\n",
            "\n",
            "üìù Final report:\n",
            "subjectLabel      238789\n",
            "predicateLabel    243026\n",
            "objectLabel       249131\n",
            "filled_count      251600\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FarsiBase Data Cleaning"
      ],
      "metadata": {
        "id": "OlmlZZcyzWqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read Data\n",
        "df = pd.read_csv(\"/content/FarsiBase/complete_triples.csv\")\n",
        "\n",
        "def convert_persian_to_english(number):\n",
        "    persian_to_english = str.maketrans('€∞€±€≤€≥€¥€µ€∂€∑€∏€π', '0123456789')\n",
        "    return str(number).translate(persian_to_english)\n",
        "\n",
        "for column in ['subjectLabel', 'predicateLabel', 'objectLabel']:\n",
        "    df[column] = df[column].apply(convert_persian_to_english)\n",
        "\n",
        "# Clean Relation\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^dcterms#subject','ŸÖŸàÿ∂Ÿàÿπ/ŸÖÿ≠ÿ™Ÿàÿß', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^subject','ŸÖŸàÿ∂Ÿàÿπ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth place','ŸÖÿ≠ŸÑ ÿ™ŸàŸÑÿØ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth_place','ŸÖÿ≠ŸÑ ÿ™ŸàŸÑÿØ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birthPlace','ŸÖÿ≠ŸÑ ÿ™ŸàŸÑÿØ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^instanceOf','ŸÜŸÖŸàŸÜŸá‚Äåÿß€å ÿßÿ≤', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^deathPlace','ŸÖÿ≠ŸÑ ŸÖÿ±⁄Ø', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^death place','ŸÖÿ≠ŸÑ ŸÖÿ±⁄Ø', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^field','ŸÖŸàÿ∂Ÿàÿπ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^genre','⁄òÿßŸÜÿ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^nationality','ŸÖŸÑ€åÿ™', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^occupation','ÿ¥ÿ∫ŸÑ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^picture','ÿ™ÿµŸà€åÿ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^ActiveYears','ÿ≥ÿßŸÑ‚ÄåŸáÿß€å ŸÅÿπÿßŸÑ€åÿ™', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^activeYears','ÿ≥ÿßŸÑ‚ÄåŸáÿß€å ŸÅÿπÿßŸÑ€åÿ™', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^timezone1 dst','ŸÜÿßÿ≠€åŸá ÿ≤ŸÖÿßŸÜ€å €±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^confed_cup','ÿ¨ÿßŸÖ ⁄©ŸÜŸÅÿØÿ±ÿßÿ≥€åŸàŸÜ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^distance to London (Œº)','ŸÅÿßÿµŸÑŸá ÿ™ÿß ŸÑŸÜÿØŸÜ (ŸÖ€åÿßŸÜ⁄Ø€åŸÜ)', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^fs_date','ÿ™ÿßÿ±€åÿÆ ÿ≥€åÿ≥ÿ™ŸÖ ŸÅÿß€åŸÑ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^pa≈Ñstwo','⁄©ÿ¥Ÿàÿ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^pa≈Ñstwo','⁄©ÿ¥Ÿàÿ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^sp_date','ÿ™ÿßÿ±€åÿÆ ÿ∑ÿ±ÿ≠', regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "A82GYtE3zjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FarsiBase Triple Normalization (Via Translation)"
      ],
      "metadata": {
        "id": "vldGHMXzxVqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# 1. Read the dataset\n",
        "df = pd.read_csv('/content/FarsiBase/complete_triples.csv')\n",
        "columns = ['subjectLabel', 'predicateLabel', 'objectLabel']\n",
        "\n",
        "# 2. Translation function with caching\n",
        "translation_cache = {}\n",
        "def translate_word(word, target_lang=\"fa\"):\n",
        "    if word in translation_cache:\n",
        "        return translation_cache[word]\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(word)\n",
        "        translation_cache[word] = translated\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error translating {word}: {e}\")\n",
        "        fallback = f\"ÿ™ÿ±ÿ¨ŸÖŸá_{word}\"\n",
        "        translation_cache[word] = fallback\n",
        "        return fallback\n",
        "\n",
        "# 3. Extract and translate unique English words per column\n",
        "column_translations = {col: {} for col in columns}\n",
        "for col in columns:\n",
        "    all_text = ' '.join(df[col].astype(str))\n",
        "    english_words = set(re.findall(r'\\b[a-zA-Z]+\\b', all_text))\n",
        "\n",
        "    for word in english_words:\n",
        "        persian_word = translate_word(word)\n",
        "        column_translations[col][word] = persian_word\n",
        "\n",
        "# 4. Apply translation mappings to each column\n",
        "for col, translations in column_translations.items():\n",
        "    for pattern, replacement in translations.items():\n",
        "        df[col] = df[col].str.replace(rf'^{pattern}', replacement, regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"‚úÖ All translations applied to the DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csvece4sxZ6H",
        "outputId": "f85ebe90-7ea4-4683-e1d0-65172c92ff50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All translations applied to the DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Shuffling And Aggregation(FarsiBase + Deepseek)"
      ],
      "metadata": {
        "id": "f-zEue4D4IXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate FarsiBase Data And DeepSeek Data\n",
        "DeepSeek_df = pd.read_excel('/content/DeepSeek_Triple.xlsx')\n",
        "input_file = '/content/FarsiBase/complete_triples.csv'\n",
        "FarsiBase_df = pd.read_csv(input_file)\n",
        "df = pd.concat([DeepSeek_df, FarsiBase_df], axis=0)\n",
        "\n",
        "# Shuffle the combined data\n",
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "output_file = '/content/FarsiBase/shuffled_triple.csv'\n",
        "shuffled_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "print(f\"The Excel file has been randomly shuffled and saved to '{output_file}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luZy5E-i4KdL",
        "outputId": "8b962590-d458-4140-e7b1-8753a1f890b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Excel file has been randomly shuffled and saved to '/content/FarsiBase/shuffled_triple.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PersianILP Normalizing"
      ],
      "metadata": {
        "id": "n8Ro48BQ4jnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Google Translation\n",
        "def translate_relation(relation, target_lang=\"fa\"):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(relation)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"Error while translating '{relation}': {e}\")\n",
        "        return relation\n",
        "\n",
        "# Translate and normalize dataset\n",
        "def normalize_excel(input_path, output_path, use_translation=False):\n",
        "\n",
        "    df = pd.read_csv(input_path)\n",
        "    normalized_relations = []\n",
        "    for relation in df['predicateLabel']:\n",
        "        if pd.isna(relation):\n",
        "            normalized = relation\n",
        "        else:\n",
        "            relation = str(relation)\n",
        "            if use_translation and relation.isascii():\n",
        "                normalized = translate_relation(relation)\n",
        "            else:\n",
        "                normalized = relation\n",
        "        normalized_relations.append(normalized)\n",
        "    df['predicateLabel'] = normalized_relations\n",
        "\n",
        "    # Remove duplicate rows and shuffle data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Normalized data has been saved to '{output_path}'.\")\n",
        "\n",
        "input_excel = \"/content/FarsiBase/complete_triples.csv\"\n",
        "output_excel = \"/content/FarsiBase/triple.csv\"\n",
        "normalize_excel(input_path=input_excel,\n",
        "                output_path=output_excel,\n",
        "                use_translation=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxlMLPEz4o58",
        "outputId": "8cda2b2c-fc92-48b8-b109-818653b500a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized data has been saved to '/content/FarsiBase/triple.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_dataset_for_link_prediction(file_path):\n",
        "\n",
        "    # 1. Read the dataset\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, names=['subject', 'predicate', 'object'])\n",
        "        print(f\"‚úÖ Dataset successfully loaded. Number of triples: {len(df):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading file: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Compute basic statistics\n",
        "    num_entities = len(set(df['subject']).union(set(df['object'])))\n",
        "    num_relations = len(set(df['predicate']))\n",
        "    print(f\"\\nüìä Basic statistics:\")\n",
        "    print(f\"Number of unique entities: {num_entities:,}\")\n",
        "    print(f\"Number of unique relations: {num_relations:,}\")\n",
        "\n",
        "    # 3. Build the graph\n",
        "    G = nx.MultiDiGraph()  # Directed graph allowing multiple edges between nodes\n",
        "    for _, row in df.iterrows():\n",
        "        G.add_edge(row['subject'], row['object'], key=row['predicate'])\n",
        "\n",
        "    # 4. Analyze node degrees\n",
        "    degrees = dict(G.degree())\n",
        "    degree_counts = Counter(degrees.values())\n",
        "\n",
        "    print(\"\\nüìà Node degree distribution:\")\n",
        "    print(f\"‚Ä¢ Nodes with degree 1: {degree_counts.get(1, 0):,} ({degree_counts.get(1, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"‚Ä¢ Nodes with degree 2: {degree_counts.get(2, 0):,} ({degree_counts.get(2, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"‚Ä¢ Nodes with degree 3: {degree_counts.get(3, 0):,} ({degree_counts.get(3, 0)/G.number_of_nodes():.1%})\")\n",
        "\n",
        "    # 5. Compute graph metrics\n",
        "    density = nx.density(G)\n",
        "    sparsity = 1 - density\n",
        "    avg_degree = sum(degrees.values()) / G.number_of_nodes()\n",
        "    print(\"\\nüîç Graph structural metrics:\")\n",
        "    print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
        "    print(f\"Number of edges: {G.number_of_edges():,}\")\n",
        "    print(f\"Graph density: {density:.6f}\")\n",
        "    print(f\"Sparsity: {sparsity:.4f}\")\n",
        "    print(f\"Average node degree: {avg_degree:.2f}\")\n",
        "\n",
        "    # 6. Check connectivity\n",
        "    if nx.is_weakly_connected(G):\n",
        "        print(\"\\nüîÑ The graph is weakly connected\")\n",
        "    else:\n",
        "        components = nx.number_weakly_connected_components(G)\n",
        "        print(f\"\\nüîó The graph has {components} disconnected components\")\n",
        "\n",
        "    # 7. Final evaluation for link prediction suitability\n",
        "    print(\"\\nüß™ Link prediction suitability assessment:\")\n",
        "\n",
        "    suitability_score = 0\n",
        "\n",
        "    # Criterion 1: Relation diversity\n",
        "    if num_relations > 50:\n",
        "        print(f\"‚úì Excellent relation diversity ({num_relations} relation types)\")\n",
        "        suitability_score += 2\n",
        "    elif num_relations > 10:\n",
        "        print(f\"‚úì Acceptable relation diversity ({num_relations} relation types)\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"‚úó Insufficient relation diversity ({num_relations} relation types)\")\n",
        "\n",
        "    # Criterion 2: Sparsity\n",
        "    if sparsity > 0.99:\n",
        "        print(\"‚úì Ideal sparsity (very suitable for link prediction)\")\n",
        "        suitability_score += 2\n",
        "    elif sparsity > 0.95:\n",
        "        print(\"‚úì Acceptable sparsity\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(\"‚úó Insufficient sparsity\")\n",
        "\n",
        "    # Criterion 3: Degree distribution\n",
        "    if degree_counts.get(1, 0) < G.number_of_nodes() * 0.4:\n",
        "        print(\"‚úì Balanced degree distribution\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"‚úó Unbalanced degree distribution ({degree_counts.get(1, 0)/G.number_of_nodes():.1%} of nodes have degree 1)\")\n",
        "\n",
        "    # Final conclusion\n",
        "    print(\"\\nüéØ Final conclusion:\")\n",
        "    if suitability_score >= 4:\n",
        "        print(\"‚úÖ This dataset is highly suitable for link prediction\")\n",
        "    elif suitability_score >= 2:\n",
        "        print(\"‚ö†Ô∏è This dataset requires improvements for link prediction\")\n",
        "    else:\n",
        "        print(\"‚ùå This dataset is not suitable for link prediction\")\n",
        "\n",
        "# Example usage\n",
        "analyze_dataset_for_link_prediction('/content/FarsiBase/complete_triples.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8WVPKdt4vw7",
        "outputId": "ebbf960e-782c-4ba5-dac0-5df26df7537a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset successfully loaded. Number of triples: 191,415\n",
            "\n",
            "üìä Basic statistics:\n",
            "Number of unique entities: 49,951\n",
            "Number of unique relations: 2,728\n",
            "\n",
            "üìà Node degree distribution:\n",
            "‚Ä¢ Nodes with degree 1: 9,003 (18.0%)\n",
            "‚Ä¢ Nodes with degree 2: 11,742 (23.5%)\n",
            "‚Ä¢ Nodes with degree 3: 5,258 (10.5%)\n",
            "\n",
            "üîç Graph structural metrics:\n",
            "Number of nodes: 49,951\n",
            "Number of edges: 191,415\n",
            "Graph density: 0.000077\n",
            "Sparsity: 0.9999\n",
            "Average node degree: 7.66\n",
            "\n",
            "üîó The graph has 573 disconnected components\n",
            "\n",
            "üß™ Link prediction suitability assessment:\n",
            "‚úì Excellent relation diversity (2728 relation types)\n",
            "‚úì Ideal sparsity (very suitable for link prediction)\n",
            "‚úì Balanced degree distribution\n",
            "\n",
            "üéØ Final conclusion:\n",
            "‚úÖ This dataset is highly suitable for link prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting three variants of the main dataset"
      ],
      "metadata": {
        "id": "rSpunV6j5EmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os, zipfile\n",
        "\n",
        "TARGET_SIZE = 15000\n",
        "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.7, 0.1, 0.2\n",
        "\n",
        "\n",
        "# Full inductive split\n",
        "def split_full_inductive(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects, objects = set(df.iloc[:, 0].dropna()), set(df.iloc[:, 2].dropna())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    # disjoint split of entities\n",
        "    train_entities, remainder = train_test_split(list(all_entities), test_size=(1 - TRAIN_RATIO), random_state=42)\n",
        "    val_entities, test_entities = train_test_split(\n",
        "        remainder, test_size=TEST_RATIO / (VAL_RATIO + TEST_RATIO), random_state=42\n",
        "    )\n",
        "\n",
        "    # enforce disjoint sets\n",
        "    train_entities, val_entities, test_entities = set(train_entities), set(val_entities), set(test_entities)\n",
        "    assert train_entities.isdisjoint(val_entities)\n",
        "    assert train_entities.isdisjoint(test_entities)\n",
        "    assert val_entities.isdisjoint(test_entities)\n",
        "\n",
        "    train_df = df[df.iloc[:, 0].isin(train_entities) & df.iloc[:, 2].isin(train_entities)]\n",
        "    val_df   = df[df.iloc[:, 0].isin(val_entities) & df.iloc[:, 2].isin(val_entities)]\n",
        "    test_df  = df[df.iloc[:, 0].isin(test_entities) & df.iloc[:, 2].isin(test_entities)]\n",
        "\n",
        "    # sample to fixed size\n",
        "    n_train, n_val = int(TARGET_SIZE*TRAIN_RATIO), int(TARGET_SIZE*VAL_RATIO)\n",
        "    return (\n",
        "        train_df.sample(n=n_train, random_state=42),\n",
        "        val_df.sample(n=n_val, random_state=42),\n",
        "        test_df.sample(n=TARGET_SIZE-n_train-n_val, random_state=42)\n",
        "    )\n",
        "\n",
        "\n",
        "# Semi-inductive split\n",
        "def split_semi_inductive(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects, objects = set(df.iloc[:, 0].dropna()), set(df.iloc[:, 2].dropna())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    seen_entities, remainder = train_test_split(list(all_entities), test_size=(1 - TRAIN_RATIO), random_state=42)\n",
        "    val_entities, test_entities = train_test_split(\n",
        "        remainder, test_size=TEST_RATIO / (VAL_RATIO + TEST_RATIO), random_state=42\n",
        "    )\n",
        "\n",
        "    seen_entities, val_entities, test_entities = set(seen_entities), set(val_entities), set(test_entities)\n",
        "\n",
        "    # enforce disjoint sets\n",
        "    assert seen_entities.isdisjoint(val_entities.union(test_entities))\n",
        "    assert val_entities.isdisjoint(test_entities)\n",
        "\n",
        "    # train: both entities in seen\n",
        "    train_df = df[df.iloc[:, 0].isin(seen_entities) & df.iloc[:, 2].isin(seen_entities)]\n",
        "\n",
        "    # val: one entity in seen, one in val_entities\n",
        "    val_mask = (\n",
        "        (df.iloc[:,0].isin(seen_entities) & df.iloc[:,2].isin(val_entities)) |\n",
        "        (df.iloc[:,0].isin(val_entities) & df.iloc[:,2].isin(seen_entities))\n",
        "    )\n",
        "    val_df = df[val_mask]\n",
        "\n",
        "    # test: one entity in seen, one in test_entities\n",
        "    test_mask = (\n",
        "        (df.iloc[:,0].isin(seen_entities) & df.iloc[:,2].isin(test_entities)) |\n",
        "        (df.iloc[:,0].isin(test_entities) & df.iloc[:,2].isin(seen_entities))\n",
        "    )\n",
        "    test_df = df[test_mask]\n",
        "\n",
        "    # sample to fixed size\n",
        "    n_train, n_val = int(TARGET_SIZE*TRAIN_RATIO), int(TARGET_SIZE*VAL_RATIO)\n",
        "    return (\n",
        "        train_df.sample(n=n_train, random_state=42),\n",
        "        val_df.sample(n=n_val, random_state=42),\n",
        "        test_df.sample(n=TARGET_SIZE-n_train-n_val, random_state=42)\n",
        "    )\n",
        "\n",
        "\n",
        "# Transductive split\n",
        "def split_transductive(file_path):\n",
        "    df = pd.read_csv(file_path).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    n_train, n_val = int(TARGET_SIZE*TRAIN_RATIO), int(TARGET_SIZE*VAL_RATIO)\n",
        "    return (\n",
        "        df.iloc[:n_train],\n",
        "        df.iloc[n_train:n_train+n_val],\n",
        "        df.iloc[n_train+n_val:n_train+n_val+(TARGET_SIZE-n_train-n_val)]\n",
        "    )\n",
        "\n",
        "\n",
        "# Paths and setup\n",
        "file_path = '/content/FarsiBase/complete_triples.csv'\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "versions = {\n",
        "    'PersianILP_V1': split_full_inductive,\n",
        "    'PersianILP_V2': split_semi_inductive,\n",
        "    'PersianILP_V3': split_transductive\n",
        "}\n",
        "\n",
        "for version_name, split_func in versions.items():\n",
        "    version_dir = os.path.join(output_dir, version_name)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "\n",
        "    train_data, val_data, test_data = split_func(file_path)\n",
        "    train_data.to_csv(os.path.join(version_dir,'train.csv'),index=False,encoding='utf-8-sig')\n",
        "    val_data.to_csv(os.path.join(version_dir,'valid.csv'),index=False,encoding='utf-8-sig')\n",
        "    test_data.to_csv(os.path.join(version_dir,'test.csv'),index=False,encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"[{version_name}] Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "# compress into one zip\n",
        "zip_path = os.path.join(output_dir, 'PersianILP-data.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root,_,files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.zip'):\n",
        "                file_path=os.path.join(root,file)\n",
        "                arcname=os.path.relpath(file_path,output_dir)\n",
        "                zipf.write(file_path,arcname)\n",
        "print(f\"‚úÖ ZIP file created successfully:\\n{zip_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwqDPmIE5H_l",
        "outputId": "09b2ab15-ad40-4910-da86-48c0921e4de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PersianILP_V1] Train: 10500, Val: 1500, Test: 3000\n",
            "[PersianILP_V2] Train: 10500, Val: 1500, Test: 3000\n",
            "[PersianILP_V3] Train: 10500, Val: 1500, Test: 3000\n",
            "‚úÖ ZIP file created successfully:\n",
            "/content/PersianILP-trainTest/PersianILP-data.zip\n"
          ]
        }
      ]
    }
  ]
}