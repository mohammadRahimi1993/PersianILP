{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZw-LPTl7tx"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SfQkfVglkHb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "!pip install numpy==1.26.4\n",
        "clear_output()\n",
        "print(\"Numpy install successful!\")\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpV8iTO6l0i_",
        "outputId": "1bbad3cc-7733-408d-8a59-2c807516b6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "DGL installed!\n",
            "PyTorch Version:  2.2.0+cu121\n",
            "TorchMetrics Version:  1.2.1\n",
            "Transformers Version:  4.38.0\n",
            "DGL Version:  2.4.0\n",
            "TorchEval Is:  0.0.7\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "!pip install torchmetrics==1.2.1 transformers==4.38.0\n",
        "!pip install safetensors==0.4.1\n",
        "!pip install torcheval\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import dgl\n",
        "import torch\n",
        "import torchmetrics\n",
        "import transformers\n",
        "import torcheval\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    import dgl.graphbolt as gb\n",
        "    installed = True\n",
        "except ImportError as error:\n",
        "    installed = False\n",
        "    print(error)\n",
        "\n",
        "print(\"DGL installed!\" if installed else \"DGL not found!\")\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"TorchMetrics Version: \", torchmetrics.__version__)\n",
        "print(\"Transformers Version: \", transformers.__version__)\n",
        "print(\"DGL Version: \", dgl.__version__)\n",
        "print(\"TorchEval Is: \", torcheval.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspcHPFBpHRD"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN6CQqq7pLpu",
        "outputId": "483a51b4-363d-4d4a-9fc0-5913575af732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/ILPDataSet.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir:str='/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            datasets[dataset_name] = {\n",
        "                \"train\": os.path.join(dataset_path, \"train.txt\"),\n",
        "                \"valid\": os.path.join(dataset_path, \"valid.txt\"),\n",
        "                \"test\":  os.path.join(dataset_path, \"test.txt\")}\n",
        "    return datasets\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/ILPDataSet')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZymFSOqqnf"
      },
      "source": [
        "### Analysis PersianILP With English BencmarkDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvlSVPPVZSn5",
        "outputId": "622b69c1-c534-49b5-ecf2-871124225687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| Dataset             |   Triples |   Entities |   Relations |   Deg_1 |   Deg_2 |   Deg_3 |   Avg_Degree |   Density |   Sparsity |\n",
            "+=====================+===========+============+=============+=========+=========+=========+==============+===========+============+\n",
            "| PersianILP-V1_test  |      3000 |       2869 |         513 |    1757 |     640 |     213 |         2.09 |  0.000364 |   0.999636 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V1_train |     10500 |      11029 |         966 |    7407 |    2000 |     736 |         1.9  |  8.6e-05  |   0.999914 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V1_valid |      1500 |       1241 |         355 |     630 |     321 |     136 |         2.42 |  0.000975 |   0.999025 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V2_test  |      3000 |       4245 |         554 |    3565 |     445 |     116 |         1.41 |  0.000167 |   0.999833 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V2_train |     10500 |      11029 |         966 |    7407 |    2000 |     736 |         1.9  |  8.6e-05  |   0.999914 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V2_valid |      1500 |       2185 |         390 |    1844 |     223 |      61 |         1.37 |  0.000314 |   0.999686 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V3_test  |      3000 |       4514 |         541 |    3853 |     457 |     104 |         1.33 |  0.000147 |   0.999853 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V3_train |     10500 |      12286 |         984 |    9017 |    1947 |     637 |         1.71 |  7e-05    |   0.99993  |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP-V3_valid |      1500 |       2456 |         404 |    2188 |     183 |      48 |         1.22 |  0.000249 |   0.999751 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_test  |       188 |        286 |           6 |     212 |      61 |      11 |         1.31 |  0.002306 |   0.997694 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_train |      1618 |        922 |           8 |     187 |     227 |     155 |         3.51 |  0.001905 |   0.998095 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_valid |       185 |        290 |           7 |     223 |      55 |      11 |         1.28 |  0.002207 |   0.997793 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_test  |       441 |        710 |           9 |     573 |     111 |      19 |         1.24 |  0.000876 |   0.999124 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_train |      4011 |       2757 |          10 |     829 |     684 |     453 |         2.91 |  0.000528 |   0.999472 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_valid |       411 |        661 |          10 |     524 |     117 |      16 |         1.24 |  0.000942 |   0.999058 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_test  |       605 |        975 |          10 |     832 |     110 |      21 |         1.24 |  0.000637 |   0.999363 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_train |      6327 |       5084 |          11 |    2144 |    1619 |     581 |         2.49 |  0.000245 |   0.999755 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_valid |       538 |        882 |           9 |     758 |     106 |       4 |         1.22 |  0.000692 |   0.999308 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_test  |      1429 |       2270 |           9 |    1793 |     391 |      64 |         1.26 |  0.000277 |   0.999723 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_train |     12334 |       7084 |           9 |    1424 |    1710 |    1311 |         3.48 |  0.000246 |   0.999754 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_valid |      1394 |       2205 |           9 |    1731 |     386 |      70 |         1.26 |  0.000287 |   0.999713 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_test   |       205 |        301 |          68 |     226 |      55 |      14 |         1.36 |  0.00227  |   0.99773  |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_train  |      1993 |       1093 |         142 |     376 |     227 |     152 |         3.65 |  0.00167  |   0.99833  |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_valid  |       206 |        287 |          66 |     211 |      48 |      17 |         1.44 |  0.00251  |   0.99749  |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_test   |       478 |        562 |         107 |     362 |     118 |      41 |         1.7  |  0.001516 |   0.998484 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_train  |      4145 |       1660 |         172 |     419 |     300 |     263 |         4.99 |  0.001505 |   0.998495 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_valid  |       469 |        548 |          92 |     359 |     116 |      38 |         1.71 |  0.001565 |   0.998435 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_test   |       865 |        981 |         128 |     609 |     213 |      98 |         1.76 |  0.0009   |   0.9991   |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_train  |      7406 |       2501 |         183 |     531 |     354 |     330 |         5.92 |  0.001184 |   0.998816 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_valid  |       866 |        973 |         120 |     587 |     225 |     101 |         1.78 |  0.000916 |   0.999084 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_test   |      1424 |       1427 |         166 |     817 |     316 |     145 |         2    |  0.0007   |   0.9993   |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_train  |     11714 |       3051 |         200 |     493 |     378 |     312 |         7.68 |  0.001259 |   0.998741 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_valid  |      1416 |       1418 |         155 |     820 |     314 |     129 |         2    |  0.000705 |   0.999295 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_test    |       100 |         84 |           7 |      67 |      12 |       3 |         2.38 |  0.014343 |   0.985657 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_train   |       833 |        225 |          14 |      12 |      22 |      40 |         7.4  |  0.016528 |   0.983472 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_valid   |       101 |         84 |           6 |      66 |      13 |       3 |         2.4  |  0.014487 |   0.985513 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_test    |       476 |        478 |          54 |     316 |      67 |      32 |         1.99 |  0.002088 |   0.997912 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_train   |      4586 |       2086 |          79 |     968 |     415 |     196 |         4.4  |  0.001054 |   0.998946 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_valid   |       459 |        487 |          58 |     304 |     101 |      25 |         1.89 |  0.001939 |   0.998061 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_test    |       809 |        798 |          87 |     521 |     150 |      60 |         2.03 |  0.001272 |   0.998728 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_train   |      8048 |       3566 |         122 |    1683 |     705 |     359 |         4.51 |  0.000633 |   0.999367 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_valid   |       811 |        814 |          73 |     532 |     151 |      57 |         1.99 |  0.001225 |   0.998775 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_test    |       731 |        630 |          47 |     381 |      98 |      47 |         2.32 |  0.001845 |   0.998155 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_train   |      7073 |       2795 |          61 |    1342 |     570 |     272 |         5.06 |  0.000906 |   0.999094 |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_valid   |       716 |        616 |          46 |     380 |      93 |      35 |         2.32 |  0.00189  |   0.99811  |\n",
            "+---------------------+-----------+------------+-------------+---------+---------+---------+--------------+-----------+------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_data(file_path):\n",
        "    sep = \",\" if file_path.endswith('.csv') else \"\\t\"\n",
        "    return pd.read_csv(file_path, sep=sep, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "def analyze_graph_metrics(file_path):\n",
        "    df = load_data(file_path)\n",
        "    G = nx.MultiDiGraph()\n",
        "    G.add_edges_from(zip(df[\"head\"], df[\"tail\"], df[\"relation\"]))\n",
        "\n",
        "    degrees = dict(G.degree())\n",
        "    counter = Counter(degrees.values())\n",
        "    avg_deg = sum(degrees.values()) / G.number_of_nodes() if G.number_of_nodes() else 0\n",
        "\n",
        "    # محاسبه‌ی تعداد سه‌تایی‌ها، موجودیت‌ها و روابط\n",
        "    num_triples = len(df)\n",
        "    num_entities = len(set(df[\"head\"]).union(set(df[\"tail\"])))\n",
        "    num_relations = len(set(df[\"relation\"]))\n",
        "\n",
        "    return {\n",
        "        \"Triples\": num_triples,\n",
        "        \"Entities\": num_entities,\n",
        "        \"Relations\": num_relations,\n",
        "        \"Deg_1\": counter.get(1, 0),\n",
        "        \"Deg_2\": counter.get(2, 0),\n",
        "        \"Deg_3\": counter.get(3, 0),\n",
        "        \"Avg_Degree\": round(avg_deg, 2),\n",
        "        \"Density\": round(nx.density(G), 6),\n",
        "        \"Sparsity\": round(1 - nx.density(G), 6)\n",
        "    }\n",
        "\n",
        "def process_file(file_path, label):\n",
        "    if os.path.isfile(file_path) and file_path.endswith(('.csv', '.txt')):\n",
        "        metrics = analyze_graph_metrics(file_path)\n",
        "        if metrics:\n",
        "            metrics['Dataset'] = label\n",
        "            return metrics\n",
        "    return None\n",
        "\n",
        "def analyze_all_datasets(all_dirs):\n",
        "    results = []\n",
        "    for base_dir in all_dirs:\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            dataset_name = os.path.basename(root)\n",
        "            for file in files:\n",
        "                path = os.path.join(root, file)\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                label_type = \"CSV\" if ext == '.csv' else \"TXT\"\n",
        "                label = f\"{dataset_name}_{os.path.splitext(file)[0]}\"\n",
        "                result = process_file(path, label)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)[[\n",
        "        \"Dataset\", \"Triples\", \"Entities\", \"Relations\",\n",
        "        \"Deg_1\", \"Deg_2\", \"Deg_3\", \"Avg_Degree\", \"Density\", \"Sparsity\"\n",
        "    ]]\n",
        "\n",
        "# مسیر دیتاست‌ها را مشخص کن\n",
        "all_dirs = [\n",
        "    \"/content/ILPDataSet\",\n",
        "    \"/content/PersianILP-trainTest\"\n",
        "]\n",
        "\n",
        "# اجرای تحلیل و نمایش جدول\n",
        "df_result = analyze_all_datasets(all_dirs).sort_values(\"Dataset\")\n",
        "print(tabulate(df_result, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beH7-u5KqyOq"
      },
      "source": [
        "### Inductive Link Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61E1j7rlgi0Q",
        "outputId": "8e0a9e48-650f-4fb9-d71e-0b28b5da5dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| Dataset       |   train_triples |   test_triples |   train_relations |   test_relations |   train_entities |   test_entities |\n",
            "+===============+=================+================+===================+==================+==================+=================+\n",
            "| PersianILP-V1 |           10500 |           3000 |               966 |              513 |            11029 |            2869 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP-V2 |           10500 |           3000 |               966 |              554 |            11029 |            4245 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP-V3 |           10500 |           3000 |               984 |              541 |            12286 |            4514 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v1_ind |            1618 |            188 |                 8 |                6 |              922 |             286 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v2_ind |            4011 |            441 |                10 |                9 |             2757 |             710 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v3_ind |            6327 |            605 |                11 |               10 |             5084 |             975 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v4_ind |           12334 |           1429 |                 9 |                9 |             7084 |            2270 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v1_ind  |            1993 |            205 |               142 |               68 |             1093 |             301 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v2_ind  |            4145 |            478 |               172 |              107 |             1660 |             562 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v3_ind  |            7406 |            865 |               183 |              128 |             2501 |             981 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v4_ind  |           11714 |           1424 |               200 |              166 |             3051 |            1427 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v1_ind   |             833 |            100 |                14 |                7 |              225 |              84 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v2_ind   |            4586 |            476 |                79 |               54 |             2086 |             478 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v3_ind   |            8048 |            809 |               122 |               87 |             3566 |             798 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v4_ind   |            7073 |            731 |                61 |               47 |             2795 |             630 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "def analyze_kg_files(base_dir):\n",
        "    \"\"\"Analyze knowledge graph files with better error handling\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for dataset in sorted(os.listdir(base_dir)):\n",
        "        dataset_path = os.path.join(base_dir, dataset)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        stats = {'Dataset': dataset}\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for ext in ['.csv', '.txt']:\n",
        "                file_path = os.path.join(dataset_path, f\"{split}{ext}\")\n",
        "                if not os.path.exists(file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Read file with automatic format detection\n",
        "                    try:\n",
        "                        # First, try reading with header\n",
        "                        df = pd.read_csv(file_path)\n",
        "                        # If required columns are missing, read without header\n",
        "                        if not all(col in df.columns for col in ['head', 'relation', 'tail']):\n",
        "                            df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                             header=None, names=['head', 'relation', 'tail'])\n",
        "                    except:\n",
        "                        # If error occurs, read without header\n",
        "                        df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                         header=None, names=['head', 'relation', 'tail'])\n",
        "\n",
        "                    # Compute basic statistics\n",
        "                    stats.update({\n",
        "                        f'{split}_triples': int(len(df)),\n",
        "                        f'{split}_relations': int(df['relation'].nunique()),\n",
        "                        f'{split}_entities': int(pd.concat([df['head'], df['tail']]).nunique())\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        results.append(stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def show_stats(base_dir):\n",
        "    \"\"\"Display results table with proper formatting\"\"\"\n",
        "    df = analyze_kg_files(base_dir)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No valid dataset found\")\n",
        "        return\n",
        "\n",
        "    # Select and sort columns\n",
        "    columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    df = df.dropna(subset=['train_triples'])[columns].sort_values('Dataset')\n",
        "\n",
        "    # Rename columns\n",
        "    df.columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    # Display numbers as integers\n",
        "    print(tabulate(\n",
        "        df,\n",
        "        headers='keys',\n",
        "        tablefmt='grid',\n",
        "        showindex=False,\n",
        "        numalign=\"right\",\n",
        "        floatfmt=\".0f\"\n",
        "    ))\n",
        "\n",
        "# Sample execution\n",
        "show_stats(\"/content/ILPDataSet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reb0FyYEri_X"
      },
      "source": [
        "### Create DGL Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gVMYZesrrk3m"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "class PersianDGLDataset(DGLDataset):\n",
        "    def __init__(self, train_file, test_file, seed=42):\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.seed = seed\n",
        "        self.process()\n",
        "        super().__init__(name=\"PersianLinkPrediction\")\n",
        "\n",
        "    def process(self):\n",
        "        # Initialize mappings\n",
        "        self.entity2id = {}\n",
        "        self.relation2id = {}\n",
        "        ent_id, rel_id = 0, 0\n",
        "\n",
        "        # Process training data\n",
        "        train_triples = self._load_and_process_file(self.train_file, ent_id, rel_id)\n",
        "        ent_id, rel_id = len(self.entity2id), len(self.relation2id)\n",
        "\n",
        "        # Process test data (using same mappings)\n",
        "        test_triples = self._load_and_process_file(self.test_file, ent_id, rel_id)\n",
        "\n",
        "        # Build graphs\n",
        "        self.graphs = {\n",
        "            \"train\": self._build_graph(train_triples),\n",
        "            \"test\": self._build_graph(test_triples)\n",
        "        }\n",
        "\n",
        "    def _load_file(self, file_path):\n",
        "        \"\"\"Load file based on its extension\"\"\"\n",
        "        if file_path.endswith('.csv'):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            return pd.read_csv(file_path, sep='\\t', header=None,\n",
        "                             names=['subjectLabel', 'predicateLabel', 'objectLabel'])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Only .csv and .txt files are supported.\")\n",
        "\n",
        "    def _load_and_process_file(self, file_path, ent_id_start, rel_id_start):\n",
        "        \"\"\"Load and process a single file, updating mappings\"\"\"\n",
        "        triples = []\n",
        "        df = self._load_file(file_path)\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row['subjectLabel'], row['predicateLabel'], row['objectLabel']\n",
        "\n",
        "            # Update entity mappings\n",
        "            for ent in [h, t]:\n",
        "                if ent not in self.entity2id:\n",
        "                    self.entity2id[ent] = ent_id_start\n",
        "                    ent_id_start += 1\n",
        "\n",
        "            # Update relation mappings\n",
        "            if r not in self.relation2id:\n",
        "                self.relation2id[r] = rel_id_start\n",
        "                rel_id_start += 1\n",
        "\n",
        "            triples.append((\n",
        "                self.entity2id[h],\n",
        "                self.relation2id[r],\n",
        "                self.entity2id[t]))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _build_graph(self, triples):\n",
        "        \"\"\"Build DGL graph from triples\"\"\"\n",
        "        src, rel, dst = zip(*triples)\n",
        "        src = torch.tensor(src)\n",
        "        dst = torch.tensor(dst)\n",
        "        rel = torch.tensor(rel)\n",
        "\n",
        "        g = dgl.graph((src, dst), num_nodes=len(self.entity2id))\n",
        "        g.edata[\"e_type\"] = rel\n",
        "        g.edata[\"edge_mask\"] = torch.ones(g.num_edges(), dtype=torch.bool)\n",
        "        g.ndata[\"ntype\"] = torch.zeros(g.num_nodes(), dtype=torch.int)\n",
        "        g.ndata[\"feat\"] = torch.randn(g.num_nodes(), 64)\n",
        "        return g\n",
        "\n",
        "    def __getitem__(self, split):\n",
        "        return self.graphs[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "class GraphBatchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, graphs, pos_graphs, neg_graphs):\n",
        "        self.graphs = graphs\n",
        "        self.pos_graphs = pos_graphs\n",
        "        self.neg_graphs = neg_graphs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"graph\": self.graphs[idx],\n",
        "            \"pos_graph\": self.pos_graphs[idx],\n",
        "            \"neg_graph\": self.neg_graphs[idx]}\n",
        "\n",
        "\n",
        "dataset = PersianDGLDataset(train_file = ILP_dataset_paths['PersianILP-V1']['train'],\n",
        "                            test_file = ILP_dataset_paths['PersianILP-V1']['test'])\n",
        "train_g = dataset[\"train\"]\n",
        "test_g = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoK1-UFrpiR"
      },
      "source": [
        "### Generate Positive Graph And Negative Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gsFU2PjVrs5_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dgl\n",
        "import scipy.sparse as sp\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "\n",
        "class GraphNegativeSampler:\n",
        "    def __init__(self, train_graph, test_graph, train_neg_ratio=1.0, test_neg_ratio=1.0):\n",
        "        self.train_graph = train_graph\n",
        "        self.test_graph = test_graph\n",
        "        self.train_neg_ratio = train_neg_ratio\n",
        "        self.test_neg_ratio = test_neg_ratio\n",
        "        self.train_pos_g, self.train_neg_g = self._prepare_graphs(train_graph, train_neg_ratio)\n",
        "        self.test_pos_g, self.test_neg_g = self._prepare_graphs(test_graph, test_neg_ratio)\n",
        "\n",
        "    def _generate_negative_samples(self, graph):\n",
        "        u, v = graph.edges()\n",
        "        adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())),\n",
        "                          shape=(graph.num_nodes(), graph.num_nodes()))\n",
        "        return np.where(1 - adj.todense() - np.eye(graph.num_nodes()) != 0)\n",
        "\n",
        "    def _prepare_graphs(self, graph, ratio):\n",
        "        return ( self._create_positive_graph(graph),\n",
        "                 self._create_negative_graph(graph, ratio))\n",
        "\n",
        "    def _create_positive_graph(self, graph):\n",
        "        g = dgl.graph(graph.edges(), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = graph.edata[\"e_type\"]\n",
        "        g.ndata.update({k: graph.ndata[k] for k in [\"feat\", \"ntype\"]})\n",
        "        return g\n",
        "\n",
        "    def _create_negative_graph(self, graph, ratio):\n",
        "        neg_u, neg_v = self._generate_negative_samples(graph)\n",
        "        num_samples = int(graph.num_edges() * ratio)\n",
        "        replace = len(neg_u) < num_samples\n",
        "        sample_ids = np.random.choice(len(neg_u), num_samples, replace=replace)\n",
        "\n",
        "        g = dgl.graph((neg_u[sample_ids], neg_v[sample_ids]), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = torch.randint(0, graph.edata[\"e_type\"].max().item()+1, (g.num_edges(),))\n",
        "        g.ndata.update({\n",
        "            \"feat\": graph.ndata[\"feat\"],\n",
        "            \"ntype\": torch.ones(graph.num_nodes(), dtype=torch.int)})\n",
        "        return g\n",
        "\n",
        "    @property\n",
        "    def training_graphs(self):\n",
        "        return self.train_pos_g, self.train_neg_g\n",
        "\n",
        "    @property\n",
        "    def test_graphs(self):\n",
        "        return self.test_pos_g, self.test_neg_g\n",
        "\n",
        "# Sampling From Knowladge Graph\n",
        "sampler = GraphNegativeSampler(dataset['train'],\n",
        "                               dataset['test'],\n",
        "                               train_neg_ratio=1,\n",
        "                               test_neg_ratio=1)\n",
        "\n",
        "train_pos, train_neg = sampler.training_graphs\n",
        "test_pos, test_neg = sampler.test_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGioSEvKr24x"
      },
      "source": [
        "### Link Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rZffOokcr50c"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedGraphSAGE(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats, out_feats, dropout=0.5):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, out_feats, \"mean\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            return g.edata[\"score\"][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRW_iO3fr7Ek"
      },
      "source": [
        "### Train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sN25QhydsCYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import dgl\n",
        "\n",
        "def train_model(model,\n",
        "                pred,\n",
        "                dataloader,\n",
        "                epochs,\n",
        "                lr=0.01):\n",
        "\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(),\n",
        "                                                 pred.parameters()),\n",
        "                                                 lr=lr)\n",
        "\n",
        "    all_losses = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch_graph = batch[\"graph\"]    # گراف اصلی\n",
        "            pos_graph = batch[\"pos_graph\"]  # گراف مثبت\n",
        "            neg_graph = batch[\"neg_graph\"]  # گراف منفی\n",
        "\n",
        "            # Forward pass\n",
        "            h = model(batch_graph, batch_graph.ndata[\"feat\"])\n",
        "            pos_score = pred(pos_graph, h)\n",
        "            neg_score = pred(neg_graph, h)\n",
        "            loss = compute_loss(pos_score,neg_score)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            e = epoch\n",
        "            loss = epoch_loss\n",
        "\n",
        "        all_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEpoch: {e}, Loss: {loss:.4f}\")\n",
        "    return h, all_losses\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxC8E0sWsGhm"
      },
      "source": [
        "### Train And Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N_Mp6D-8sPxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
        "from sklearn import metrics\n",
        "\n",
        "def train_and_evaluate_model(result:dict,\n",
        "                             dataset_name,\n",
        "                             ILP_dataset_paths,\n",
        "                             h_feats=16,\n",
        "                             out_feats=10,\n",
        "                             dropout=0.5,\n",
        "                             epochs=2000,\n",
        "                             lr=0.001,\n",
        "                             train_neg_ratio=10,\n",
        "                             test_neg_ratio=1):\n",
        "\n",
        "    # ===== Step 1: Dataset Preparation =====\n",
        "    graphs = PersianDGLDataset(\n",
        "        train_file=ILP_dataset_paths[dataset_name]['train'],\n",
        "        test_file=ILP_dataset_paths[dataset_name]['test']\n",
        "    )\n",
        "\n",
        "    sampler = GraphNegativeSampler(\n",
        "        graphs['train'], graphs['test'],\n",
        "        train_neg_ratio=train_neg_ratio,\n",
        "        test_neg_ratio=test_neg_ratio\n",
        "    )\n",
        "\n",
        "    train_pos_g, train_neg_g = sampler.training_graphs\n",
        "    test_pos_g, test_neg_g = sampler.test_graphs\n",
        "\n",
        "    train_dataset = GraphBatchDataset([graphs['train']], [train_pos_g], [train_neg_g])\n",
        "    train_loader = GraphDataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    test_dataset = GraphBatchDataset([graphs['test']], [test_pos_g], [test_neg_g])\n",
        "    test_loader = GraphDataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    # ===== Step 2: Training =====\n",
        "    def compute_loss(pos_score, neg_score):\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([\n",
        "            torch.ones(pos_score.shape[0]),\n",
        "            torch.zeros(neg_score.shape[0])\n",
        "        ])\n",
        "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "    in_feats = graphs['train'].ndata['feat'].shape[1]\n",
        "    model = ImprovedGraphSAGE(\n",
        "        in_feats=in_feats,\n",
        "        h_feats=h_feats,\n",
        "        out_feats=out_feats,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    pred = DotPredictor()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        itertools.chain(model.parameters(), pred.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch in train_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            pos_score = pred(batch['pos_graph'], h)\n",
        "            neg_score = pred(batch['neg_graph'], h)\n",
        "            loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ===== Step 3: Evaluation =====\n",
        "    pos_scores, pos_labels = [], []\n",
        "    neg_scores, neg_labels = [], []\n",
        "    hit1_list, hit3_list, hit10_list = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ranks = []\n",
        "        for batch in test_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            score_pos = pred(batch['pos_graph'], h).squeeze()\n",
        "            score_neg = pred(batch['neg_graph'], h).squeeze()\n",
        "\n",
        "            neg_per_pos = len(score_neg) // len(score_pos)\n",
        "            pos_scores += score_pos.tolist() if score_pos.dim() > 0 else [score_pos.item()]\n",
        "            neg_scores += score_neg.tolist() if score_neg.dim() > 0 else [score_neg.item()]\n",
        "            pos_labels += [1] * len(score_pos) if score_pos.dim() > 0 else [1]\n",
        "            neg_labels += [0] * len(score_neg) if score_neg.dim() > 0 else [0]\n",
        "\n",
        "            score_pos_exp = score_pos.view(-1, 1).repeat(1, neg_per_pos).view(-1)\n",
        "            scores = torch.stack([score_pos_exp, score_neg], dim=1)\n",
        "            scores = torch.softmax(scores, dim=1).cpu().numpy()\n",
        "            rank = np.argwhere(np.argsort(scores, axis=1)[:, ::-1] == 0)[:, 1] + 1\n",
        "\n",
        "            ranks += rank.tolist()\n",
        "            hit1_list += [1 if r <= 1 else 0 for r in rank]\n",
        "            hit3_list += [1 if r <= 3 else 0 for r in rank]\n",
        "            hit10_list += [1 if r <= 10 else 0 for r in rank]\n",
        "\n",
        "    # ===== Step 4: Result Metrics =====\n",
        "    result[dataset_name] = {\n",
        "        \"AUC\": metrics.roc_auc_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"AUC_PR\": metrics.average_precision_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"MRR\": np.mean(1.0 / np.array(ranks)).item(),\n",
        "        \"Hit1\": np.mean(hit1_list),\n",
        "        \"Hit3\": np.mean(hit3_list),\n",
        "        \"Hit10\": np.mean(hit10_list)}\n",
        "\n",
        "    return result\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tabulate import tabulate\n",
        "def display_results_table(result_dict):\n",
        "    clear_output()\n",
        "    headers = ['Dataset', 'AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']\n",
        "    rows = []\n",
        "    for name, metrics in result_dict.items():\n",
        "        row = [name] + [metrics[h] for h in headers[1:]]\n",
        "        rows.append(row)\n",
        "    print(\"\\n\" + tabulate(rows,\n",
        "                          headers=headers,\n",
        "                          tablefmt=\"fancy_grid\",\n",
        "                          floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment with 1 Negative Samples**"
      ],
      "metadata": {
        "id": "bBYqHVg4Wgle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCPemgqsWKU",
        "outputId": "d05bd159-6ca3-4ab4-f621-e082d55847e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Results After 20 Runs:\n",
            "╒═══════════════╤══════════════════╤═════════════════════╤══════════════════╤═══════════════════╤═══════════════════╤════════════════════╕\n",
            "│ Dataset       │ AUC (mean±std)   │ AUC_PR (mean±std)   │ MRR (mean±std)   │ Hit1 (mean±std)   │ Hit3 (mean±std)   │ Hit10 (mean±std)   │\n",
            "╞═══════════════╪══════════════════╪═════════════════════╪══════════════════╪═══════════════════╪═══════════════════╪════════════════════╡\n",
            "│ PersianILP-V1 │ 0.7535±0.0112    │ 0.7940±0.0100       │ 0.8766±0.0068    │ 0.7532±0.0135     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP-V2 │ 0.7928±0.0082    │ 0.8196±0.0078       │ 0.8963±0.0043    │ 0.7925±0.0086     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP-V3 │ 0.8130±0.0091    │ 0.8358±0.0086       │ 0.9066±0.0054    │ 0.8132±0.0109     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v1_ind │ 0.8099±0.0200    │ 0.8427±0.0186       │ 0.9062±0.0123    │ 0.8125±0.0247     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v2_ind │ 0.8300±0.0243    │ 0.8548±0.0196       │ 0.9171±0.0155    │ 0.8341±0.0310     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v3_ind │ 0.7667±0.0319    │ 0.8107±0.0255       │ 0.8852±0.0150    │ 0.7704±0.0299     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v4_ind │ 0.8284±0.0148    │ 0.8515±0.0117       │ 0.9145±0.0063    │ 0.8289±0.0125     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v1_ind  │ 0.7839±0.0254    │ 0.8110±0.0232       │ 0.8900±0.0163    │ 0.7800±0.0327     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v2_ind  │ 0.7634±0.0206    │ 0.8011±0.0167       │ 0.8801±0.0102    │ 0.7601±0.0205     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v3_ind  │ 0.7300±0.0194    │ 0.7719±0.0165       │ 0.8647±0.0108    │ 0.7293±0.0215     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v4_ind  │ 0.7140±0.0188    │ 0.7560±0.0164       │ 0.8563±0.0099    │ 0.7126±0.0198     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v1_ind   │ 0.6738±0.0947    │ 0.6650±0.1134       │ 0.8405±0.0484    │ 0.6810±0.0967     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v2_ind   │ 0.7396±0.0247    │ 0.7975±0.0188       │ 0.8702±0.0149    │ 0.7403±0.0298     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v3_ind   │ 0.7385±0.0242    │ 0.7946±0.0213       │ 0.8677±0.0127    │ 0.7354±0.0255     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v4_ind   │ 0.7349±0.0261    │ 0.7989±0.0220       │ 0.8682±0.0131    │ 0.7364±0.0263     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "╘═══════════════╧══════════════════╧═════════════════════╧══════════════════╧═══════════════════╧═══════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "            result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=1,\n",
        "            test_neg_ratio=1)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (mean±std)',\n",
        "    'AUC_PR (mean±std)',\n",
        "    'MRR (mean±std)',\n",
        "    'Hit1 (mean±std)',\n",
        "    'Hit3 (mean±std)',\n",
        "    'Hit10 (mean±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment with 50 Negative Samples**"
      ],
      "metadata": {
        "id": "8Kdh3cbmWYTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1xcYDfssbb-",
        "outputId": "b6438865-51c7-49b4-eccd-a2815c194b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 20 Runs:\n",
            "╒═══════════════╤══════════════════╤═════════════════════╤══════════════════╤═══════════════════╤═══════════════════╤════════════════════╕\n",
            "│ Dataset       │ AUC (mean±std)   │ AUC_PR (mean±std)   │ MRR (mean±std)   │ Hit1 (mean±std)   │ Hit3 (mean±std)   │ Hit10 (mean±std)   │\n",
            "╞═══════════════╪══════════════════╪═════════════════════╪══════════════════╪═══════════════════╪═══════════════════╪════════════════════╡\n",
            "│ PersianILP-V1 │ 0.6017±0.0184    │ 0.0719±0.0109       │ 0.8008±0.0093    │ 0.6016±0.0186     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP-V2 │ 0.6271±0.0253    │ 0.0616±0.0117       │ 0.8136±0.0127    │ 0.6272±0.0254     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP-V3 │ 0.6442±0.0238    │ 0.0682±0.0107       │ 0.8222±0.0119    │ 0.6443±0.0238     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v1_ind │ 0.6043±0.0401    │ 0.0790±0.0221       │ 0.8025±0.0203    │ 0.6051±0.0406     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v2_ind │ 0.6126±0.0375    │ 0.0922±0.0219       │ 0.8059±0.0189    │ 0.6118±0.0379     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v3_ind │ 0.5515±0.0383    │ 0.0706±0.0180       │ 0.7756±0.0193    │ 0.5512±0.0385     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v4_ind │ 0.6295±0.0338    │ 0.0860±0.0198       │ 0.8148±0.0169    │ 0.6297±0.0338     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v1_ind  │ 0.5654±0.0375    │ 0.0544±0.0119       │ 0.7830±0.0187    │ 0.5661±0.0375     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v2_ind  │ 0.5555±0.0336    │ 0.0559±0.0126       │ 0.7776±0.0166    │ 0.5552±0.0333     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v3_ind  │ 0.5588±0.0290    │ 0.0476±0.0096       │ 0.7795±0.0146    │ 0.5589±0.0291     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v4_ind  │ 0.5301±0.0314    │ 0.0390±0.0084       │ 0.7650±0.0158    │ 0.5301±0.0316     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v1_ind   │ 0.4959±0.0855    │ 0.0242±0.0122       │ 0.7481±0.0434    │ 0.4962±0.0868     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v2_ind   │ 0.5497±0.0334    │ 0.0724±0.0161       │ 0.7748±0.0165    │ 0.5496±0.0331     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v3_ind   │ 0.5439±0.0261    │ 0.0695±0.0146       │ 0.7719±0.0132    │ 0.5439±0.0265     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v4_ind   │ 0.5576±0.0309    │ 0.0789±0.0166       │ 0.7786±0.0156    │ 0.5573±0.0311     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "╘═══════════════╧══════════════════╧═════════════════════╧══════════════════╧═══════════════════╧═══════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "        result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=10,\n",
        "            test_neg_ratio=50)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (mean±std)',\n",
        "    'AUC_PR (mean±std)',\n",
        "    'MRR (mean±std)',\n",
        "    'Hit1 (mean±std)',\n",
        "    'Hit3 (mean±std)',\n",
        "    'Hit10 (mean±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgAnUk4D8hjk8r2dwHu5np"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}