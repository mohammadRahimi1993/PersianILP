{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZw-LPTl7tx"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SfQkfVglkHb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "!pip install numpy==1.26.4\n",
        "clear_output()\n",
        "print(\"Numpy install successful!\")\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpV8iTO6l0i_",
        "outputId": "8e28e89d-0895-47b6-8ff6-560d25dc1feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "DGL installed!\n",
            "PyTorch Version:  2.2.0+cu121\n",
            "TorchMetrics Version:  1.2.1\n",
            "Transformers Version:  4.38.0\n",
            "DGL Version:  2.4.0\n",
            "TorchEval Is:  0.0.7\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "!pip install torchmetrics==1.2.1 transformers==4.38.0\n",
        "!pip install torcheval\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import dgl\n",
        "import torch\n",
        "import torchmetrics\n",
        "import transformers\n",
        "import torcheval\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    import dgl.graphbolt as gb\n",
        "    installed = True\n",
        "except ImportError as error:\n",
        "    installed = False\n",
        "    print(error)\n",
        "\n",
        "print(\"DGL installed!\" if installed else \"DGL not found!\")\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"TorchMetrics Version: \", torchmetrics.__version__)\n",
        "print(\"Transformers Version: \", transformers.__version__)\n",
        "print(\"DGL Version: \", dgl.__version__)\n",
        "print(\"TorchEval Is: \", torcheval.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmPWFxJ0mFbJ"
      },
      "source": [
        "## 1- PersainILP Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAxLwMynCMh"
      },
      "source": [
        "**Extrac Zip File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQj8gFpymRcR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/SPARQL.zip\"\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "excel_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "merged_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGjgfF8m10D"
      },
      "source": [
        "**Save Incomplete Triple**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfQqUUJmtbt",
        "outputId": "0002283f-6c20-4420-ded4-56357dc8cf5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: 206\n",
            "âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù†Ø¯.\n",
            "\n",
            "ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:\n",
            "=================================\n",
            "â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: 2,056,413\n",
            "â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: 3\n",
            "\n",
            "ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± null Ø¯Ø± Ù‡Ø± Ø³ØªÙˆÙ†:\n",
            "subjectLabel      1920360\n",
            "predicateLabel     245845\n",
            "objectLabel        182278\n",
            "dtype: int64\n",
            "\n",
            "â™» ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ: 32,196\n",
            "\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ú©Ø§Ù…Ù„ (9,523 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/complete_triples.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ Ø¯Ùˆ Ù…Ù‚Ø¯Ø§Ø± (21,492 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/triples_with_two_values.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± (1,181 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/triples_with_one_value.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "\n",
            "ğŸ‰ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\n",
            "\n",
            "ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:\n",
            "subjectLabel      19385\n",
            "predicateLabel    23622\n",
            "objectLabel       29727\n",
            "filled_count      32196\n",
            "dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-533447339.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "output_path = \"/content/FarsiBase\"\n",
        "os.makedirs(output_path, exist_ok=True)  # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n",
        "\n",
        "csv_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.csv')]\n",
        "print(f\"ğŸ” ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: {len(csv_files)}\")\n",
        "\n",
        "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "merged_df.to_csv('/content/mergeData', index=False, encoding='utf-8-sig')\n",
        "print(\"âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù†Ø¯.\\n\")\n",
        "\n",
        "print(\"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:\")\n",
        "print(\"=================================\")\n",
        "print(f\"â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(merged_df):,}\")\n",
        "print(f\"â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {len(merged_df.columns)}\")\n",
        "print(\"\\nğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± null Ø¯Ø± Ù‡Ø± Ø³ØªÙˆÙ†:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "def simplify_uri(uri):\n",
        "    if isinstance(uri, str):\n",
        "        return uri.strip().split(\"/\")[-1].split(\"#\")[-1]  # Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† URIÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù\n",
        "    return uri\n",
        "\n",
        "cols_to_simplify = [\"subjectLabel\", \"predicateLabel\", \"objectLabel\"]\n",
        "simplified_df = merged_df.copy()\n",
        "for col in cols_to_simplify:\n",
        "    simplified_df[col] = simplified_df[col].apply(simplify_uri)\n",
        "\n",
        "simplified_df.drop_duplicates(inplace=True)\n",
        "simplified_df = simplified_df.dropna(how='all')\n",
        "print(f\"\\nâ™» ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ: {len(simplified_df):,}\")\n",
        "\n",
        "simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n",
        "\n",
        "complete_df = simplified_df[simplified_df[\"filled_count\"] == 3].drop(columns=[\"filled_count\"])\n",
        "complete_path = os.path.join(output_path, \"complete_triples.csv\")\n",
        "complete_df.to_csv(complete_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\nğŸ’¾ ÙØ§ÛŒÙ„ triples Ú©Ø§Ù…Ù„ ({len(complete_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {complete_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "two_filled_df = simplified_df[simplified_df[\"filled_count\"] == 2].drop(columns=[\"filled_count\"])\n",
        "two_path = os.path.join(output_path, \"triples_with_two_values.csv\")\n",
        "two_filled_df.to_csv(two_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ Ø¯Ùˆ Ù…Ù‚Ø¯Ø§Ø± ({len(two_filled_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {two_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "one_filled_df = simplified_df[simplified_df[\"filled_count\"] == 1].drop(columns=[\"filled_count\"])\n",
        "one_path = os.path.join(output_path, \"triples_with_one_value.csv\")\n",
        "one_filled_df.to_csv(one_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± ({len(one_filled_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {one_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "print(\"\\nğŸ‰ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\")\n",
        "print(f\"\\nğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:\\n{simplified_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaoDYMbnj5_"
      },
      "source": [
        "**FarsiBase Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orB7I9PenrDz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def convert_persian_to_english(number):\n",
        "    persian_to_english = str.maketrans('Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹', '0123456789')\n",
        "    return str(number).translate(persian_to_english)\n",
        "\n",
        "df = pd.read_csv(\"/content/FarsiBase/complete_triples.csv\")\n",
        "for column in ['subjectLabel', 'predicateLabel', 'objectLabel']:\n",
        "    df[column] = df[column].apply(convert_persian_to_english)\n",
        "\n",
        "# Clean Relation\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^dcterms#subject','Ù…ÙˆØ¶ÙˆØ¹/Ù…Ø­ØªÙˆØ§', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^subject','Ù…ÙˆØ¶ÙˆØ¹', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth place','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth_place','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birthPlace','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^instanceOf','Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø²', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^deathPlace','Ù…Ø­Ù„ Ù…Ø±Ú¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^death place','Ù…Ø­Ù„ Ù…Ø±Ú¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^field','Ù…ÙˆØ¶ÙˆØ¹', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^genre','Ú˜Ø§Ù†Ø±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^nationality','Ù…Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^occupation','Ø´ØºÙ„', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^picture','ØªØµÙˆÛŒØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^ActiveYears','Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^activeYears','Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^timezone1 dst','Ù†Ø§Ø­ÛŒÙ‡ Ø²Ù…Ø§Ù†ÛŒ Û±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^confed_cup','Ø¬Ø§Ù… Ú©Ù†ÙØ¯Ø±Ø§Ø³ÛŒÙˆÙ†', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^distance to London (Î¼)','ÙØ§ØµÙ„Ù‡ ØªØ§ Ù„Ù†Ø¯Ù† (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†)', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^fs_date','ØªØ§Ø±ÛŒØ® Ø³ÛŒØ³ØªÙ… ÙØ§ÛŒÙ„', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^paÅ„stwo','Ú©Ø´ÙˆØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^paÅ„stwo','Ú©Ø´ÙˆØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^sp_date','ØªØ§Ø±ÛŒØ® Ø·Ø±Ø­', regex=True)\n",
        "\n",
        "df = df.drop(df[df['predicateLabel'] == '22-rdf-syntax-ns#instanceOf'].index)\n",
        "df = df[~df['objectLabel'].str.contains('relation', case=False, na=False)]\n",
        "df = df[~df['objectLabel'].str.endswith('.JPG')] # Delete Row with .JPG Value\n",
        "df = df[~df['objectLabel'].str.endswith('.jpg')]\n",
        "df = df[~df['objectLabel'].str.endswith('.png')]\n",
        "df = df[~df['objectLabel'].str.endswith('.svg')]\n",
        "df = df[~df['objectLabel'].str.endswith('Pages_using_infobox3cols_with_multidatastyle')]\n",
        "df = df[~df['objectLabel'].str.endswith(':hy:ÕÕµÕ¸Ö‚Õ¦Õ¡Õ¶_Ô³Õ¡Ö€Õ¡Õ£Õ¡Õ·')]\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Actor','Ø¨Ø§Ø²ÛŒÚ¯Ø±', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Ali_Daei','Ø¹Ù„ÛŒ Ø¯Ø§ÛŒÛŒ', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Person','Ø´Ø®Øµ', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^SoccerPlayer','Ø¨Ø§Ø²ÛŒÚ©Ù† Ø³ÙˆÚ©Ø±', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Writer','Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡', regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doaLPtD1oBbq"
      },
      "source": [
        "**Data Shuffling And Aggregation(FarsiBase + Deepseek)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CKXSVFboCLf",
        "outputId": "b5970a82-abf3-4796-8163-d2a40334d2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ø¯Ø± '/content/FarsiBase/shuffled_triple.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate FarsiBase Data And DeepSeek Data\n",
        "DeepSeek_df = pd.read_excel('/content/DeepSeek_Triple.xlsx')\n",
        "input_file = '/content/FarsiBase/complete_triples.csv'\n",
        "FarsiBase_df = pd.read_csv(input_file)\n",
        "df = pd.concat([DeepSeek_df, FarsiBase_df], axis=0)\n",
        "\n",
        "# Shuffled Data\n",
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "output_file = '/content/FarsiBase/shuffled_triple.csv'\n",
        "shuffled_df.to_csv(output_file ,index=False , encoding='utf-8-sig')\n",
        "print(f\"ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ø¯Ø± '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeM5zEOoMN1"
      },
      "source": [
        "**PersianILP Normalizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16eOYAhBoH5n",
        "outputId": "c894f572-257a-4590-b6ff-52b1fd77fd41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± '/content/FarsiBase/triple.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Translate Google\n",
        "def translate_relation(relation, target_lang=\"fa\"):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(relation)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ '{relation}': {e}\")\n",
        "        return relation\n",
        "\n",
        "# Translate Data\n",
        "def normalize_excel(input_path, output_path, use_translation=False):\n",
        "\n",
        "    df = pd.read_csv(input_path)\n",
        "    normalized_relations = []\n",
        "    for relation in df['predicateLabel']:\n",
        "        if pd.isna(relation):\n",
        "            normalized = relation\n",
        "        else:\n",
        "            relation = str(relation)\n",
        "            if use_translation and relation.isascii():\n",
        "              normalized = translate_relation(relation)\n",
        "            else:\n",
        "              normalized = relation\n",
        "        normalized_relations.append(normalized)\n",
        "    df['predicateLabel'] = normalized_relations\n",
        "\n",
        "    # Delete Duplicate Row And Shuffling Data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± '{output_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "input_excel = \"/content/FarsiBase/shuffled_triple.csv\"\n",
        "output_excel = \"/content/FarsiBase/triple.csv\"\n",
        "normalize_excel(input_path=input_excel,\n",
        "                output_path=output_excel,\n",
        "                use_translation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5SXP1vZoY1l",
        "outputId": "6cbe30ac-ae5c-465f-fcab-29c94250f16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡â€ŒØªØ§ÛŒÛŒâ€ŒÙ‡Ø§: 16,306\n",
            "\n",
            "ğŸ“Š Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡:\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: 10,693\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: 432\n",
            "\n",
            "ğŸ“ˆ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§:\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û±: 5,770 (54.0%)\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û²: 2,663 (24.9%)\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û³: 900 (8.4%)\n",
            "\n",
            "ğŸ” Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ú¯Ø±Ø§Ù:\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: 10,693\n",
            "ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: 16,306\n",
            "Ú†Ú¯Ø§Ù„ÛŒ Ú¯Ø±Ø§Ù: 0.000143\n",
            "Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†: 0.9999\n",
            "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: 3.05\n",
            "\n",
            "ğŸ”— Ú¯Ø±Ø§Ù Ø¯Ø§Ø±Ø§ÛŒ 731 Ø¬Ø²Ø¡ Ù†Ø§Ù‡Ù…Ø¨Ù†Ø¯ Ø§Ø³Øª\n",
            "\n",
            "ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯:\n",
            "âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù„ÛŒ (432 Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\n",
            "âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯)\n",
            "âœ— ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ (54.0% Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¯Ø±Ø¬Ù‡ Û± Ø¯Ø§Ø±Ù†Ø¯)\n",
            "\n",
            "ğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:\n",
            "âœ… Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_dataset_for_link_prediction(file_path):\n",
        "\n",
        "    # 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, names=['subject', 'predicate', 'object'])\n",
        "        print(f\"âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡â€ŒØªØ§ÛŒÛŒâ€ŒÙ‡Ø§: {len(df):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡\n",
        "    num_entities = len(set(df['subject']).union(set(df['object'])))\n",
        "    num_relations = len(set(df['predicate']))\n",
        "    print(f\"\\nğŸ“Š Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡:\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {num_entities:,}\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {num_relations:,}\")\n",
        "\n",
        "    # 3. Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø§Ù\n",
        "    G = nx.MultiDiGraph()  # Ú¯Ø±Ø§Ù Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø± Ø¨Ø§ Ø§Ù…Ú©Ø§Ù† Ú†Ù†Ø¯ÛŒÙ† ÛŒØ§Ù„ Ø¨ÛŒÙ† Ú¯Ø±Ù‡â€ŒÙ‡Ø§\n",
        "    for _, row in df.iterrows():\n",
        "        G.add_edge(row['subject'], row['object'], key=row['predicate'])\n",
        "\n",
        "    # 4. ØªØ­Ù„ÛŒÙ„ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§\n",
        "    degrees = dict(G.degree())\n",
        "    degree_counts = Counter(degrees.values())\n",
        "\n",
        "    print(\"\\nğŸ“ˆ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§:\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û±: {degree_counts.get(1, 0):,} ({degree_counts.get(1, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û²: {degree_counts.get(2, 0):,} ({degree_counts.get(2, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û³: {degree_counts.get(3, 0):,} ({degree_counts.get(3, 0)/G.number_of_nodes():.1%})\")\n",
        "\n",
        "    # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù\n",
        "    density = nx.density(G)\n",
        "    sparsity = 1 - density\n",
        "    avg_degree = sum(degrees.values()) / G.number_of_nodes()\n",
        "    print(\"\\nğŸ” Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ú¯Ø±Ø§Ù:\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: {G.number_of_nodes():,}\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: {G.number_of_edges():,}\")\n",
        "    print(f\"Ú†Ú¯Ø§Ù„ÛŒ Ú¯Ø±Ø§Ù: {density:.6f}\")\n",
        "    print(f\"Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†: {sparsity:.4f}\")\n",
        "    print(f\"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: {avg_degree:.2f}\")\n",
        "\n",
        "    # 6. Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„Ø§Øª\n",
        "    if nx.is_weakly_connected(G):\n",
        "        print(\"\\nğŸ”„ Ú¯Ø±Ø§Ù Ø¨Ù‡ ØµÙˆØ±Øª Ø¶Ø¹ÛŒÙ Ù…ØªØµÙ„ Ø§Ø³Øª\")\n",
        "    else:\n",
        "        components = nx.number_weakly_connected_components(G)\n",
        "        print(f\"\\nğŸ”— Ú¯Ø±Ø§Ù Ø¯Ø§Ø±Ø§ÛŒ {components} Ø¬Ø²Ø¡ Ù†Ø§Ù‡Ù…Ø¨Ù†Ø¯ Ø§Ø³Øª\")\n",
        "\n",
        "    # 7. ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯\n",
        "    print(\"\\nğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯:\")\n",
        "\n",
        "    suitability_score = 0\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 1: ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø·\n",
        "    if num_relations > 50:\n",
        "        print(f\"âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù„ÛŒ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "        suitability_score += 2\n",
        "    elif num_relations > 10:\n",
        "        print(f\"âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"âœ— ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ù†Ø§Ú©Ø§ÙÛŒ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 2: Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†\n",
        "    if sparsity > 0.99:\n",
        "        print(\"âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯)\")\n",
        "        suitability_score += 2\n",
        "    elif sparsity > 0.95:\n",
        "        print(\"âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(\"âœ— Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ù†Ø§Ú©Ø§ÙÛŒ\")\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 3: ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡\n",
        "    if degree_counts.get(1, 0) < G.number_of_nodes() * 0.4:\n",
        "        print(\"âœ“ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù…ØªØ¹Ø§Ø¯Ù„\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"âœ— ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ ({degree_counts.get(1, 0)/G.number_of_nodes():.1%} Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¯Ø±Ø¬Ù‡ Û± Ø¯Ø§Ø±Ù†Ø¯)\")\n",
        "\n",
        "    # Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "    print(\"\\nğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:\")\n",
        "    if suitability_score >= 4:\n",
        "        print(\"âœ… Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\")\n",
        "    elif suitability_score >= 2:\n",
        "        print(\"âš ï¸ Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯\")\n",
        "    else:\n",
        "        print(\"âŒ Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª\")\n",
        "\n",
        "# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
        "analyze_dataset_for_link_prediction('/content/FarsiBase/triple.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hd_vtyosTr"
      },
      "source": [
        "**Extracting three variants of the main dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bc9eQG4otIv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.makedirs('/content/PersianILP', exist_ok=True)\n",
        "df = pd.read_csv('/content/FarsiBase/triple.csv')\n",
        "n = len(df)\n",
        "idx1 = int(0.25 * n)\n",
        "idx2 = int(0.60 * n)\n",
        "\n",
        "part1 = df.iloc[:idx1].to_csv('/content/PersianILP/PersianILP_V1.csv', index=False, encoding='utf-8-sig')\n",
        "part2 = df.iloc[idx1:idx2].to_csv('/content/PersianILP/PersianILP_V2.csv', index=False, encoding='utf-8-sig')\n",
        "part3 = df.iloc[idx2:].to_csv('/content/PersianILP/PersianILP_V3.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQEAWiz0ozxH",
        "outputId": "a7771a17-6e5f-454f-9772-12382e662655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ÙØ§ÛŒÙ„ zip Ø¯Ø± Ù…Ø³ÛŒØ± Ø²ÛŒØ± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: /content/PersianILP-trainTest/PersianILP-data.zip\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ØªÙÚ©ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª\n",
        "def split_train_test_for_file(file_path, test_size=0.2):\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    train_entities, test_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    train_entities = set(train_entities)\n",
        "    test_entities = set(test_entities)\n",
        "\n",
        "    train_mask = df.iloc[:, 0].isin(train_entities) & df.iloc[:, 2].isin(train_entities)\n",
        "    test_mask = df.iloc[:, 0].isin(test_entities) & df.iloc[:, 2].isin(test_entities)\n",
        "    train_df = df[train_mask]\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§ÛŒÛŒ\n",
        "file_paths = ['/content/PersianILP/PersianILP_V1.csv',\n",
        "              '/content/PersianILP/PersianILP_V2.csv',\n",
        "              '/content/PersianILP/PersianILP_V3.csv']\n",
        "\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for file_path in file_paths:\n",
        "    base_name = os.path.basename(file_path)\n",
        "    file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "    version_dir = os.path.join(output_dir, file_name)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "    train_data, test_data = split_train_test_for_file(file_path, test_size=0.3)\n",
        "\n",
        "    train_output = os.path.join(version_dir, f'train.csv')\n",
        "    test_output = os.path.join(version_dir, f'test.csv')\n",
        "\n",
        "    train_data.to_csv(train_output, index=False, encoding='utf-8-sig')\n",
        "    test_data.to_csv(test_output, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø²ÛŒÙ¾\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "zip_path = os.path.join(output_dir, 'PersianILP-data.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"ÙØ§ÛŒÙ„ zip Ø¯Ø± Ù…Ø³ÛŒØ± Ø²ÛŒØ± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {zip_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspcHPFBpHRD"
      },
      "source": [
        "**Ù‹Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN6CQqq7pLpu",
        "outputId": "59e57a2e-8b3d-49f6-c2ee-2eed3f3bad00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/Data_InductiveLinkPrediction.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir:str='/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            datasets[dataset_name] = {\n",
        "                \"train\": os.path.join(dataset_path, \"train.txt\"),\n",
        "                \"valid\": os.path.join(dataset_path, \"valid.txt\"),\n",
        "                \"test\":  os.path.join(dataset_path, \"test.txt\")}\n",
        "    return datasets\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/Data_InductiveLinkPrediction')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZymFSOqqnf"
      },
      "source": [
        "**Analysis PersianILP With English BencmarkDataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT-hkQWWqiy1",
        "outputId": "8a7e3b4c-b378-402d-869e-ba20e89bbe38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| Dataset             |   Deg_1 |   Deg_2 |   Deg_3 |   Avg_Degree |   Density |   Sparsity |\n",
            "+=====================+=========+=========+=========+==============+===========+============+\n",
            "| PersianILP_V1_test  |     423 |      50 |      35 |         1.57 |  0.001467 |   0.998533 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V1_train |    1625 |     264 |     107 |         1.72 |  0.000395 |   0.999605 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_test  |     518 |      97 |      32 |         1.51 |  0.001103 |   0.998897 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_train |    2227 |     377 |     107 |         1.93 |  0.000322 |   0.999678 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_test  |     565 |     103 |      43 |         1.68 |  0.001088 |   0.998912 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_train |    2353 |     444 |     122 |         1.97 |  0.000306 |   0.999694 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_test  |     212 |      61 |      11 |         1.31 |  0.002306 |   0.997694 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_train |     187 |     227 |     155 |         3.51 |  0.001905 |   0.998095 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_valid |     223 |      55 |      11 |         1.28 |  0.002207 |   0.997793 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_test  |     573 |     111 |      19 |         1.24 |  0.000876 |   0.999124 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_train |     829 |     684 |     453 |         2.91 |  0.000528 |   0.999472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_valid |     524 |     117 |      16 |         1.24 |  0.000942 |   0.999058 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_test  |     832 |     110 |      21 |         1.24 |  0.000637 |   0.999363 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_train |    2144 |    1619 |     581 |         2.49 |  0.000245 |   0.999755 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_valid |     758 |     106 |       4 |         1.22 |  0.000692 |   0.999308 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_test  |    1793 |     391 |      64 |         1.26 |  0.000277 |   0.999723 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_train |    1424 |    1710 |    1311 |         3.48 |  0.000246 |   0.999754 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_valid |    1731 |     386 |      70 |         1.26 |  0.000287 |   0.999713 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_test   |     226 |      55 |      14 |         1.36 |  0.00227  |   0.99773  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_train  |     376 |     227 |     152 |         3.65 |  0.00167  |   0.99833  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_valid  |     211 |      48 |      17 |         1.44 |  0.00251  |   0.99749  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_test   |     362 |     118 |      41 |         1.7  |  0.001516 |   0.998484 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_train  |     419 |     300 |     263 |         4.99 |  0.001505 |   0.998495 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_valid  |     359 |     116 |      38 |         1.71 |  0.001565 |   0.998435 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_test   |     609 |     213 |      98 |         1.76 |  0.0009   |   0.9991   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_train  |     531 |     354 |     330 |         5.92 |  0.001184 |   0.998816 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_valid  |     587 |     225 |     101 |         1.78 |  0.000916 |   0.999084 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_test   |     817 |     316 |     145 |         2    |  0.0007   |   0.9993   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_train  |     493 |     378 |     312 |         7.68 |  0.001259 |   0.998741 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_valid  |     820 |     314 |     129 |         2    |  0.000705 |   0.999295 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_test    |      67 |      12 |       3 |         2.38 |  0.014343 |   0.985657 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_train   |      12 |      22 |      40 |         7.4  |  0.016528 |   0.983472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_valid   |      66 |      13 |       3 |         2.4  |  0.014487 |   0.985513 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_test    |     316 |      67 |      32 |         1.99 |  0.002088 |   0.997912 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_train   |     968 |     415 |     196 |         4.4  |  0.001054 |   0.998946 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_valid   |     304 |     101 |      25 |         1.89 |  0.001939 |   0.998061 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_test    |     521 |     150 |      60 |         2.03 |  0.001272 |   0.998728 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_train   |    1683 |     705 |     359 |         4.51 |  0.000633 |   0.999367 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_valid   |     532 |     151 |      57 |         1.99 |  0.001225 |   0.998775 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_test    |     381 |      98 |      47 |         2.32 |  0.001845 |   0.998155 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_train   |    1342 |     570 |     272 |         5.06 |  0.000906 |   0.999094 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_valid   |     380 |      93 |      35 |         2.32 |  0.00189  |   0.99811  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_data(file_path):\n",
        "    sep = \",\" if file_path.endswith('.csv') else \"\\t\"\n",
        "    return pd.read_csv(file_path, sep=sep, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "def analyze_graph_metrics(file_path):\n",
        "        df = load_data(file_path)\n",
        "        G = nx.MultiDiGraph()\n",
        "        G.add_edges_from(zip(df[\"head\"], df[\"tail\"], df[\"relation\"]))\n",
        "\n",
        "        degrees = dict(G.degree())\n",
        "        counter = Counter(degrees.values())\n",
        "        avg_deg = sum(degrees.values()) / G.number_of_nodes() if G.number_of_nodes() else 0\n",
        "\n",
        "        return {\n",
        "            \"Deg_1\": counter.get(1, 0),\n",
        "            \"Deg_2\": counter.get(2, 0),\n",
        "            \"Deg_3\": counter.get(3, 0),\n",
        "            \"Avg_Degree\": round(avg_deg, 2),\n",
        "            \"Density\": round(nx.density(G), 6),\n",
        "            \"Sparsity\": round(1 - nx.density(G), 6)\n",
        "        }\n",
        "\n",
        "def process_file(file_path, label):\n",
        "    if os.path.isfile(file_path) and file_path.endswith(('.csv', '.txt')):\n",
        "        metrics = analyze_graph_metrics(file_path)\n",
        "        if metrics:\n",
        "            metrics['Dataset'] = label\n",
        "            return metrics\n",
        "    return None\n",
        "\n",
        "def analyze_all_datasets(all_dirs):\n",
        "    results = []\n",
        "    for base_dir in all_dirs:\n",
        "\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            dataset_name = os.path.basename(root)\n",
        "            for file in files:\n",
        "                path = os.path.join(root, file)\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                label_type = \"CSV\" if ext == '.csv' else \"TXT\"\n",
        "                label = f\"{dataset_name}_{os.path.splitext(file)[0]}\"\n",
        "                result = process_file(path, label)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)[[\"Dataset\", \"Deg_1\", \"Deg_2\", \"Deg_3\", \"Avg_Degree\", \"Density\", \"Sparsity\"]]\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
        "all_dirs = [\n",
        "    \"/content/Data_InductiveLinkPrediction\",\n",
        "    \"/content/PersianILP-trainTest\"]\n",
        "df_result = analyze_all_datasets(all_dirs).sort_values(\"Dataset\")\n",
        "print(tabulate(df_result, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beH7-u5KqyOq"
      },
      "source": [
        "## 2-Inductive Link Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtIzmhRrcr4"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgNVh6Q6rM6K",
        "outputId": "729c59f1-907e-48dd-dd14-6a32679d8f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/ILPDataSet.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir: str = '/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            dataset_files = {\n",
        "                \"train\": None,\n",
        "                \"valid\": None,\n",
        "                \"test\": None}\n",
        "\n",
        "            # Check for both .txt and .csv files\n",
        "            for split in dataset_files.keys():\n",
        "                txt_path = os.path.join(dataset_path, f\"{split}.txt\")\n",
        "                csv_path = os.path.join(dataset_path, f\"{split}.csv\")\n",
        "\n",
        "                if os.path.exists(txt_path):\n",
        "                    dataset_files[split] = txt_path\n",
        "                elif os.path.exists(csv_path):\n",
        "                    dataset_files[split] = csv_path\n",
        "\n",
        "            datasets[dataset_name] = dataset_files\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/ILPDataSet')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61E1j7rlgi0Q",
        "outputId": "f43139ab-2aca-4bba-db77-04b2ff0dafcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| Dataset       |   train_triples |   test_triples |   train_relations |   test_relations |   train_entities |   test_entities |\n",
            "+===============+=================+================+===================+==================+==================+=================+\n",
            "| PersianILP_V1 |            2010 |            352 |               166 |               55 |             2340 |             460 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V2 |            2612 |            571 |               197 |               86 |             2758 |             730 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V3 |            2895 |            766 |               196 |              108 |             3088 |             858 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v1_ind |            1618 |            188 |                 8 |                6 |              922 |             286 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v2_ind |            4011 |            441 |                10 |                9 |             2757 |             710 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v3_ind |            6327 |            605 |                11 |               10 |             5084 |             975 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v4_ind |           12334 |           1429 |                 9 |                9 |             7084 |            2270 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v1_ind  |            1993 |            205 |               142 |               68 |             1093 |             301 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v2_ind  |            4145 |            478 |               172 |              107 |             1660 |             562 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v3_ind  |            7406 |            865 |               183 |              128 |             2501 |             981 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v4_ind  |           11714 |           1424 |               200 |              166 |             3051 |            1427 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v1_ind   |             833 |            100 |                14 |                7 |              225 |              84 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v2_ind   |            4586 |            476 |                79 |               54 |             2086 |             478 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v3_ind   |            8048 |            809 |               122 |               87 |             3566 |             798 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v4_ind   |            7073 |            731 |                61 |               47 |             2795 |             630 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "def analyze_kg_files(base_dir):\n",
        "    \"\"\"ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´â€ŒÚ¯Ø±Ø§Ù Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for dataset in sorted(os.listdir(base_dir)):\n",
        "        dataset_path = os.path.join(base_dir, dataset)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        stats = {'Dataset': dataset}\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for ext in ['.csv', '.txt']:\n",
        "                file_path = os.path.join(dataset_path, f\"{split}{ext}\")\n",
        "                if not os.path.exists(file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¨Ø§ ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ±Ù…Øª\n",
        "                    try:\n",
        "                        # Ø§Ø¨ØªØ¯Ø§ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        df = pd.read_csv(file_path)\n",
        "                        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        if not all(col in df.columns for col in ['head', 'relation', 'tail']):\n",
        "                            df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                           header=None, names=['head', 'relation', 'tail'])\n",
        "                    except:\n",
        "                        # Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                       header=None, names=['head', 'relation', 'tail'])\n",
        "\n",
        "                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡\n",
        "                    stats.update({\n",
        "                        f'{split}_triples': int(len(df)),\n",
        "                        f'{split}_relations': int(df['relation'].nunique()),\n",
        "                        f'{split}_entities': int(pd.concat([df['head'], df['tail']]).nunique())\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        results.append(stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def show_stats(base_dir):\n",
        "    \"\"\"Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ ØµØ­ÛŒØ­\"\"\"\n",
        "    df = analyze_kg_files(base_dir)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Ù‡ÛŒÚ† Ø¯ÛŒØªØ§Ø³Øª Ù…Ø¹ØªØ¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
        "        return\n",
        "\n",
        "    # Ø§Ù†ØªØ®Ø§Ø¨ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
        "    columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± NaN\n",
        "    df = df.dropna(subset=['train_triples'])[columns].sort_values('Dataset')\n",
        "\n",
        "    # Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
        "    df.columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    print(tabulate(\n",
        "        df,\n",
        "        headers='keys',\n",
        "        tablefmt='grid',\n",
        "        showindex=False,\n",
        "        numalign=\"right\",\n",
        "        floatfmt=\".0f\"  # Ù†Ù…Ø§ÛŒØ´ Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª ØµØ­ÛŒØ­\n",
        "    ))\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
        "show_stats(\"/content/ILPDataSet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reb0FyYEri_X"
      },
      "source": [
        "**Create DGL Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gVMYZesrrk3m"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "class PersianDGLDataset(DGLDataset):\n",
        "    def __init__(self, train_file, test_file, seed=42):\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.seed = seed\n",
        "        self.process()\n",
        "        super().__init__(name=\"PersianLinkPrediction\")\n",
        "\n",
        "    def process(self):\n",
        "        # Initialize mappings\n",
        "        self.entity2id = {}\n",
        "        self.relation2id = {}\n",
        "        ent_id, rel_id = 0, 0\n",
        "\n",
        "        # Process training data\n",
        "        train_triples = self._load_and_process_file(self.train_file, ent_id, rel_id)\n",
        "        ent_id, rel_id = len(self.entity2id), len(self.relation2id)\n",
        "\n",
        "        # Process test data (using same mappings)\n",
        "        test_triples = self._load_and_process_file(self.test_file, ent_id, rel_id)\n",
        "\n",
        "        # Build graphs\n",
        "        self.graphs = {\n",
        "            \"train\": self._build_graph(train_triples),\n",
        "            \"test\": self._build_graph(test_triples)\n",
        "        }\n",
        "\n",
        "    def _load_file(self, file_path):\n",
        "        \"\"\"Load file based on its extension\"\"\"\n",
        "        if file_path.endswith('.csv'):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            return pd.read_csv(file_path, sep='\\t', header=None,\n",
        "                             names=['subjectLabel', 'predicateLabel', 'objectLabel'])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Only .csv and .txt files are supported.\")\n",
        "\n",
        "    def _load_and_process_file(self, file_path, ent_id_start, rel_id_start):\n",
        "        \"\"\"Load and process a single file, updating mappings\"\"\"\n",
        "        triples = []\n",
        "        df = self._load_file(file_path)\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row['subjectLabel'], row['predicateLabel'], row['objectLabel']\n",
        "\n",
        "            # Update entity mappings\n",
        "            for ent in [h, t]:\n",
        "                if ent not in self.entity2id:\n",
        "                    self.entity2id[ent] = ent_id_start\n",
        "                    ent_id_start += 1\n",
        "\n",
        "            # Update relation mappings\n",
        "            if r not in self.relation2id:\n",
        "                self.relation2id[r] = rel_id_start\n",
        "                rel_id_start += 1\n",
        "\n",
        "            triples.append((\n",
        "                self.entity2id[h],\n",
        "                self.relation2id[r],\n",
        "                self.entity2id[t]))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _build_graph(self, triples):\n",
        "        \"\"\"Build DGL graph from triples\"\"\"\n",
        "        src, rel, dst = zip(*triples)\n",
        "        src = torch.tensor(src)\n",
        "        dst = torch.tensor(dst)\n",
        "        rel = torch.tensor(rel)\n",
        "\n",
        "        g = dgl.graph((src, dst), num_nodes=len(self.entity2id))\n",
        "        g.edata[\"e_type\"] = rel\n",
        "        g.edata[\"edge_mask\"] = torch.ones(g.num_edges(), dtype=torch.bool)\n",
        "        g.ndata[\"ntype\"] = torch.zeros(g.num_nodes(), dtype=torch.int)\n",
        "        g.ndata[\"feat\"] = torch.randn(g.num_nodes(), 64)\n",
        "        return g\n",
        "\n",
        "    def __getitem__(self, split):\n",
        "        return self.graphs[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "class GraphBatchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, graphs, pos_graphs, neg_graphs):\n",
        "        self.graphs = graphs\n",
        "        self.pos_graphs = pos_graphs\n",
        "        self.neg_graphs = neg_graphs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"graph\": self.graphs[idx],\n",
        "            \"pos_graph\": self.pos_graphs[idx],\n",
        "            \"neg_graph\": self.neg_graphs[idx]}\n",
        "\n",
        "\n",
        "dataset = PersianDGLDataset(train_file = ILP_dataset_paths['PersianILP_V1']['train'],\n",
        "                            test_file = ILP_dataset_paths['PersianILP_V1']['test'])\n",
        "train_g = dataset[\"train\"]\n",
        "test_g = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoK1-UFrpiR"
      },
      "source": [
        "**Generate Positive Graph And Negative Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gsFU2PjVrs5_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dgl\n",
        "import scipy.sparse as sp\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "\n",
        "class GraphNegativeSampler:\n",
        "    def __init__(self, train_graph, test_graph, train_neg_ratio=1.0, test_neg_ratio=1.0):\n",
        "        self.train_graph = train_graph\n",
        "        self.test_graph = test_graph\n",
        "        self.train_neg_ratio = train_neg_ratio\n",
        "        self.test_neg_ratio = test_neg_ratio\n",
        "        self.train_pos_g, self.train_neg_g = self._prepare_graphs(train_graph, train_neg_ratio)\n",
        "        self.test_pos_g, self.test_neg_g = self._prepare_graphs(test_graph, test_neg_ratio)\n",
        "\n",
        "    def _generate_negative_samples(self, graph):\n",
        "        u, v = graph.edges()\n",
        "        adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())),\n",
        "                          shape=(graph.num_nodes(), graph.num_nodes()))\n",
        "        return np.where(1 - adj.todense() - np.eye(graph.num_nodes()) != 0)\n",
        "\n",
        "    def _prepare_graphs(self, graph, ratio):\n",
        "        return ( self._create_positive_graph(graph),\n",
        "                 self._create_negative_graph(graph, ratio))\n",
        "\n",
        "    def _create_positive_graph(self, graph):\n",
        "        g = dgl.graph(graph.edges(), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = graph.edata[\"e_type\"]\n",
        "        g.ndata.update({k: graph.ndata[k] for k in [\"feat\", \"ntype\"]})\n",
        "        return g\n",
        "\n",
        "    def _create_negative_graph(self, graph, ratio):\n",
        "        neg_u, neg_v = self._generate_negative_samples(graph)\n",
        "        num_samples = int(graph.num_edges() * ratio)\n",
        "        replace = len(neg_u) < num_samples\n",
        "        sample_ids = np.random.choice(len(neg_u), num_samples, replace=replace)\n",
        "\n",
        "        g = dgl.graph((neg_u[sample_ids], neg_v[sample_ids]), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = torch.randint(0, graph.edata[\"e_type\"].max().item()+1, (g.num_edges(),))\n",
        "        g.ndata.update({\n",
        "            \"feat\": graph.ndata[\"feat\"],\n",
        "            \"ntype\": torch.ones(graph.num_nodes(), dtype=torch.int)})\n",
        "        return g\n",
        "\n",
        "    @property\n",
        "    def training_graphs(self):\n",
        "        return self.train_pos_g, self.train_neg_g\n",
        "\n",
        "    @property\n",
        "    def test_graphs(self):\n",
        "        return self.test_pos_g, self.test_neg_g\n",
        "\n",
        "# Sampling From Knowladge Graph\n",
        "sampler = GraphNegativeSampler(dataset['train'],\n",
        "                               dataset['test'],\n",
        "                               train_neg_ratio=1,\n",
        "                               test_neg_ratio=1)\n",
        "\n",
        "train_pos, train_neg = sampler.training_graphs\n",
        "test_pos, test_neg = sampler.test_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGioSEvKr24x"
      },
      "source": [
        "**Link Prediction Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rZffOokcr50c"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedGraphSAGE(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats, out_feats, dropout=0.5):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, out_feats, \"mean\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            return g.edata[\"score\"][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRW_iO3fr7Ek"
      },
      "source": [
        "**Train method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sN25QhydsCYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import dgl\n",
        "\n",
        "def train_model(model,\n",
        "                pred,\n",
        "                dataloader,\n",
        "                epochs,\n",
        "                lr=0.01):\n",
        "\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(),\n",
        "                                                 pred.parameters()),\n",
        "                                                 lr=lr)\n",
        "\n",
        "    all_losses = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch_graph = batch[\"graph\"]    # Ú¯Ø±Ø§Ù Ø§ØµÙ„ÛŒ\n",
        "            pos_graph = batch[\"pos_graph\"]  # Ú¯Ø±Ø§Ù Ù…Ø«Ø¨Øª\n",
        "            neg_graph = batch[\"neg_graph\"]  # Ú¯Ø±Ø§Ù Ù…Ù†ÙÛŒ\n",
        "\n",
        "            # Forward pass\n",
        "            h = model(batch_graph, batch_graph.ndata[\"feat\"])\n",
        "            pos_score = pred(pos_graph, h)\n",
        "            neg_score = pred(neg_graph, h)\n",
        "            loss = compute_loss(pos_score,neg_score)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            e = epoch\n",
        "            loss = epoch_loss\n",
        "\n",
        "        all_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEpoch: {e}, Loss: {loss:.4f}\")\n",
        "    return h, all_losses\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxC8E0sWsGhm"
      },
      "source": [
        "**Train And Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N_Mp6D-8sPxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
        "from sklearn import metrics\n",
        "\n",
        "def train_and_evaluate_model(result:dict,\n",
        "                             dataset_name,\n",
        "                             ILP_dataset_paths,\n",
        "                             h_feats=16,\n",
        "                             out_feats=10,\n",
        "                             dropout=0.5,\n",
        "                             epochs=2000,\n",
        "                             lr=0.001,\n",
        "                             train_neg_ratio=10,\n",
        "                             test_neg_ratio=1):\n",
        "\n",
        "    # ===== Step 1: Dataset Preparation =====\n",
        "    graphs = PersianDGLDataset(\n",
        "        train_file=ILP_dataset_paths[dataset_name]['train'],\n",
        "        test_file=ILP_dataset_paths[dataset_name]['test']\n",
        "    )\n",
        "\n",
        "    sampler = GraphNegativeSampler(\n",
        "        graphs['train'], graphs['test'],\n",
        "        train_neg_ratio=train_neg_ratio,\n",
        "        test_neg_ratio=test_neg_ratio\n",
        "    )\n",
        "\n",
        "    train_pos_g, train_neg_g = sampler.training_graphs\n",
        "    test_pos_g, test_neg_g = sampler.test_graphs\n",
        "\n",
        "    train_dataset = GraphBatchDataset([graphs['train']], [train_pos_g], [train_neg_g])\n",
        "    train_loader = GraphDataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    test_dataset = GraphBatchDataset([graphs['test']], [test_pos_g], [test_neg_g])\n",
        "    test_loader = GraphDataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    # ===== Step 2: Training =====\n",
        "    def compute_loss(pos_score, neg_score):\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([\n",
        "            torch.ones(pos_score.shape[0]),\n",
        "            torch.zeros(neg_score.shape[0])\n",
        "        ])\n",
        "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "    in_feats = graphs['train'].ndata['feat'].shape[1]\n",
        "    model = ImprovedGraphSAGE(\n",
        "        in_feats=in_feats,\n",
        "        h_feats=h_feats,\n",
        "        out_feats=out_feats,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    pred = DotPredictor()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        itertools.chain(model.parameters(), pred.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch in train_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            pos_score = pred(batch['pos_graph'], h)\n",
        "            neg_score = pred(batch['neg_graph'], h)\n",
        "            loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ===== Step 3: Evaluation =====\n",
        "    pos_scores, pos_labels = [], []\n",
        "    neg_scores, neg_labels = [], []\n",
        "    hit1_list, hit3_list, hit10_list = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ranks = []\n",
        "        for batch in test_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            score_pos = pred(batch['pos_graph'], h).squeeze()\n",
        "            score_neg = pred(batch['neg_graph'], h).squeeze()\n",
        "\n",
        "            neg_per_pos = len(score_neg) // len(score_pos)\n",
        "            pos_scores += score_pos.tolist() if score_pos.dim() > 0 else [score_pos.item()]\n",
        "            neg_scores += score_neg.tolist() if score_neg.dim() > 0 else [score_neg.item()]\n",
        "            pos_labels += [1] * len(score_pos) if score_pos.dim() > 0 else [1]\n",
        "            neg_labels += [0] * len(score_neg) if score_neg.dim() > 0 else [0]\n",
        "\n",
        "            score_pos_exp = score_pos.view(-1, 1).repeat(1, neg_per_pos).view(-1)\n",
        "            scores = torch.stack([score_pos_exp, score_neg], dim=1)\n",
        "            scores = torch.softmax(scores, dim=1).cpu().numpy()\n",
        "            rank = np.argwhere(np.argsort(scores, axis=1)[:, ::-1] == 0)[:, 1] + 1\n",
        "\n",
        "            ranks += rank.tolist()\n",
        "            hit1_list += [1 if r <= 1 else 0 for r in rank]\n",
        "            hit3_list += [1 if r <= 3 else 0 for r in rank]\n",
        "            hit10_list += [1 if r <= 10 else 0 for r in rank]\n",
        "\n",
        "    # ===== Step 4: Result Metrics =====\n",
        "    result[dataset_name] = {\n",
        "        \"AUC\": metrics.roc_auc_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"AUC_PR\": metrics.average_precision_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"MRR\": np.mean(1.0 / np.array(ranks)).item(),\n",
        "        \"Hit1\": np.mean(hit1_list),\n",
        "        \"Hit3\": np.mean(hit3_list),\n",
        "        \"Hit10\": np.mean(hit10_list)}\n",
        "\n",
        "    return result\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tabulate import tabulate\n",
        "def display_results_table(result_dict):\n",
        "    clear_output()\n",
        "    headers = ['Dataset', 'AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']\n",
        "    rows = []\n",
        "    for name, metrics in result_dict.items():\n",
        "        row = [name] + [metrics[h] for h in headers[1:]]\n",
        "        rows.append(row)\n",
        "    print(\"\\n\" + tabulate(rows,\n",
        "                          headers=headers,\n",
        "                          tablefmt=\"fancy_grid\",\n",
        "                          floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCPemgqsWKU",
        "outputId": "09807e0f-4a0a-45f5-df6b-577814382ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Dataset       â”‚ AUC (meanÂ±std)   â”‚ AUC_PR (meanÂ±std)   â”‚ MRR (meanÂ±std)   â”‚ Hit1 (meanÂ±std)   â”‚ Hit3 (meanÂ±std)   â”‚ Hit10 (meanÂ±std)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ PersianILP_V1 â”‚ 0.8000Â±0.0228    â”‚ 0.8299Â±0.0254       â”‚ 0.9027Â±0.0160    â”‚ 0.8054Â±0.0321     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V2 â”‚ 0.7829Â±0.0142    â”‚ 0.8140Â±0.0147       â”‚ 0.8897Â±0.0101    â”‚ 0.7795Â±0.0202     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V3 â”‚ 0.7639Â±0.0168    â”‚ 0.7948Â±0.0155       â”‚ 0.8808Â±0.0093    â”‚ 0.7617Â±0.0186     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v1_ind â”‚ 0.8269Â±0.0124    â”‚ 0.8470Â±0.0165       â”‚ 0.9144Â±0.0099    â”‚ 0.8287Â±0.0199     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v2_ind â”‚ 0.8396Â±0.0154    â”‚ 0.8640Â±0.0127       â”‚ 0.9227Â±0.0096    â”‚ 0.8454Â±0.0192     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v3_ind â”‚ 0.7872Â±0.0251    â”‚ 0.8239Â±0.0216       â”‚ 0.8929Â±0.0140    â”‚ 0.7858Â±0.0281     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v4_ind â”‚ 0.8331Â±0.0179    â”‚ 0.8553Â±0.0144       â”‚ 0.9171Â±0.0105    â”‚ 0.8341Â±0.0209     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v1_ind  â”‚ 0.7666Â±0.0229    â”‚ 0.8032Â±0.0238       â”‚ 0.8795Â±0.0134    â”‚ 0.7590Â±0.0267     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v2_ind  â”‚ 0.7601Â±0.0249    â”‚ 0.8006Â±0.0200       â”‚ 0.8792Â±0.0146    â”‚ 0.7584Â±0.0292     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v3_ind  â”‚ 0.7365Â±0.0211    â”‚ 0.7774Â±0.0169       â”‚ 0.8689Â±0.0131    â”‚ 0.7378Â±0.0261     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v4_ind  â”‚ 0.7125Â±0.0211    â”‚ 0.7550Â±0.0171       â”‚ 0.8562Â±0.0125    â”‚ 0.7124Â±0.0249     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v1_ind   â”‚ 0.6872Â±0.0459    â”‚ 0.7074Â±0.0250       â”‚ 0.8370Â±0.0200    â”‚ 0.6740Â±0.0400     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v2_ind   â”‚ 0.7250Â±0.0184    â”‚ 0.7854Â±0.0164       â”‚ 0.8607Â±0.0113    â”‚ 0.7214Â±0.0226     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v3_ind   â”‚ 0.7352Â±0.0227    â”‚ 0.7918Â±0.0199       â”‚ 0.8676Â±0.0133    â”‚ 0.7352Â±0.0267     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v4_ind   â”‚ 0.7324Â±0.0161    â”‚ 0.7991Â±0.0072       â”‚ 0.8650Â±0.0096    â”‚ 0.7301Â±0.0191     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 10\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "        result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=1,\n",
        "            test_neg_ratio=1)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (meanÂ±std)',\n",
        "    'AUC_PR (meanÂ±std)',\n",
        "    'MRR (meanÂ±std)',\n",
        "    'Hit1 (meanÂ±std)',\n",
        "    'Hit3 (meanÂ±std)',\n",
        "    'Hit10 (meanÂ±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}Â±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 10 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a1xcYDfssbb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635b0218-f24b-4a78-fc4f-4af64d014765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Dataset       â”‚ AUC (meanÂ±std)   â”‚ AUC_PR (meanÂ±std)   â”‚ MRR (meanÂ±std)   â”‚ Hit1 (meanÂ±std)   â”‚ Hit3 (meanÂ±std)   â”‚ Hit10 (meanÂ±std)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ PersianILP_V1 â”‚ 0.6127Â±0.0305    â”‚ 0.0799Â±0.0164       â”‚ 0.8061Â±0.0152    â”‚ 0.6122Â±0.0304     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V2 â”‚ 0.6151Â±0.0354    â”‚ 0.0756Â±0.0099       â”‚ 0.8077Â±0.0177    â”‚ 0.6154Â±0.0354     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V3 â”‚ 0.5906Â±0.0179    â”‚ 0.0582Â±0.0065       â”‚ 0.7955Â±0.0094    â”‚ 0.5910Â±0.0187     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v1_ind â”‚ 0.6146Â±0.0396    â”‚ 0.0832Â±0.0189       â”‚ 0.8068Â±0.0192    â”‚ 0.6136Â±0.0384     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v2_ind â”‚ 0.6231Â±0.0297    â”‚ 0.0933Â±0.0206       â”‚ 0.8115Â±0.0150    â”‚ 0.6229Â±0.0300     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v3_ind â”‚ 0.5574Â±0.0332    â”‚ 0.0624Â±0.0115       â”‚ 0.7789Â±0.0168    â”‚ 0.5578Â±0.0336     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v4_ind â”‚ 0.6370Â±0.0308    â”‚ 0.0870Â±0.0106       â”‚ 0.8182Â±0.0151    â”‚ 0.6363Â±0.0301     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v1_ind  â”‚ 0.5624Â±0.0306    â”‚ 0.0584Â±0.0156       â”‚ 0.7805Â±0.0154    â”‚ 0.5611Â±0.0307     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v2_ind  â”‚ 0.5544Â±0.0252    â”‚ 0.0631Â±0.0122       â”‚ 0.7769Â±0.0129    â”‚ 0.5538Â±0.0258     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v3_ind  â”‚ 0.5586Â±0.0249    â”‚ 0.0492Â±0.0081       â”‚ 0.7791Â±0.0126    â”‚ 0.5583Â±0.0253     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v4_ind  â”‚ 0.5415Â±0.0231    â”‚ 0.0419Â±0.0067       â”‚ 0.7711Â±0.0118    â”‚ 0.5421Â±0.0236     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v1_ind   â”‚ 0.5062Â±0.0433    â”‚ 0.0231Â±0.0066       â”‚ 0.7534Â±0.0215    â”‚ 0.5068Â±0.0430     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v2_ind   â”‚ 0.5445Â±0.0369    â”‚ 0.0717Â±0.0111       â”‚ 0.7723Â±0.0185    â”‚ 0.5445Â±0.0370     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v3_ind   â”‚ 0.5382Â±0.0362    â”‚ 0.0683Â±0.0152       â”‚ 0.7691Â±0.0180    â”‚ 0.5381Â±0.0361     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v4_ind   â”‚ 0.5573Â±0.0398    â”‚ 0.0795Â±0.0184       â”‚ 0.7788Â±0.0199    â”‚ 0.5576Â±0.0399     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 10\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "        result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=10,\n",
        "            test_neg_ratio=50)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (meanÂ±std)',\n",
        "    'AUC_PR (meanÂ±std)',\n",
        "    'MRR (meanÂ±std)',\n",
        "    'Hit1 (meanÂ±std)',\n",
        "    'Hit3 (meanÂ±std)',\n",
        "    'Hit10 (meanÂ±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}Â±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 10 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}