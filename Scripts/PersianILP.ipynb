{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZw-LPTl7tx"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SfQkfVglkHb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "!pip install numpy==1.26.4\n",
        "clear_output()\n",
        "print(\"Numpy install successful!\")\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpV8iTO6l0i_",
        "outputId": "2c28e040-830d-42d8-fd3e-89bb5ee5e494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "DGL installed!\n",
            "PyTorch Version:  2.2.0+cu121\n",
            "TorchMetrics Version:  1.2.1\n",
            "Transformers Version:  4.38.0\n",
            "DGL Version:  2.4.0\n",
            "TorchEval Is:  0.0.7\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "!pip install torchmetrics==1.2.1 transformers==4.38.0\n",
        "!pip install safetensors==0.4.1\n",
        "!pip install torcheval\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import dgl\n",
        "import torch\n",
        "import torchmetrics\n",
        "import transformers\n",
        "import torcheval\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    import dgl.graphbolt as gb\n",
        "    installed = True\n",
        "except ImportError as error:\n",
        "    installed = False\n",
        "    print(error)\n",
        "\n",
        "print(\"DGL installed!\" if installed else \"DGL not found!\")\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"TorchMetrics Version: \", torchmetrics.__version__)\n",
        "print(\"Transformers Version: \", transformers.__version__)\n",
        "print(\"DGL Version: \", dgl.__version__)\n",
        "print(\"TorchEval Is: \", torcheval.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmPWFxJ0mFbJ"
      },
      "source": [
        "## 1- PersainILP Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAxLwMynCMh"
      },
      "source": [
        "**Extrac Zip File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQj8gFpymRcR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/SPARQL.zip\"\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "excel_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "merged_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGjgfF8m10D"
      },
      "source": [
        "**Save Incomplete Triple**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfQqUUJmtbt",
        "outputId": "5af7b90e-00ea-44a5-c829-a42dc255731f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: 206\n",
            "âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù†Ø¯.\n",
            "\n",
            "ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:\n",
            "=================================\n",
            "â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: 2,056,413\n",
            "â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: 3\n",
            "\n",
            "ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± null Ø¯Ø± Ù‡Ø± Ø³ØªÙˆÙ†:\n",
            "subjectLabel      1920360\n",
            "predicateLabel     245845\n",
            "objectLabel        182278\n",
            "dtype: int64\n",
            "\n",
            "â™» ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ: 32,196\n",
            "\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ú©Ø§Ù…Ù„ (9,523 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/complete_triples.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ Ø¯Ùˆ Ù…Ù‚Ø¯Ø§Ø± (21,492 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/triples_with_two_values.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± (1,181 Ø±Ø¯ÛŒÙ) Ø¯Ø± /content/FarsiBase/triples_with_one_value.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n",
            "\n",
            "ğŸ‰ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\n",
            "\n",
            "ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:\n",
            "subjectLabel      19385\n",
            "predicateLabel    23622\n",
            "objectLabel       29727\n",
            "filled_count      32196\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "output_path = \"/content/FarsiBase\"\n",
        "os.makedirs(output_path, exist_ok=True)  # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n",
        "\n",
        "csv_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.csv')]\n",
        "print(f\"ğŸ” ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: {len(csv_files)}\")\n",
        "\n",
        "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "merged_df.to_csv('/content/mergeData.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"âœ… ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù†Ø¯.\\n\")\n",
        "\n",
        "print(\"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù‡:\")\n",
        "print(\"=================================\")\n",
        "print(f\"â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(merged_df):,}\")\n",
        "print(f\"â¡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {len(merged_df.columns)}\")\n",
        "print(\"\\nğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± null Ø¯Ø± Ù‡Ø± Ø³ØªÙˆÙ†:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "def simplify_uri(uri):\n",
        "    if isinstance(uri, str):\n",
        "        return uri.strip().split(\"/\")[-1].split(\"#\")[-1]  # Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† URIÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù\n",
        "    return uri\n",
        "\n",
        "cols_to_simplify = [\"subjectLabel\", \"predicateLabel\", \"objectLabel\"]\n",
        "simplified_df = merged_df.copy()\n",
        "for col in cols_to_simplify:\n",
        "    simplified_df[col] = simplified_df[col].apply(simplify_uri)\n",
        "\n",
        "simplified_df.drop_duplicates(inplace=True)\n",
        "simplified_df = simplified_df.dropna(how='all')\n",
        "print(f\"\\nâ™» ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ: {len(simplified_df):,}\")\n",
        "\n",
        "simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n",
        "\n",
        "complete_df = simplified_df[simplified_df[\"filled_count\"] == 3].drop(columns=[\"filled_count\"])\n",
        "complete_path = os.path.join(output_path, \"complete_triples.csv\")\n",
        "complete_df.to_csv(complete_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\nğŸ’¾ ÙØ§ÛŒÙ„ triples Ú©Ø§Ù…Ù„ ({len(complete_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {complete_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "two_filled_df = simplified_df[simplified_df[\"filled_count\"] == 2].drop(columns=[\"filled_count\"])\n",
        "two_path = os.path.join(output_path, \"triples_with_two_values.csv\")\n",
        "two_filled_df.to_csv(two_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ Ø¯Ùˆ Ù…Ù‚Ø¯Ø§Ø± ({len(two_filled_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {two_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "one_filled_df = simplified_df[simplified_df[\"filled_count\"] == 1].drop(columns=[\"filled_count\"])\n",
        "one_path = os.path.join(output_path, \"triples_with_one_value.csv\")\n",
        "one_filled_df.to_csv(one_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ triples Ø¨Ø§ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± ({len(one_filled_df):,} Ø±Ø¯ÛŒÙ) Ø¯Ø± {one_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "print(\"\\nğŸ‰ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\")\n",
        "print(f\"\\nğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:\\n{simplified_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaoDYMbnj5_"
      },
      "source": [
        "**FarsiBase Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orB7I9PenrDz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def convert_persian_to_english(number):\n",
        "    persian_to_english = str.maketrans('Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹', '0123456789')\n",
        "    return str(number).translate(persian_to_english)\n",
        "\n",
        "df = pd.read_csv(\"/content/FarsiBase/complete_triples.csv\")\n",
        "for column in ['subjectLabel', 'predicateLabel', 'objectLabel']:\n",
        "    df[column] = df[column].apply(convert_persian_to_english)\n",
        "\n",
        "# Clean Relation\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^dcterms#subject','Ù…ÙˆØ¶ÙˆØ¹/Ù…Ø­ØªÙˆØ§', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^subject','Ù…ÙˆØ¶ÙˆØ¹', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth place','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth_place','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birthPlace','Ù…Ø­Ù„ ØªÙˆÙ„Ø¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^instanceOf','Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø²', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^deathPlace','Ù…Ø­Ù„ Ù…Ø±Ú¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^death place','Ù…Ø­Ù„ Ù…Ø±Ú¯', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^field','Ù…ÙˆØ¶ÙˆØ¹', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^genre','Ú˜Ø§Ù†Ø±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^nationality','Ù…Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^occupation','Ø´ØºÙ„', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^picture','ØªØµÙˆÛŒØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^ActiveYears','Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^activeYears','Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ÛŒØª', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^timezone1 dst','Ù†Ø§Ø­ÛŒÙ‡ Ø²Ù…Ø§Ù†ÛŒ Û±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^confed_cup','Ø¬Ø§Ù… Ú©Ù†ÙØ¯Ø±Ø§Ø³ÛŒÙˆÙ†', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^distance to London (Î¼)','ÙØ§ØµÙ„Ù‡ ØªØ§ Ù„Ù†Ø¯Ù† (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†)', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^fs_date','ØªØ§Ø±ÛŒØ® Ø³ÛŒØ³ØªÙ… ÙØ§ÛŒÙ„', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^paÅ„stwo','Ú©Ø´ÙˆØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^paÅ„stwo','Ú©Ø´ÙˆØ±', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^sp_date','ØªØ§Ø±ÛŒØ® Ø·Ø±Ø­', regex=True)\n",
        "\n",
        "df = df.drop(df[df['predicateLabel'] == '22-rdf-syntax-ns#instanceOf'].index)\n",
        "df = df[~df['objectLabel'].str.contains('relation', case=False, na=False)]\n",
        "df = df[~df['objectLabel'].str.endswith('.JPG')] # Delete Row with .JPG Value\n",
        "df = df[~df['objectLabel'].str.endswith('.jpg')]\n",
        "df = df[~df['objectLabel'].str.endswith('.png')]\n",
        "df = df[~df['objectLabel'].str.endswith('.svg')]\n",
        "df = df[~df['objectLabel'].str.endswith('Pages_using_infobox3cols_with_multidatastyle')]\n",
        "df = df[~df['objectLabel'].str.endswith(':hy:ÕÕµÕ¸Ö‚Õ¦Õ¡Õ¶_Ô³Õ¡Ö€Õ¡Õ£Õ¡Õ·')]\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Actor','Ø¨Ø§Ø²ÛŒÚ¯Ø±', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Ali_Daei','Ø¹Ù„ÛŒ Ø¯Ø§ÛŒÛŒ', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Person','Ø´Ø®Øµ', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^SoccerPlayer','Ø¨Ø§Ø²ÛŒÚ©Ù† Ø³ÙˆÚ©Ø±', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Writer','Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡', regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doaLPtD1oBbq"
      },
      "source": [
        "**Data Shuffling And Aggregation(FarsiBase + Deepseek)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CKXSVFboCLf",
        "outputId": "7e096d18-4421-480d-e5b1-eaa800d5e284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ø¯Ø± '/content/FarsiBase/shuffled_triple.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate FarsiBase Data And DeepSeek Data\n",
        "DeepSeek_df = pd.read_excel('/content/DeepSeek_Triple.xlsx')\n",
        "input_file = '/content/FarsiBase/complete_triples.csv'\n",
        "FarsiBase_df = pd.read_csv(input_file)\n",
        "df = pd.concat([DeepSeek_df, FarsiBase_df], axis=0)\n",
        "\n",
        "# Shuffled Data\n",
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "output_file = '/content/FarsiBase/shuffled_triple.csv'\n",
        "shuffled_df.to_csv(output_file ,index=False , encoding='utf-8-sig')\n",
        "print(f\"ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ù‡ Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ø¯Ø± '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeM5zEOoMN1"
      },
      "source": [
        "**PersianILP Normalizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16eOYAhBoH5n",
        "outputId": "0f7897d0-4c78-48fd-8e2b-0a1a3247758f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± '/content/FarsiBase/triple.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Translate Google\n",
        "def translate_relation(relation, target_lang=\"fa\"):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(relation)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ '{relation}': {e}\")\n",
        "        return relation\n",
        "\n",
        "# Translate Data\n",
        "def normalize_excel(input_path, output_path, use_translation=False):\n",
        "\n",
        "    df = pd.read_csv(input_path)\n",
        "    normalized_relations = []\n",
        "    for relation in df['predicateLabel']:\n",
        "        if pd.isna(relation):\n",
        "            normalized = relation\n",
        "        else:\n",
        "            relation = str(relation)\n",
        "            if use_translation and relation.isascii():\n",
        "              normalized = translate_relation(relation)\n",
        "            else:\n",
        "              normalized = relation\n",
        "        normalized_relations.append(normalized)\n",
        "    df['predicateLabel'] = normalized_relations\n",
        "\n",
        "    # Delete Duplicate Row And Shuffling Data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± '{output_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "input_excel = \"/content/FarsiBase/shuffled_triple.csv\"\n",
        "output_excel = \"/content/FarsiBase/triple.csv\"\n",
        "normalize_excel(input_path=input_excel,\n",
        "                output_path=output_excel,\n",
        "                use_translation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5SXP1vZoY1l",
        "outputId": "372219d1-cc1c-46b8-a21a-b736e2a0ee2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡â€ŒØªØ§ÛŒÛŒâ€ŒÙ‡Ø§: 16,306\n",
            "\n",
            "ğŸ“Š Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡:\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: 10,693\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: 432\n",
            "\n",
            "ğŸ“ˆ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§:\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û±: 5,770 (54.0%)\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û²: 2,663 (24.9%)\n",
            "â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û³: 900 (8.4%)\n",
            "\n",
            "ğŸ” Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ú¯Ø±Ø§Ù:\n",
            "ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: 10,693\n",
            "ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: 16,306\n",
            "Ú†Ú¯Ø§Ù„ÛŒ Ú¯Ø±Ø§Ù: 0.000143\n",
            "Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†: 0.9999\n",
            "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: 3.05\n",
            "\n",
            "ğŸ”— Ú¯Ø±Ø§Ù Ø¯Ø§Ø±Ø§ÛŒ 731 Ø¬Ø²Ø¡ Ù†Ø§Ù‡Ù…Ø¨Ù†Ø¯ Ø§Ø³Øª\n",
            "\n",
            "ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯:\n",
            "âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù„ÛŒ (432 Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\n",
            "âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯)\n",
            "âœ— ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ (54.0% Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¯Ø±Ø¬Ù‡ Û± Ø¯Ø§Ø±Ù†Ø¯)\n",
            "\n",
            "ğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:\n",
            "âœ… Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_dataset_for_link_prediction(file_path):\n",
        "\n",
        "    # 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, names=['subject', 'predicate', 'object'])\n",
        "        print(f\"âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡â€ŒØªØ§ÛŒÛŒâ€ŒÙ‡Ø§: {len(df):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡\n",
        "    num_entities = len(set(df['subject']).union(set(df['object'])))\n",
        "    num_relations = len(set(df['predicate']))\n",
        "    print(f\"\\nğŸ“Š Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡:\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {num_entities:,}\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {num_relations:,}\")\n",
        "\n",
        "    # 3. Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø§Ù\n",
        "    G = nx.MultiDiGraph()  # Ú¯Ø±Ø§Ù Ø¬Ù‡Øªâ€ŒØ¯Ø§Ø± Ø¨Ø§ Ø§Ù…Ú©Ø§Ù† Ú†Ù†Ø¯ÛŒÙ† ÛŒØ§Ù„ Ø¨ÛŒÙ† Ú¯Ø±Ù‡â€ŒÙ‡Ø§\n",
        "    for _, row in df.iterrows():\n",
        "        G.add_edge(row['subject'], row['object'], key=row['predicate'])\n",
        "\n",
        "    # 4. ØªØ­Ù„ÛŒÙ„ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§\n",
        "    degrees = dict(G.degree())\n",
        "    degree_counts = Counter(degrees.values())\n",
        "\n",
        "    print(\"\\nğŸ“ˆ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§:\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û±: {degree_counts.get(1, 0):,} ({degree_counts.get(1, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û²: {degree_counts.get(2, 0):,} ({degree_counts.get(2, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"â€¢ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¯Ø±Ø¬Ù‡ Û³: {degree_counts.get(3, 0):,} ({degree_counts.get(3, 0)/G.number_of_nodes():.1%})\")\n",
        "\n",
        "    # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú¯Ø±Ø§Ù\n",
        "    density = nx.density(G)\n",
        "    sparsity = 1 - density\n",
        "    avg_degree = sum(degrees.values()) / G.number_of_nodes()\n",
        "    print(\"\\nğŸ” Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ú¯Ø±Ø§Ù:\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: {G.number_of_nodes():,}\")\n",
        "    print(f\"ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: {G.number_of_edges():,}\")\n",
        "    print(f\"Ú†Ú¯Ø§Ù„ÛŒ Ú¯Ø±Ø§Ù: {density:.6f}\")\n",
        "    print(f\"Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†: {sparsity:.4f}\")\n",
        "    print(f\"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø±Ø¬Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: {avg_degree:.2f}\")\n",
        "\n",
        "    # 6. Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„Ø§Øª\n",
        "    if nx.is_weakly_connected(G):\n",
        "        print(\"\\nğŸ”„ Ú¯Ø±Ø§Ù Ø¨Ù‡ ØµÙˆØ±Øª Ø¶Ø¹ÛŒÙ Ù…ØªØµÙ„ Ø§Ø³Øª\")\n",
        "    else:\n",
        "        components = nx.number_weakly_connected_components(G)\n",
        "        print(f\"\\nğŸ”— Ú¯Ø±Ø§Ù Ø¯Ø§Ø±Ø§ÛŒ {components} Ø¬Ø²Ø¡ Ù†Ø§Ù‡Ù…Ø¨Ù†Ø¯ Ø§Ø³Øª\")\n",
        "\n",
        "    # 7. ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯\n",
        "    print(\"\\nğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯Ù† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯:\")\n",
        "\n",
        "    suitability_score = 0\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 1: ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø·\n",
        "    if num_relations > 50:\n",
        "        print(f\"âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù„ÛŒ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "        suitability_score += 2\n",
        "    elif num_relations > 10:\n",
        "        print(f\"âœ“ ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"âœ— ØªÙ†ÙˆØ¹ Ø±ÙˆØ§Ø¨Ø· Ù†Ø§Ú©Ø§ÙÛŒ ({num_relations} Ù†ÙˆØ¹ Ø±Ø§Ø¨Ø·Ù‡)\")\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 2: Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù†\n",
        "    if sparsity > 0.99:\n",
        "        print(\"âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯)\")\n",
        "        suitability_score += 2\n",
        "    elif sparsity > 0.95:\n",
        "        print(\"âœ“ Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(\"âœ— Ø§Ø³Ù¾Ø§Ø±Ø³ Ø¨ÙˆØ¯Ù† Ù†Ø§Ú©Ø§ÙÛŒ\")\n",
        "\n",
        "    # Ù…Ø¹ÛŒØ§Ø± 3: ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡\n",
        "    if degree_counts.get(1, 0) < G.number_of_nodes() * 0.4:\n",
        "        print(\"âœ“ ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù…ØªØ¹Ø§Ø¯Ù„\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"âœ— ØªÙˆØ²ÛŒØ¹ Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ù…ØªØ¹Ø§Ø¯Ù„ ({degree_counts.get(1, 0)/G.number_of_nodes():.1%} Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¯Ø±Ø¬Ù‡ Û± Ø¯Ø§Ø±Ù†Ø¯)\")\n",
        "\n",
        "    # Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "    print(\"\\nğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:\")\n",
        "    if suitability_score >= 4:\n",
        "        print(\"âœ… Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\")\n",
        "    elif suitability_score >= 2:\n",
        "        print(\"âš ï¸ Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯\")\n",
        "    else:\n",
        "        print(\"âŒ Ø§ÛŒÙ† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù¾ÛŒÙˆÙ†Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª\")\n",
        "\n",
        "# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
        "analyze_dataset_for_link_prediction('/content/FarsiBase/triple.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hd_vtyosTr"
      },
      "source": [
        "**Extracting three variants of the main dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ØªÙÚ©ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª\n",
        "def split_full_inductive(file_path, test_size=0.2):\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "    train_entities, test_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    train_entities = set(train_entities)\n",
        "    test_entities = set(test_entities)\n",
        "    train_mask = df.iloc[:, 0].isin(train_entities) & df.iloc[:, 2].isin(train_entities)\n",
        "    test_mask = df.iloc[:, 0].isin(test_entities) & df.iloc[:, 2].isin(test_entities)\n",
        "    train_df = df[train_mask]\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# ØªÙÚ©ÛŒÚ© Ø¨Ù‡ ØµÙˆØ±Øª Ù†ÛŒÙ…Ù‡â€ŒØ§Ø³ØªÙ‚Ø±Ø§ÛŒÛŒ\n",
        "def split_semi_inductive(file_path, test_size=0.2):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract all unique entities\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    seen_entities, unseen_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    seen_entities = set(seen_entities)\n",
        "    unseen_entities = set(unseen_entities)\n",
        "\n",
        "    train_mask = df.iloc[:, 0].isin(seen_entities) & df.iloc[:, 2].isin(seen_entities)\n",
        "    train_df = df[train_mask]\n",
        "\n",
        "    test_mask = (\n",
        "        (df.iloc[:, 0].isin(seen_entities) & df.iloc[:, 2].isin(unseen_entities)) |\n",
        "        (df.iloc[:, 0].isin(unseen_entities) & df.iloc[:, 2].isin(seen_entities)))\n",
        "\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# ØªÙÚ©ÛŒÚ© Ø¨Ù‡ ØµÙˆØ±Øª Ø§Ù†ØªÙ‚Ø§Ù„ÛŒ\n",
        "def split_transductive(file_path, test_size=0.2):\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Split triples into train and test\n",
        "    split_index = int(len(df) * (1 - test_size))\n",
        "    train_df = df.iloc[:split_index]\n",
        "    test_df = df.iloc[split_index:]\n",
        "\n",
        "    # Get entities from training set\n",
        "    train_subjects = set(train_df.iloc[:, 0].dropna().unique())\n",
        "    train_objects = set(train_df.iloc[:, 2].dropna().unique())\n",
        "    train_entities = train_subjects.union(train_objects)\n",
        "\n",
        "    # Filter test triples: both subject and object must be in training entities\n",
        "    test_mask = df.iloc[split_index:, 0].isin(train_entities) & df.iloc[split_index:, 2].isin(train_entities)\n",
        "    test_df = df.iloc[split_index:][test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# Ù…Ø³ÛŒØ±Ù‡Ø§\n",
        "file_path = '/content/FarsiBase/triple.csv'\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ Ùˆ ØªÙˆØ§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø·\n",
        "versions = {\n",
        "    'PersianILP_V1': split_full_inductive,      # Full Inductive\n",
        "    'PersianILP_V2': split_semi_inductive,      # Semi Inductive\n",
        "    'PersianILP_V3': split_transductive         # Transductive\n",
        "}\n",
        "\n",
        "# Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ\n",
        "for version_name, split_func in versions.items():\n",
        "    version_dir = os.path.join(output_dir, version_name)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "\n",
        "    train_data, test_data = split_func(file_path, test_size=0.3)\n",
        "\n",
        "    train_data.to_csv(os.path.join(version_dir, 'train.csv'), index=False, encoding='utf-8-sig')\n",
        "    test_data.to_csv(os.path.join(version_dir, 'test.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "# ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
        "zip_path = os.path.join(output_dir, 'PersianILP-data.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"âœ… ÙØ§ÛŒÙ„ zip Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯:\\n{zip_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wai2GoSp3Awq",
        "outputId": "54bb808f-9afa-447a-ad32-b6fa5dc15bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ÙØ§ÛŒÙ„ zip Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯:\n",
            "/content/PersianILP-trainTest/PersianILP-data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def analyze_splits(output_dir):\n",
        "    \"\"\"\n",
        "    Analyzes train/test files in each version directory and prints a summary table.\n",
        "\n",
        "    Parameters:\n",
        "        output_dir (str): Path to the directory containing version folders (e.g., PersianILP_V1, V2, V3).\n",
        "    \"\"\"\n",
        "    summary = []\n",
        "\n",
        "    for version in ['PersianILP_V1', 'PersianILP_V2', 'PersianILP_V3']:\n",
        "        version_dir = os.path.join(output_dir, version)\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            file_path = os.path.join(version_dir, f'{split}.csv')\n",
        "            if not os.path.exists(file_path):\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(file_path)\n",
        "            num_triples = len(df)\n",
        "            entities = set(df.iloc[:, 0].dropna().unique()).union(df.iloc[:, 2].dropna().unique())\n",
        "            num_entities = len(entities)\n",
        "            num_relations = len(df.iloc[:, 1].dropna().unique())\n",
        "\n",
        "            summary.append([\n",
        "                version,\n",
        "                split,\n",
        "                num_triples,\n",
        "                num_entities,\n",
        "                num_relations\n",
        "            ])\n",
        "\n",
        "    headers = ['Version', 'Split', 'Triples', 'Entities', 'Relations']\n",
        "    print(tabulate(summary, headers=headers, tablefmt='grid'))\n",
        "\n",
        "analyze_splits('/content/PersianILP-trainTest')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc007TFj-K6G",
        "outputId": "cc2671c9-e447-4dfc-b70a-2eaad5c2433c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+-----------+------------+-------------+\n",
            "| Version       | Split   |   Triples |   Entities |   Relations |\n",
            "+===============+=========+===========+============+=============+\n",
            "| PersianILP_V1 | train   |      7563 |       5951 |         324 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V1 | test    |      1684 |       1521 |         135 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V2 | train   |      7563 |       5951 |         324 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V2 | test    |      7054 |       6165 |         278 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V3 | train   |     11413 |       8677 |         362 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V3 | test    |      2820 |       2517 |         175 |\n",
            "+---------------+---------+-----------+------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspcHPFBpHRD"
      },
      "source": [
        "**Ù‹Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN6CQqq7pLpu",
        "outputId": "e3890cad-a7cc-494e-edaa-e769bb9289ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/Data_InductiveLinkPrediction.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir:str='/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            datasets[dataset_name] = {\n",
        "                \"train\": os.path.join(dataset_path, \"train.txt\"),\n",
        "                \"valid\": os.path.join(dataset_path, \"valid.txt\"),\n",
        "                \"test\":  os.path.join(dataset_path, \"test.txt\")}\n",
        "    return datasets\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/Data_InductiveLinkPrediction')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZymFSOqqnf"
      },
      "source": [
        "**Analysis PersianILP With English BencmarkDataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT-hkQWWqiy1",
        "outputId": "3ae8b275-2d3a-40ea-95ae-14411ad80890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| Dataset             |   Deg_1 |   Deg_2 |   Deg_3 |   Avg_Degree |   Density |   Sparsity |\n",
            "+=====================+=========+=========+=========+==============+===========+============+\n",
            "| PersianILP_V1_test  |    1029 |     283 |      51 |         2.21 |  0.000727 |   0.999273 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V1_train |    3828 |    1082 |     376 |         2.54 |  0.000213 |   0.999787 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_test  |    4221 |    1002 |     326 |         2.29 |  0.000186 |   0.999814 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_train |    3828 |    1082 |     376 |         2.54 |  0.000213 |   0.999787 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_test  |    1653 |     352 |     167 |         2.24 |  0.000445 |   0.999555 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_train |    5380 |    1790 |     543 |         2.63 |  0.000152 |   0.999848 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_test  |     212 |      61 |      11 |         1.31 |  0.002306 |   0.997694 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_train |     187 |     227 |     155 |         3.51 |  0.001905 |   0.998095 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_valid |     223 |      55 |      11 |         1.28 |  0.002207 |   0.997793 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_test  |     573 |     111 |      19 |         1.24 |  0.000876 |   0.999124 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_train |     829 |     684 |     453 |         2.91 |  0.000528 |   0.999472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_valid |     524 |     117 |      16 |         1.24 |  0.000942 |   0.999058 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_test  |     832 |     110 |      21 |         1.24 |  0.000637 |   0.999363 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_train |    2144 |    1619 |     581 |         2.49 |  0.000245 |   0.999755 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_valid |     758 |     106 |       4 |         1.22 |  0.000692 |   0.999308 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_test  |    1793 |     391 |      64 |         1.26 |  0.000277 |   0.999723 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_train |    1424 |    1710 |    1311 |         3.48 |  0.000246 |   0.999754 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_valid |    1731 |     386 |      70 |         1.26 |  0.000287 |   0.999713 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_test   |     226 |      55 |      14 |         1.36 |  0.00227  |   0.99773  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_train  |     376 |     227 |     152 |         3.65 |  0.00167  |   0.99833  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_valid  |     211 |      48 |      17 |         1.44 |  0.00251  |   0.99749  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_test   |     362 |     118 |      41 |         1.7  |  0.001516 |   0.998484 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_train  |     419 |     300 |     263 |         4.99 |  0.001505 |   0.998495 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_valid  |     359 |     116 |      38 |         1.71 |  0.001565 |   0.998435 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_test   |     609 |     213 |      98 |         1.76 |  0.0009   |   0.9991   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_train  |     531 |     354 |     330 |         5.92 |  0.001184 |   0.998816 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_valid  |     587 |     225 |     101 |         1.78 |  0.000916 |   0.999084 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_test   |     817 |     316 |     145 |         2    |  0.0007   |   0.9993   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_train  |     493 |     378 |     312 |         7.68 |  0.001259 |   0.998741 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_valid  |     820 |     314 |     129 |         2    |  0.000705 |   0.999295 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_test    |      67 |      12 |       3 |         2.38 |  0.014343 |   0.985657 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_train   |      12 |      22 |      40 |         7.4  |  0.016528 |   0.983472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_valid   |      66 |      13 |       3 |         2.4  |  0.014487 |   0.985513 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_test    |     316 |      67 |      32 |         1.99 |  0.002088 |   0.997912 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_train   |     968 |     415 |     196 |         4.4  |  0.001054 |   0.998946 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_valid   |     304 |     101 |      25 |         1.89 |  0.001939 |   0.998061 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_test    |     521 |     150 |      60 |         2.03 |  0.001272 |   0.998728 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_train   |    1683 |     705 |     359 |         4.51 |  0.000633 |   0.999367 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_valid   |     532 |     151 |      57 |         1.99 |  0.001225 |   0.998775 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_test    |     381 |      98 |      47 |         2.32 |  0.001845 |   0.998155 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_train   |    1342 |     570 |     272 |         5.06 |  0.000906 |   0.999094 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_valid   |     380 |      93 |      35 |         2.32 |  0.00189  |   0.99811  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_data(file_path):\n",
        "    sep = \",\" if file_path.endswith('.csv') else \"\\t\"\n",
        "    return pd.read_csv(file_path, sep=sep, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "def analyze_graph_metrics(file_path):\n",
        "        df = load_data(file_path)\n",
        "        G = nx.MultiDiGraph()\n",
        "        G.add_edges_from(zip(df[\"head\"], df[\"tail\"], df[\"relation\"]))\n",
        "\n",
        "        degrees = dict(G.degree())\n",
        "        counter = Counter(degrees.values())\n",
        "        avg_deg = sum(degrees.values()) / G.number_of_nodes() if G.number_of_nodes() else 0\n",
        "\n",
        "        return {\n",
        "            \"Deg_1\": counter.get(1, 0),\n",
        "            \"Deg_2\": counter.get(2, 0),\n",
        "            \"Deg_3\": counter.get(3, 0),\n",
        "            \"Avg_Degree\": round(avg_deg, 2),\n",
        "            \"Density\": round(nx.density(G), 6),\n",
        "            \"Sparsity\": round(1 - nx.density(G), 6)\n",
        "        }\n",
        "\n",
        "def process_file(file_path, label):\n",
        "    if os.path.isfile(file_path) and file_path.endswith(('.csv', '.txt')):\n",
        "        metrics = analyze_graph_metrics(file_path)\n",
        "        if metrics:\n",
        "            metrics['Dataset'] = label\n",
        "            return metrics\n",
        "    return None\n",
        "\n",
        "def analyze_all_datasets(all_dirs):\n",
        "    results = []\n",
        "    for base_dir in all_dirs:\n",
        "\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            dataset_name = os.path.basename(root)\n",
        "            for file in files:\n",
        "                path = os.path.join(root, file)\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                label_type = \"CSV\" if ext == '.csv' else \"TXT\"\n",
        "                label = f\"{dataset_name}_{os.path.splitext(file)[0]}\"\n",
        "                result = process_file(path, label)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)[[\"Dataset\", \"Deg_1\", \"Deg_2\", \"Deg_3\", \"Avg_Degree\", \"Density\", \"Sparsity\"]]\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
        "all_dirs = [ \"/content/Data_InductiveLinkPrediction\",\n",
        "             \"/content/PersianILP-trainTest\"]\n",
        "\n",
        "df_result = analyze_all_datasets(all_dirs).sort_values(\"Dataset\")\n",
        "print(tabulate(df_result, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beH7-u5KqyOq"
      },
      "source": [
        "## 2-Inductive Link Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtIzmhRrcr4"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgNVh6Q6rM6K",
        "outputId": "672d500e-c86b-4119-bff8-2c83911be19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/ILPDataSet.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir: str = '/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            dataset_files = {\n",
        "                \"train\": None,\n",
        "                \"valid\": None,\n",
        "                \"test\": None}\n",
        "\n",
        "            # Check for both .txt and .csv files\n",
        "            for split in dataset_files.keys():\n",
        "                txt_path = os.path.join(dataset_path, f\"{split}.txt\")\n",
        "                csv_path = os.path.join(dataset_path, f\"{split}.csv\")\n",
        "\n",
        "                if os.path.exists(txt_path):\n",
        "                    dataset_files[split] = txt_path\n",
        "                elif os.path.exists(csv_path):\n",
        "                    dataset_files[split] = csv_path\n",
        "\n",
        "            datasets[dataset_name] = dataset_files\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/ILPDataSet')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61E1j7rlgi0Q",
        "outputId": "c7a40052-063d-417e-8326-cc4ee5fe27ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| Dataset       |   train_triples |   test_triples |   train_relations |   test_relations |   train_entities |   test_entities |\n",
            "+===============+=================+================+===================+==================+==================+=================+\n",
            "| PersianILP_V1 |            7564 |           1685 |               325 |              136 |             5953 |            1523 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V2 |            7564 |           7055 |               325 |              279 |             5953 |            6167 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V3 |           11414 |           2821 |               363 |              176 |             8679 |            2519 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v1_ind |            1618 |            188 |                 8 |                6 |              922 |             286 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v2_ind |            4011 |            441 |                10 |                9 |             2757 |             710 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v3_ind |            6327 |            605 |                11 |               10 |             5084 |             975 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v4_ind |           12334 |           1429 |                 9 |                9 |             7084 |            2270 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v1_ind  |            1993 |            205 |               142 |               68 |             1093 |             301 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v2_ind  |            4145 |            478 |               172 |              107 |             1660 |             562 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v3_ind  |            7406 |            865 |               183 |              128 |             2501 |             981 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v4_ind  |           11714 |           1424 |               200 |              166 |             3051 |            1427 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v1_ind   |             833 |            100 |                14 |                7 |              225 |              84 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v2_ind   |            4586 |            476 |                79 |               54 |             2086 |             478 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v3_ind   |            8048 |            809 |               122 |               87 |             3566 |             798 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v4_ind   |            7073 |            731 |                61 |               47 |             2795 |             630 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "def analyze_kg_files(base_dir):\n",
        "    \"\"\"ØªØ­Ù„ÛŒÙ„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´â€ŒÚ¯Ø±Ø§Ù Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for dataset in sorted(os.listdir(base_dir)):\n",
        "        dataset_path = os.path.join(base_dir, dataset)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        stats = {'Dataset': dataset}\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for ext in ['.csv', '.txt']:\n",
        "                file_path = os.path.join(dataset_path, f\"{split}{ext}\")\n",
        "                if not os.path.exists(file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¨Ø§ ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ±Ù…Øª\n",
        "                    try:\n",
        "                        # Ø§Ø¨ØªØ¯Ø§ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        df = pd.read_csv(file_path)\n",
        "                        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        if not all(col in df.columns for col in ['head', 'relation', 'tail']):\n",
        "                            df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                           header=None, names=['head', 'relation', 'tail'])\n",
        "                    except:\n",
        "                        # Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ø± Ø¨Ø®ÙˆØ§Ù†ÛŒÙ…\n",
        "                        df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                       header=None, names=['head', 'relation', 'tail'])\n",
        "\n",
        "                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡\n",
        "                    stats.update({\n",
        "                        f'{split}_triples': int(len(df)),\n",
        "                        f'{split}_relations': int(df['relation'].nunique()),\n",
        "                        f'{split}_entities': int(pd.concat([df['head'], df['tail']]).nunique())\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        results.append(stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def show_stats(base_dir):\n",
        "    \"\"\"Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ ØµØ­ÛŒØ­\"\"\"\n",
        "    df = analyze_kg_files(base_dir)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Ù‡ÛŒÚ† Ø¯ÛŒØªØ§Ø³Øª Ù…Ø¹ØªØ¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
        "        return\n",
        "\n",
        "    # Ø§Ù†ØªØ®Ø§Ø¨ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
        "    columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± NaN\n",
        "    df = df.dropna(subset=['train_triples'])[columns].sort_values('Dataset')\n",
        "\n",
        "    # Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§\n",
        "    df.columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    print(tabulate(\n",
        "        df,\n",
        "        headers='keys',\n",
        "        tablefmt='grid',\n",
        "        showindex=False,\n",
        "        numalign=\"right\",\n",
        "        floatfmt=\".0f\"  # Ù†Ù…Ø§ÛŒØ´ Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª ØµØ­ÛŒØ­\n",
        "    ))\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
        "show_stats(\"/content/ILPDataSet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reb0FyYEri_X"
      },
      "source": [
        "**Create DGL Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVMYZesrrk3m"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "class PersianDGLDataset(DGLDataset):\n",
        "    def __init__(self, train_file, test_file, seed=42):\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.seed = seed\n",
        "        self.process()\n",
        "        super().__init__(name=\"PersianLinkPrediction\")\n",
        "\n",
        "    def process(self):\n",
        "        # Initialize mappings\n",
        "        self.entity2id = {}\n",
        "        self.relation2id = {}\n",
        "        ent_id, rel_id = 0, 0\n",
        "\n",
        "        # Process training data\n",
        "        train_triples = self._load_and_process_file(self.train_file, ent_id, rel_id)\n",
        "        ent_id, rel_id = len(self.entity2id), len(self.relation2id)\n",
        "\n",
        "        # Process test data (using same mappings)\n",
        "        test_triples = self._load_and_process_file(self.test_file, ent_id, rel_id)\n",
        "\n",
        "        # Build graphs\n",
        "        self.graphs = {\n",
        "            \"train\": self._build_graph(train_triples),\n",
        "            \"test\": self._build_graph(test_triples)\n",
        "        }\n",
        "\n",
        "    def _load_file(self, file_path):\n",
        "        \"\"\"Load file based on its extension\"\"\"\n",
        "        if file_path.endswith('.csv'):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            return pd.read_csv(file_path, sep='\\t', header=None,\n",
        "                             names=['subjectLabel', 'predicateLabel', 'objectLabel'])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Only .csv and .txt files are supported.\")\n",
        "\n",
        "    def _load_and_process_file(self, file_path, ent_id_start, rel_id_start):\n",
        "        \"\"\"Load and process a single file, updating mappings\"\"\"\n",
        "        triples = []\n",
        "        df = self._load_file(file_path)\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row['subjectLabel'], row['predicateLabel'], row['objectLabel']\n",
        "\n",
        "            # Update entity mappings\n",
        "            for ent in [h, t]:\n",
        "                if ent not in self.entity2id:\n",
        "                    self.entity2id[ent] = ent_id_start\n",
        "                    ent_id_start += 1\n",
        "\n",
        "            # Update relation mappings\n",
        "            if r not in self.relation2id:\n",
        "                self.relation2id[r] = rel_id_start\n",
        "                rel_id_start += 1\n",
        "\n",
        "            triples.append((\n",
        "                self.entity2id[h],\n",
        "                self.relation2id[r],\n",
        "                self.entity2id[t]))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _build_graph(self, triples):\n",
        "        \"\"\"Build DGL graph from triples\"\"\"\n",
        "        src, rel, dst = zip(*triples)\n",
        "        src = torch.tensor(src)\n",
        "        dst = torch.tensor(dst)\n",
        "        rel = torch.tensor(rel)\n",
        "\n",
        "        g = dgl.graph((src, dst), num_nodes=len(self.entity2id))\n",
        "        g.edata[\"e_type\"] = rel\n",
        "        g.edata[\"edge_mask\"] = torch.ones(g.num_edges(), dtype=torch.bool)\n",
        "        g.ndata[\"ntype\"] = torch.zeros(g.num_nodes(), dtype=torch.int)\n",
        "        g.ndata[\"feat\"] = torch.randn(g.num_nodes(), 64)\n",
        "        return g\n",
        "\n",
        "    def __getitem__(self, split):\n",
        "        return self.graphs[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "class GraphBatchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, graphs, pos_graphs, neg_graphs):\n",
        "        self.graphs = graphs\n",
        "        self.pos_graphs = pos_graphs\n",
        "        self.neg_graphs = neg_graphs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"graph\": self.graphs[idx],\n",
        "            \"pos_graph\": self.pos_graphs[idx],\n",
        "            \"neg_graph\": self.neg_graphs[idx]}\n",
        "\n",
        "\n",
        "dataset = PersianDGLDataset(train_file = ILP_dataset_paths['PersianILP_V1']['train'],\n",
        "                            test_file = ILP_dataset_paths['PersianILP_V1']['test'])\n",
        "train_g = dataset[\"train\"]\n",
        "test_g = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoK1-UFrpiR"
      },
      "source": [
        "**Generate Positive Graph And Negative Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsFU2PjVrs5_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dgl\n",
        "import scipy.sparse as sp\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "\n",
        "class GraphNegativeSampler:\n",
        "    def __init__(self, train_graph, test_graph, train_neg_ratio=1.0, test_neg_ratio=1.0):\n",
        "        self.train_graph = train_graph\n",
        "        self.test_graph = test_graph\n",
        "        self.train_neg_ratio = train_neg_ratio\n",
        "        self.test_neg_ratio = test_neg_ratio\n",
        "        self.train_pos_g, self.train_neg_g = self._prepare_graphs(train_graph, train_neg_ratio)\n",
        "        self.test_pos_g, self.test_neg_g = self._prepare_graphs(test_graph, test_neg_ratio)\n",
        "\n",
        "    def _generate_negative_samples(self, graph):\n",
        "        u, v = graph.edges()\n",
        "        adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())),\n",
        "                          shape=(graph.num_nodes(), graph.num_nodes()))\n",
        "        return np.where(1 - adj.todense() - np.eye(graph.num_nodes()) != 0)\n",
        "\n",
        "    def _prepare_graphs(self, graph, ratio):\n",
        "        return ( self._create_positive_graph(graph),\n",
        "                 self._create_negative_graph(graph, ratio))\n",
        "\n",
        "    def _create_positive_graph(self, graph):\n",
        "        g = dgl.graph(graph.edges(), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = graph.edata[\"e_type\"]\n",
        "        g.ndata.update({k: graph.ndata[k] for k in [\"feat\", \"ntype\"]})\n",
        "        return g\n",
        "\n",
        "    def _create_negative_graph(self, graph, ratio):\n",
        "        neg_u, neg_v = self._generate_negative_samples(graph)\n",
        "        num_samples = int(graph.num_edges() * ratio)\n",
        "        replace = len(neg_u) < num_samples\n",
        "        sample_ids = np.random.choice(len(neg_u), num_samples, replace=replace)\n",
        "\n",
        "        g = dgl.graph((neg_u[sample_ids], neg_v[sample_ids]), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = torch.randint(0, graph.edata[\"e_type\"].max().item()+1, (g.num_edges(),))\n",
        "        g.ndata.update({\n",
        "            \"feat\": graph.ndata[\"feat\"],\n",
        "            \"ntype\": torch.ones(graph.num_nodes(), dtype=torch.int)})\n",
        "        return g\n",
        "\n",
        "    @property\n",
        "    def training_graphs(self):\n",
        "        return self.train_pos_g, self.train_neg_g\n",
        "\n",
        "    @property\n",
        "    def test_graphs(self):\n",
        "        return self.test_pos_g, self.test_neg_g\n",
        "\n",
        "# Sampling From Knowladge Graph\n",
        "sampler = GraphNegativeSampler(dataset['train'],\n",
        "                               dataset['test'],\n",
        "                               train_neg_ratio=1,\n",
        "                               test_neg_ratio=1)\n",
        "\n",
        "train_pos, train_neg = sampler.training_graphs\n",
        "test_pos, test_neg = sampler.test_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGioSEvKr24x"
      },
      "source": [
        "**Link Prediction Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZffOokcr50c"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedGraphSAGE(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats, out_feats, dropout=0.5):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, out_feats, \"mean\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            return g.edata[\"score\"][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRW_iO3fr7Ek"
      },
      "source": [
        "**Train method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN25QhydsCYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import dgl\n",
        "\n",
        "def train_model(model,\n",
        "                pred,\n",
        "                dataloader,\n",
        "                epochs,\n",
        "                lr=0.01):\n",
        "\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(),\n",
        "                                                 pred.parameters()),\n",
        "                                                 lr=lr)\n",
        "\n",
        "    all_losses = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch_graph = batch[\"graph\"]    # Ú¯Ø±Ø§Ù Ø§ØµÙ„ÛŒ\n",
        "            pos_graph = batch[\"pos_graph\"]  # Ú¯Ø±Ø§Ù Ù…Ø«Ø¨Øª\n",
        "            neg_graph = batch[\"neg_graph\"]  # Ú¯Ø±Ø§Ù Ù…Ù†ÙÛŒ\n",
        "\n",
        "            # Forward pass\n",
        "            h = model(batch_graph, batch_graph.ndata[\"feat\"])\n",
        "            pos_score = pred(pos_graph, h)\n",
        "            neg_score = pred(neg_graph, h)\n",
        "            loss = compute_loss(pos_score,neg_score)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            e = epoch\n",
        "            loss = epoch_loss\n",
        "\n",
        "        all_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEpoch: {e}, Loss: {loss:.4f}\")\n",
        "    return h, all_losses\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxC8E0sWsGhm"
      },
      "source": [
        "**Train And Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_Mp6D-8sPxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
        "from sklearn import metrics\n",
        "\n",
        "def train_and_evaluate_model(result:dict,\n",
        "                             dataset_name,\n",
        "                             ILP_dataset_paths,\n",
        "                             h_feats=16,\n",
        "                             out_feats=10,\n",
        "                             dropout=0.5,\n",
        "                             epochs=2000,\n",
        "                             lr=0.001,\n",
        "                             train_neg_ratio=10,\n",
        "                             test_neg_ratio=1):\n",
        "\n",
        "    # ===== Step 1: Dataset Preparation =====\n",
        "    graphs = PersianDGLDataset(\n",
        "        train_file=ILP_dataset_paths[dataset_name]['train'],\n",
        "        test_file=ILP_dataset_paths[dataset_name]['test']\n",
        "    )\n",
        "\n",
        "    sampler = GraphNegativeSampler(\n",
        "        graphs['train'], graphs['test'],\n",
        "        train_neg_ratio=train_neg_ratio,\n",
        "        test_neg_ratio=test_neg_ratio\n",
        "    )\n",
        "\n",
        "    train_pos_g, train_neg_g = sampler.training_graphs\n",
        "    test_pos_g, test_neg_g = sampler.test_graphs\n",
        "\n",
        "    train_dataset = GraphBatchDataset([graphs['train']], [train_pos_g], [train_neg_g])\n",
        "    train_loader = GraphDataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    test_dataset = GraphBatchDataset([graphs['test']], [test_pos_g], [test_neg_g])\n",
        "    test_loader = GraphDataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    # ===== Step 2: Training =====\n",
        "    def compute_loss(pos_score, neg_score):\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([\n",
        "            torch.ones(pos_score.shape[0]),\n",
        "            torch.zeros(neg_score.shape[0])\n",
        "        ])\n",
        "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "    in_feats = graphs['train'].ndata['feat'].shape[1]\n",
        "    model = ImprovedGraphSAGE(\n",
        "        in_feats=in_feats,\n",
        "        h_feats=h_feats,\n",
        "        out_feats=out_feats,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    pred = DotPredictor()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        itertools.chain(model.parameters(), pred.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch in train_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            pos_score = pred(batch['pos_graph'], h)\n",
        "            neg_score = pred(batch['neg_graph'], h)\n",
        "            loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ===== Step 3: Evaluation =====\n",
        "    pos_scores, pos_labels = [], []\n",
        "    neg_scores, neg_labels = [], []\n",
        "    hit1_list, hit3_list, hit10_list = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ranks = []\n",
        "        for batch in test_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            score_pos = pred(batch['pos_graph'], h).squeeze()\n",
        "            score_neg = pred(batch['neg_graph'], h).squeeze()\n",
        "\n",
        "            neg_per_pos = len(score_neg) // len(score_pos)\n",
        "            pos_scores += score_pos.tolist() if score_pos.dim() > 0 else [score_pos.item()]\n",
        "            neg_scores += score_neg.tolist() if score_neg.dim() > 0 else [score_neg.item()]\n",
        "            pos_labels += [1] * len(score_pos) if score_pos.dim() > 0 else [1]\n",
        "            neg_labels += [0] * len(score_neg) if score_neg.dim() > 0 else [0]\n",
        "\n",
        "            score_pos_exp = score_pos.view(-1, 1).repeat(1, neg_per_pos).view(-1)\n",
        "            scores = torch.stack([score_pos_exp, score_neg], dim=1)\n",
        "            scores = torch.softmax(scores, dim=1).cpu().numpy()\n",
        "            rank = np.argwhere(np.argsort(scores, axis=1)[:, ::-1] == 0)[:, 1] + 1\n",
        "\n",
        "            ranks += rank.tolist()\n",
        "            hit1_list += [1 if r <= 1 else 0 for r in rank]\n",
        "            hit3_list += [1 if r <= 3 else 0 for r in rank]\n",
        "            hit10_list += [1 if r <= 10 else 0 for r in rank]\n",
        "\n",
        "    # ===== Step 4: Result Metrics =====\n",
        "    result[dataset_name] = {\n",
        "        \"AUC\": metrics.roc_auc_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"AUC_PR\": metrics.average_precision_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"MRR\": np.mean(1.0 / np.array(ranks)).item(),\n",
        "        \"Hit1\": np.mean(hit1_list),\n",
        "        \"Hit3\": np.mean(hit3_list),\n",
        "        \"Hit10\": np.mean(hit10_list)}\n",
        "\n",
        "    return result\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tabulate import tabulate\n",
        "def display_results_table(result_dict):\n",
        "    clear_output()\n",
        "    headers = ['Dataset', 'AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']\n",
        "    rows = []\n",
        "    for name, metrics in result_dict.items():\n",
        "        row = [name] + [metrics[h] for h in headers[1:]]\n",
        "        rows.append(row)\n",
        "    print(\"\\n\" + tabulate(rows,\n",
        "                          headers=headers,\n",
        "                          tablefmt=\"fancy_grid\",\n",
        "                          floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCPemgqsWKU",
        "outputId": "46678f34-73a5-4436-d67b-b41458e7215c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Dataset       â”‚ AUC (meanÂ±std)   â”‚ AUC_PR (meanÂ±std)   â”‚ MRR (meanÂ±std)   â”‚ Hit1 (meanÂ±std)   â”‚ Hit3 (meanÂ±std)   â”‚ Hit10 (meanÂ±std)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ PersianILP_V1 â”‚ 0.7184Â±0.0217    â”‚ 0.7557Â±0.0181       â”‚ 0.8578Â±0.0107    â”‚ 0.7156Â±0.0214     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V2 â”‚ 0.7122Â±0.0071    â”‚ 0.7064Â±0.0068       â”‚ 0.8566Â±0.0041    â”‚ 0.7132Â±0.0082     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V3 â”‚ 0.6727Â±0.0138    â”‚ 0.6995Â±0.0140       â”‚ 0.8359Â±0.0067    â”‚ 0.6719Â±0.0134     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v1_ind â”‚ 0.8270Â±0.0234    â”‚ 0.8527Â±0.0199       â”‚ 0.9166Â±0.0114    â”‚ 0.8332Â±0.0228     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v2_ind â”‚ 0.8215Â±0.0214    â”‚ 0.8499Â±0.0177       â”‚ 0.9123Â±0.0113    â”‚ 0.8246Â±0.0225     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v3_ind â”‚ 0.7828Â±0.0280    â”‚ 0.8212Â±0.0224       â”‚ 0.8911Â±0.0149    â”‚ 0.7821Â±0.0298     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v4_ind â”‚ 0.8320Â±0.0193    â”‚ 0.8552Â±0.0162       â”‚ 0.9160Â±0.0113    â”‚ 0.8319Â±0.0225     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v1_ind  â”‚ 0.7766Â±0.0258    â”‚ 0.8075Â±0.0259       â”‚ 0.8907Â±0.0153    â”‚ 0.7815Â±0.0306     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v2_ind  â”‚ 0.7620Â±0.0278    â”‚ 0.7994Â±0.0231       â”‚ 0.8802Â±0.0166    â”‚ 0.7604Â±0.0332     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v3_ind  â”‚ 0.7365Â±0.0258    â”‚ 0.7788Â±0.0217       â”‚ 0.8696Â±0.0141    â”‚ 0.7391Â±0.0282     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v4_ind  â”‚ 0.7212Â±0.0187    â”‚ 0.7598Â±0.0167       â”‚ 0.8612Â±0.0101    â”‚ 0.7223Â±0.0201     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v1_ind   â”‚ 0.6486Â±0.0903    â”‚ 0.6626Â±0.0968       â”‚ 0.8245Â±0.0470    â”‚ 0.6490Â±0.0939     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v2_ind   â”‚ 0.7278Â±0.0306    â”‚ 0.7866Â±0.0262       â”‚ 0.8639Â±0.0165    â”‚ 0.7277Â±0.0329     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v3_ind   â”‚ 0.7256Â±0.0183    â”‚ 0.7877Â±0.0163       â”‚ 0.8620Â±0.0098    â”‚ 0.7240Â±0.0196     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v4_ind   â”‚ 0.7327Â±0.0238    â”‚ 0.7990Â±0.0195       â”‚ 0.8655Â±0.0134    â”‚ 0.7310Â±0.0269     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "            result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=1,\n",
        "            test_neg_ratio=1)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (meanÂ±std)',\n",
        "    'AUC_PR (meanÂ±std)',\n",
        "    'MRR (meanÂ±std)',\n",
        "    'Hit1 (meanÂ±std)',\n",
        "    'Hit3 (meanÂ±std)',\n",
        "    'Hit10 (meanÂ±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}Â±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a1xcYDfssbb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8beb173-abf5-48a6-cc23-36e3ee32bd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Dataset       â”‚ AUC (meanÂ±std)   â”‚ AUC_PR (meanÂ±std)   â”‚ MRR (meanÂ±std)   â”‚ Hit1 (meanÂ±std)   â”‚ Hit3 (meanÂ±std)   â”‚ Hit10 (meanÂ±std)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ PersianILP_V1 â”‚ 0.5608Â±0.0251    â”‚ 0.0608Â±0.0078       â”‚ 0.7804Â±0.0125    â”‚ 0.5608Â±0.0251     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V2 â”‚ 0.5773Â±0.0118    â”‚ 0.0267Â±0.0018       â”‚ 0.7887Â±0.0059    â”‚ 0.5774Â±0.0118     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PersianILP_V3 â”‚ 0.5356Â±0.0170    â”‚ 0.0338Â±0.0028       â”‚ 0.7678Â±0.0084    â”‚ 0.5356Â±0.0168     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v1_ind â”‚ 0.5943Â±0.0380    â”‚ 0.0731Â±0.0178       â”‚ 0.7968Â±0.0194    â”‚ 0.5937Â±0.0388     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v2_ind â”‚ 0.6250Â±0.0429    â”‚ 0.0991Â±0.0210       â”‚ 0.8124Â±0.0214    â”‚ 0.6249Â±0.0427     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v3_ind â”‚ 0.5589Â±0.0350    â”‚ 0.0680Â±0.0146       â”‚ 0.7794Â±0.0173    â”‚ 0.5588Â±0.0346     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ WN18RR_v4_ind â”‚ 0.6098Â±0.0242    â”‚ 0.0773Â±0.0102       â”‚ 0.8048Â±0.0121    â”‚ 0.6096Â±0.0243     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v1_ind  â”‚ 0.5752Â±0.0350    â”‚ 0.0597Â±0.0172       â”‚ 0.7878Â±0.0173    â”‚ 0.5756Â±0.0346     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v2_ind  â”‚ 0.5631Â±0.0364    â”‚ 0.0613Â±0.0164       â”‚ 0.7817Â±0.0183    â”‚ 0.5633Â±0.0366     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v3_ind  â”‚ 0.5504Â±0.0222    â”‚ 0.0488Â±0.0056       â”‚ 0.7752Â±0.0114    â”‚ 0.5505Â±0.0227     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ fb237_v4_ind  â”‚ 0.5343Â±0.0230    â”‚ 0.0399Â±0.0053       â”‚ 0.7671Â±0.0115    â”‚ 0.5343Â±0.0230     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v1_ind   â”‚ 0.4788Â±0.0904    â”‚ 0.0227Â±0.0083       â”‚ 0.7393Â±0.0456    â”‚ 0.4785Â±0.0912     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v2_ind   â”‚ 0.5505Â±0.0232    â”‚ 0.0689Â±0.0128       â”‚ 0.7753Â±0.0114    â”‚ 0.5506Â±0.0229     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v3_ind   â”‚ 0.5369Â±0.0350    â”‚ 0.0654Â±0.0171       â”‚ 0.7684Â±0.0175    â”‚ 0.5368Â±0.0350     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ nell_v4_ind   â”‚ 0.5599Â±0.0291    â”‚ 0.0756Â±0.0159       â”‚ 0.7801Â±0.0147    â”‚ 0.5602Â±0.0294     â”‚ 1.0000Â±0.0000     â”‚ 1.0000Â±0.0000      â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "        result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=10,\n",
        "            test_neg_ratio=50)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (meanÂ±std)',\n",
        "    'AUC_PR (meanÂ±std)',\n",
        "    'MRR (meanÂ±std)',\n",
        "    'Hit1 (meanÂ±std)',\n",
        "    'Hit3 (meanÂ±std)',\n",
        "    'Hit10 (meanÂ±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}Â±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOzNcv9wjzDwNWmBlt4IYI"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}