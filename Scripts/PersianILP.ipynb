{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPqm4jDrjgu9/7X2ibgU89O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadRahimi1993/PersianILP/blob/main/Scripts/PersianILP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Library"
      ],
      "metadata": {
        "id": "5gZw-LPTl7tx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SfQkfVglkHb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "!pip install numpy==1.26.4\n",
        "clear_output()\n",
        "print(\"Numpy install successful!\")\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "!pip install torchmetrics==1.2.1 transformers==4.38.0\n",
        "!pip install torcheval\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import dgl\n",
        "import torch\n",
        "import torchmetrics\n",
        "import transformers\n",
        "import torcheval\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    import dgl.graphbolt as gb\n",
        "    installed = True\n",
        "except ImportError as error:\n",
        "    installed = False\n",
        "    print(error)\n",
        "\n",
        "print(\"DGL installed!\" if installed else \"DGL not found!\")\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"TorchMetrics Version: \", torchmetrics.__version__)\n",
        "print(\"Transformers Version: \", transformers.__version__)\n",
        "print(\"DGL Version: \", dgl.__version__)\n",
        "print(\"TorchEval Is: \", torcheval.__version__)"
      ],
      "metadata": {
        "id": "mpV8iTO6l0i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- PersainILP Dataset"
      ],
      "metadata": {
        "id": "AmPWFxJ0mFbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extrac Zip File**"
      ],
      "metadata": {
        "id": "amAxLwMynCMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/SPARQL.zip\"\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "excel_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "merged_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "rQj8gFpymRcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Incomplete Triple**"
      ],
      "metadata": {
        "id": "2WGjgfF8m10D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "output_path = \"/content/FarsiBase\"\n",
        "os.makedirs(output_path, exist_ok=True)  # ایجاد پوشه خروجی اگر وجود نداشته باشد\n",
        "\n",
        "csv_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.csv')]\n",
        "print(f\"🔎 تعداد فایل‌های CSV یافت‌شده: {len(csv_files)}\")\n",
        "\n",
        "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "merged_df.to_csv('/content/mergeData', index=False, encoding='utf-8-sig')\n",
        "print(\"✅ تمام فایل‌ها با موفقیت ادغام شدند.\\n\")\n",
        "\n",
        "print(\"📊 گزارش کامل دیتافریم ادغام شده:\")\n",
        "print(\"=================================\")\n",
        "print(f\"➡ تعداد کل ردیف‌ها: {len(merged_df):,}\")\n",
        "print(f\"➡ تعداد کل ستون‌ها: {len(merged_df.columns)}\")\n",
        "print(\"\\n🔹 تعداد مقادیر null در هر ستون:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "def simplify_uri(uri):\n",
        "    if isinstance(uri, str):\n",
        "        return uri.strip().split(\"/\")[-1].split(\"#\")[-1]  # بهبود برای هندل کردن URIهای مختلف\n",
        "    return uri\n",
        "\n",
        "cols_to_simplify = [\"subjectLabel\", \"predicateLabel\", \"objectLabel\"]\n",
        "simplified_df = merged_df.copy()\n",
        "for col in cols_to_simplify:\n",
        "    simplified_df[col] = simplified_df[col].apply(simplify_uri)\n",
        "\n",
        "simplified_df.drop_duplicates(inplace=True)\n",
        "simplified_df = simplified_df.dropna(how='all')\n",
        "print(f\"\\n♻ تعداد ردیف‌ها پس از حذف موارد تکراری: {len(simplified_df):,}\")\n",
        "\n",
        "simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n",
        "\n",
        "complete_df = simplified_df[simplified_df[\"filled_count\"] == 3].drop(columns=[\"filled_count\"])\n",
        "complete_path = os.path.join(output_path, \"complete_triples.csv\")\n",
        "complete_df.to_csv(complete_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n💾 فایل triples کامل ({len(complete_df):,} ردیف) در {complete_path} ذخیره شد.\")\n",
        "\n",
        "two_filled_df = simplified_df[simplified_df[\"filled_count\"] == 2].drop(columns=[\"filled_count\"])\n",
        "two_path = os.path.join(output_path, \"triples_with_two_values.csv\")\n",
        "two_filled_df.to_csv(two_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"💾 فایل triples با دو مقدار ({len(two_filled_df):,} ردیف) در {two_path} ذخیره شد.\")\n",
        "\n",
        "one_filled_df = simplified_df[simplified_df[\"filled_count\"] == 1].drop(columns=[\"filled_count\"])\n",
        "one_path = os.path.join(output_path, \"triples_with_one_value.csv\")\n",
        "one_filled_df.to_csv(one_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"💾 فایل triples با یک مقدار ({len(one_filled_df):,} ردیف) در {one_path} ذخیره شد.\")\n",
        "\n",
        "print(\"\\n🎉 پردازش با موفقیت به پایان رسید!\")\n",
        "print(f\"\\n📝 گزارش نهایی:\\n{simplified_df.count()}\")"
      ],
      "metadata": {
        "id": "ZAfQqUUJmtbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FarsiBase Data Cleaning**"
      ],
      "metadata": {
        "id": "fjaoDYMbnj5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def convert_persian_to_english(number):\n",
        "    persian_to_english = str.maketrans('۰۱۲۳۴۵۶۷۸۹', '0123456789')\n",
        "    return str(number).translate(persian_to_english)\n",
        "\n",
        "df = pd.read_csv(\"/content/FarsiBase/complete_triples.csv\")\n",
        "for column in ['subjectLabel', 'predicateLabel', 'objectLabel']:\n",
        "    df[column] = df[column].apply(convert_persian_to_english)\n",
        "\n",
        "# Clean Relation\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^dcterms#subject','موضوع/محتوا', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^subject','موضوع', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth place','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth_place','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birthPlace','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^instanceOf','نمونه‌ای از', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^deathPlace','محل مرگ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^death place','محل مرگ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^field','موضوع', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^genre','ژانر', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^nationality','ملیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^occupation','شغل', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^picture','تصویر', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^ActiveYears','سال‌های فعالیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^activeYears','سال‌های فعالیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^timezone1 dst','ناحیه زمانی ۱', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^confed_cup','جام کنفدراسیون', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^distance to London (μ)','فاصله تا لندن (میانگین)', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^fs_date','تاریخ سیستم فایل', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^państwo','کشور', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^państwo','کشور', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^sp_date','تاریخ طرح', regex=True)\n",
        "\n",
        "df = df.drop(df[df['predicateLabel'] == '22-rdf-syntax-ns#instanceOf'].index)\n",
        "df = df[~df['objectLabel'].str.contains('relation', case=False, na=False)]\n",
        "df = df[~df['objectLabel'].str.endswith('.JPG')] # Delete Row with .JPG Value\n",
        "df = df[~df['objectLabel'].str.endswith('.jpg')]\n",
        "df = df[~df['objectLabel'].str.endswith('.png')]\n",
        "df = df[~df['objectLabel'].str.endswith('.svg')]\n",
        "df = df[~df['objectLabel'].str.endswith('Pages_using_infobox3cols_with_multidatastyle')]\n",
        "df = df[~df['objectLabel'].str.endswith(':hy:Սյուզան_Գարագաշ')]\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Actor','بازیگر', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Ali_Daei','علی دایی', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Person','شخص', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^SoccerPlayer','بازیکن سوکر', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Writer','نویسنده', regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "orB7I9PenrDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Shuffling And Aggregation(FarsiBase + Deepseek)**"
      ],
      "metadata": {
        "id": "doaLPtD1oBbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate FarsiBase Data And DeepSeek Data\n",
        "DeepSeek_df = pd.read_excel('/content/DeepSeek_Triple.xlsx')\n",
        "input_file = '/content/FarsiBase/complete_triples.csv'\n",
        "FarsiBase_df = pd.read_csv(input_file)\n",
        "df = pd.concat([DeepSeek_df, FarsiBase_df], axis=0)\n",
        "\n",
        "# Shuffled Data\n",
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "output_file = '/content/FarsiBase/shuffled_triple.csv'\n",
        "shuffled_df.to_csv(output_file ,index=False , encoding='utf-8-sig')\n",
        "print(f\"فایل اکسل به صورت تصادفی به هم ریخته و در '{output_file}' ذخیره شد.\")"
      ],
      "metadata": {
        "id": "9CKXSVFboCLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PersianILP Normalizing**"
      ],
      "metadata": {
        "id": "wdeM5zEOoMN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Translate Google\n",
        "def translate_relation(relation, target_lang=\"fa\"):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(relation)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"خطا در ترجمه '{relation}': {e}\")\n",
        "        return relation\n",
        "\n",
        "# Translate Data\n",
        "def normalize_excel(input_path, output_path, use_translation=False):\n",
        "\n",
        "    df = pd.read_csv(input_path)\n",
        "    normalized_relations = []\n",
        "    for relation in df['predicateLabel']:\n",
        "        if pd.isna(relation):\n",
        "            normalized = relation\n",
        "        else:\n",
        "            relation = str(relation)\n",
        "            if use_translation and relation.isascii():\n",
        "              normalized = translate_relation(relation)\n",
        "            else:\n",
        "              normalized = relation\n",
        "        normalized_relations.append(normalized)\n",
        "    df['predicateLabel'] = normalized_relations\n",
        "\n",
        "    # Delete Duplicate Row And Shuffling Data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"داده‌های نرمال‌سازی شده در '{output_path}' ذخیره شد.\")\n",
        "\n",
        "input_excel = \"/content/FarsiBase/shuffled_triple.csv\"\n",
        "output_excel = \"/content/FarsiBase/triple.csv\"\n",
        "normalize_excel(input_path=input_excel,\n",
        "                output_path=output_excel,\n",
        "                use_translation=True)"
      ],
      "metadata": {
        "id": "16eOYAhBoH5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_dataset_for_link_prediction(file_path):\n",
        "\n",
        "    # 1. خواندن داده‌ها\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, names=['subject', 'predicate', 'object'])\n",
        "        print(f\"✅ دیتاست با موفقیت خوانده شد. تعداد سه‌تایی‌ها: {len(df):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ خطا در خواندن فایل: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. محاسبه معیارهای پایه\n",
        "    num_entities = len(set(df['subject']).union(set(df['object'])))\n",
        "    num_relations = len(set(df['predicate']))\n",
        "    print(f\"\\n📊 آماره‌های پایه:\")\n",
        "    print(f\"تعداد موجودیت‌های منحصر به فرد: {num_entities:,}\")\n",
        "    print(f\"تعداد روابط منحصر به فرد: {num_relations:,}\")\n",
        "\n",
        "    # 3. ایجاد گراف\n",
        "    G = nx.MultiDiGraph()  # گراف جهت‌دار با امکان چندین یال بین گره‌ها\n",
        "    for _, row in df.iterrows():\n",
        "        G.add_edge(row['subject'], row['object'], key=row['predicate'])\n",
        "\n",
        "    # 4. تحلیل درجه گره‌ها\n",
        "    degrees = dict(G.degree())\n",
        "    degree_counts = Counter(degrees.values())\n",
        "\n",
        "    print(\"\\n📈 توزیع درجه گره‌ها:\")\n",
        "    print(f\"• گره‌های با درجه ۱: {degree_counts.get(1, 0):,} ({degree_counts.get(1, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"• گره‌های با درجه ۲: {degree_counts.get(2, 0):,} ({degree_counts.get(2, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"• گره‌های با درجه ۳: {degree_counts.get(3, 0):,} ({degree_counts.get(3, 0)/G.number_of_nodes():.1%})\")\n",
        "\n",
        "    # 5. محاسبه معیارهای گراف\n",
        "    density = nx.density(G)\n",
        "    sparsity = 1 - density\n",
        "    avg_degree = sum(degrees.values()) / G.number_of_nodes()\n",
        "    print(\"\\n🔍 معیارهای ساختاری گراف:\")\n",
        "    print(f\"تعداد گره‌ها: {G.number_of_nodes():,}\")\n",
        "    print(f\"تعداد یال‌ها: {G.number_of_edges():,}\")\n",
        "    print(f\"چگالی گراف: {density:.6f}\")\n",
        "    print(f\"اسپارس بودن: {sparsity:.4f}\")\n",
        "    print(f\"میانگین درجه گره‌ها: {avg_degree:.2f}\")\n",
        "\n",
        "    # 6. بررسی اتصالات\n",
        "    if nx.is_weakly_connected(G):\n",
        "        print(\"\\n🔄 گراف به صورت ضعیف متصل است\")\n",
        "    else:\n",
        "        components = nx.number_weakly_connected_components(G)\n",
        "        print(f\"\\n🔗 گراف دارای {components} جزء ناهمبند است\")\n",
        "\n",
        "    # 7. تحلیل نهایی برای پیش‌بینی پیوند\n",
        "    print(\"\\n🧪 ارزیابی مناسب بودن برای پیش‌بینی پیوند:\")\n",
        "\n",
        "    suitability_score = 0\n",
        "\n",
        "    # معیار 1: تنوع روابط\n",
        "    if num_relations > 50:\n",
        "        print(f\"✓ تنوع روابط عالی ({num_relations} نوع رابطه)\")\n",
        "        suitability_score += 2\n",
        "    elif num_relations > 10:\n",
        "        print(f\"✓ تنوع روابط قابل قبول ({num_relations} نوع رابطه)\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"✗ تنوع روابط ناکافی ({num_relations} نوع رابطه)\")\n",
        "\n",
        "    # معیار 2: اسپارس بودن\n",
        "    if sparsity > 0.99:\n",
        "        print(\"✓ اسپارس بودن ایده‌آل (بسیار مناسب برای پیش‌بینی پیوند)\")\n",
        "        suitability_score += 2\n",
        "    elif sparsity > 0.95:\n",
        "        print(\"✓ اسپارس بودن قابل قبول\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(\"✗ اسپارس بودن ناکافی\")\n",
        "\n",
        "    # معیار 3: توزیع درجه\n",
        "    if degree_counts.get(1, 0) < G.number_of_nodes() * 0.4:\n",
        "        print(\"✓ توزیع درجه متعادل\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"✗ توزیع درجه نامتعادل ({degree_counts.get(1, 0)/G.number_of_nodes():.1%} گره‌ها درجه ۱ دارند)\")\n",
        "\n",
        "    # نتیجه‌گیری نهایی\n",
        "    print(\"\\n🎯 نتیجه‌گیری نهایی:\")\n",
        "    if suitability_score >= 4:\n",
        "        print(\"✅ این دیتاست برای پیش‌بینی پیوند بسیار مناسب است\")\n",
        "    elif suitability_score >= 2:\n",
        "        print(\"⚠️ این دیتاست برای پیش‌بینی پیوند نیاز به بهبودهایی دارد\")\n",
        "    else:\n",
        "        print(\"❌ این دیتاست برای پیش‌بینی پیوند مناسب نیست\")\n",
        "\n",
        "# نمونه استفاده\n",
        "analyze_dataset_for_link_prediction('/content/FarsiBase/triple.csv')"
      ],
      "metadata": {
        "id": "m5SXP1vZoY1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting three variants of the main dataset**"
      ],
      "metadata": {
        "id": "l2hd_vtyosTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.makedirs('/content/PersianILP', exist_ok=True)\n",
        "df = pd.read_csv('/content/FarsiBase/triple.csv')\n",
        "n = len(df)\n",
        "idx1 = int(0.25 * n)\n",
        "idx2 = int(0.60 * n)\n",
        "\n",
        "part1 = df.iloc[:idx1].to_csv('/content/PersianILP/PersianILP_V1.csv', index=False, encoding='utf-8-sig')\n",
        "part2 = df.iloc[idx1:idx2].to_csv('/content/PersianILP/PersianILP_V2.csv', index=False, encoding='utf-8-sig')\n",
        "part3 = df.iloc[idx2:].to_csv('/content/PersianILP/PersianILP_V3.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "8bc9eQG4otIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# تفکیک مجموعه‌ی آموزش و تست\n",
        "def split_train_test_for_file(file_path, test_size=0.2):\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    train_entities, test_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    train_entities = set(train_entities)\n",
        "    test_entities = set(test_entities)\n",
        "\n",
        "    train_mask = df.iloc[:, 0].isin(train_entities) & df.iloc[:, 2].isin(train_entities)\n",
        "    test_mask = df.iloc[:, 0].isin(test_entities) & df.iloc[:, 2].isin(test_entities)\n",
        "    train_df = df[train_mask]\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# ایجاد مجموعه داده‌ی فارسی و انگلیسی استقرایی\n",
        "file_paths = ['/content/PersianILP/PersianILP_V1.csv',\n",
        "              '/content/PersianILP/PersianILP_V2.csv',\n",
        "              '/content/PersianILP/PersianILP_V3.csv']\n",
        "\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for file_path in file_paths:\n",
        "    base_name = os.path.basename(file_path)\n",
        "    file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "    version_dir = os.path.join(output_dir, file_name)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "    train_data, test_data = split_train_test_for_file(file_path, test_size=0.3)\n",
        "\n",
        "    train_output = os.path.join(version_dir, f'train.csv')\n",
        "    test_output = os.path.join(version_dir, f'test.csv')\n",
        "\n",
        "    train_data.to_csv(train_output, index=False, encoding='utf-8-sig')\n",
        "    test_data.to_csv(test_output, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# ایجاد فایل زیپ\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "zip_path = os.path.join(output_dir, 'PersianILP-data.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"فایل zip در مسیر زیر ایجاد شد: {zip_path}\")"
      ],
      "metadata": {
        "id": "QQEAWiz0ozxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ًImport Dataset**"
      ],
      "metadata": {
        "id": "nspcHPFBpHRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/Data_InductiveLinkPrediction.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir:str='/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            datasets[dataset_name] = {\n",
        "                \"train\": os.path.join(dataset_path, \"train.txt\"),\n",
        "                \"valid\": os.path.join(dataset_path, \"valid.txt\"),\n",
        "                \"test\":  os.path.join(dataset_path, \"test.txt\")}\n",
        "    return datasets\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/Data_InductiveLinkPrediction')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ],
      "metadata": {
        "id": "AN6CQqq7pLpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis PersianILP With English BencmarkDataset**"
      ],
      "metadata": {
        "id": "AlZymFSOqqnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_data(file_path):\n",
        "    sep = \",\" if file_path.endswith('.csv') else \"\\t\"\n",
        "    return pd.read_csv(file_path, sep=sep, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "def analyze_graph_metrics(file_path):\n",
        "        df = load_data(file_path)\n",
        "        G = nx.MultiDiGraph()\n",
        "        G.add_edges_from(zip(df[\"head\"], df[\"tail\"], df[\"relation\"]))\n",
        "\n",
        "        degrees = dict(G.degree())\n",
        "        counter = Counter(degrees.values())\n",
        "        avg_deg = sum(degrees.values()) / G.number_of_nodes() if G.number_of_nodes() else 0\n",
        "\n",
        "        return {\n",
        "            \"Deg_1\": counter.get(1, 0),\n",
        "            \"Deg_2\": counter.get(2, 0),\n",
        "            \"Deg_3\": counter.get(3, 0),\n",
        "            \"Avg_Degree\": round(avg_deg, 2),\n",
        "            \"Density\": round(nx.density(G), 6),\n",
        "            \"Sparsity\": round(1 - nx.density(G), 6)\n",
        "        }\n",
        "\n",
        "def process_file(file_path, label):\n",
        "    if os.path.isfile(file_path) and file_path.endswith(('.csv', '.txt')):\n",
        "        metrics = analyze_graph_metrics(file_path)\n",
        "        if metrics:\n",
        "            metrics['Dataset'] = label\n",
        "            return metrics\n",
        "    return None\n",
        "\n",
        "def analyze_all_datasets(all_dirs):\n",
        "    results = []\n",
        "    for base_dir in all_dirs:\n",
        "\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            dataset_name = os.path.basename(root)\n",
        "            for file in files:\n",
        "                path = os.path.join(root, file)\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                label_type = \"CSV\" if ext == '.csv' else \"TXT\"\n",
        "                label = f\"{dataset_name}_{os.path.splitext(file)[0]}\"\n",
        "                result = process_file(path, label)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)[[\"Dataset\", \"Deg_1\", \"Deg_2\", \"Deg_3\", \"Avg_Degree\", \"Density\", \"Sparsity\"]]\n",
        "\n",
        "# مقایسه‌ی ساختار مجموعه داده‌های فارسی و انگلیسی\n",
        "all_dirs = [\n",
        "    \"/content/Data_InductiveLinkPrediction\",\n",
        "    \"/content/PersianILP-trainTest\"]\n",
        "df_result = analyze_all_datasets(all_dirs).sort_values(\"Dataset\")\n",
        "print(tabulate(df_result, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
      ],
      "metadata": {
        "id": "jT-hkQWWqiy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-Inductive Link Prediction"
      ],
      "metadata": {
        "id": "beH7-u5KqyOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Data**"
      ],
      "metadata": {
        "id": "yCtIzmhRrcr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/ILPDataSet.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir: str = '/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            dataset_files = {\n",
        "                \"train\": None,\n",
        "                \"valid\": None,\n",
        "                \"test\": None}\n",
        "\n",
        "            # Check for both .txt and .csv files\n",
        "            for split in dataset_files.keys():\n",
        "                txt_path = os.path.join(dataset_path, f\"{split}.txt\")\n",
        "                csv_path = os.path.join(dataset_path, f\"{split}.csv\")\n",
        "\n",
        "                if os.path.exists(txt_path):\n",
        "                    dataset_files[split] = txt_path\n",
        "                elif os.path.exists(csv_path):\n",
        "                    dataset_files[split] = csv_path\n",
        "\n",
        "            datasets[dataset_name] = dataset_files\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/ILPDataSet')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ],
      "metadata": {
        "id": "MgNVh6Q6rM6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DGL Dataset**"
      ],
      "metadata": {
        "id": "Reb0FyYEri_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "class PersianDGLDataset(DGLDataset):\n",
        "    def __init__(self, train_file, test_file, seed=42):\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.seed = seed\n",
        "        self.process()\n",
        "        super().__init__(name=\"PersianLinkPrediction\")\n",
        "\n",
        "    def process(self):\n",
        "        # Initialize mappings\n",
        "        self.entity2id = {}\n",
        "        self.relation2id = {}\n",
        "        ent_id, rel_id = 0, 0\n",
        "\n",
        "        # Process training data\n",
        "        train_triples = self._load_and_process_file(self.train_file, ent_id, rel_id)\n",
        "        ent_id, rel_id = len(self.entity2id), len(self.relation2id)\n",
        "\n",
        "        # Process test data (using same mappings)\n",
        "        test_triples = self._load_and_process_file(self.test_file, ent_id, rel_id)\n",
        "\n",
        "        # Build graphs\n",
        "        self.graphs = {\n",
        "            \"train\": self._build_graph(train_triples),\n",
        "            \"test\": self._build_graph(test_triples)\n",
        "        }\n",
        "\n",
        "    def _load_file(self, file_path):\n",
        "        \"\"\"Load file based on its extension\"\"\"\n",
        "        if file_path.endswith('.csv'):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            return pd.read_csv(file_path, sep='\\t', header=None,\n",
        "                             names=['subjectLabel', 'predicateLabel', 'objectLabel'])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Only .csv and .txt files are supported.\")\n",
        "\n",
        "    def _load_and_process_file(self, file_path, ent_id_start, rel_id_start):\n",
        "        \"\"\"Load and process a single file, updating mappings\"\"\"\n",
        "        triples = []\n",
        "        df = self._load_file(file_path)\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row['subjectLabel'], row['predicateLabel'], row['objectLabel']\n",
        "\n",
        "            # Update entity mappings\n",
        "            for ent in [h, t]:\n",
        "                if ent not in self.entity2id:\n",
        "                    self.entity2id[ent] = ent_id_start\n",
        "                    ent_id_start += 1\n",
        "\n",
        "            # Update relation mappings\n",
        "            if r not in self.relation2id:\n",
        "                self.relation2id[r] = rel_id_start\n",
        "                rel_id_start += 1\n",
        "\n",
        "            triples.append((\n",
        "                self.entity2id[h],\n",
        "                self.relation2id[r],\n",
        "                self.entity2id[t]))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _build_graph(self, triples):\n",
        "        \"\"\"Build DGL graph from triples\"\"\"\n",
        "        src, rel, dst = zip(*triples)\n",
        "        src = torch.tensor(src)\n",
        "        dst = torch.tensor(dst)\n",
        "        rel = torch.tensor(rel)\n",
        "\n",
        "        g = dgl.graph((src, dst), num_nodes=len(self.entity2id))\n",
        "        g.edata[\"e_type\"] = rel\n",
        "        g.edata[\"edge_mask\"] = torch.ones(g.num_edges(), dtype=torch.bool)\n",
        "        g.ndata[\"ntype\"] = torch.zeros(g.num_nodes(), dtype=torch.int)\n",
        "        g.ndata[\"feat\"] = torch.randn(g.num_nodes(), 64)\n",
        "        return g\n",
        "\n",
        "    def __getitem__(self, split):\n",
        "        return self.graphs[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "class GraphBatchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, graphs, pos_graphs, neg_graphs):\n",
        "        self.graphs = graphs\n",
        "        self.pos_graphs = pos_graphs\n",
        "        self.neg_graphs = neg_graphs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"graph\": self.graphs[idx],\n",
        "            \"pos_graph\": self.pos_graphs[idx],\n",
        "            \"neg_graph\": self.neg_graphs[idx]}\n",
        "\n",
        "\n",
        "dataset = PersianDGLDataset(train_file = ILP_dataset_paths['PersianILP_V1']['train'],\n",
        "                            test_file = ILP_dataset_paths['PersianILP_V1']['test'])\n",
        "train_g = dataset[\"train\"]\n",
        "test_g = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "gVMYZesrrk3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Positive Graph And Negative Graph**"
      ],
      "metadata": {
        "id": "ExoK1-UFrpiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dgl\n",
        "import scipy.sparse as sp\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "\n",
        "class GraphNegativeSampler:\n",
        "    def __init__(self, train_graph, test_graph, train_neg_ratio=1.0, test_neg_ratio=1.0):\n",
        "        self.train_graph = train_graph\n",
        "        self.test_graph = test_graph\n",
        "        self.train_neg_ratio = train_neg_ratio\n",
        "        self.test_neg_ratio = test_neg_ratio\n",
        "        self.train_pos_g, self.train_neg_g = self._prepare_graphs(train_graph, train_neg_ratio)\n",
        "        self.test_pos_g, self.test_neg_g = self._prepare_graphs(test_graph, test_neg_ratio)\n",
        "\n",
        "    def _generate_negative_samples(self, graph):\n",
        "        u, v = graph.edges()\n",
        "        adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())),\n",
        "                          shape=(graph.num_nodes(), graph.num_nodes()))\n",
        "        return np.where(1 - adj.todense() - np.eye(graph.num_nodes()) != 0)\n",
        "\n",
        "    def _prepare_graphs(self, graph, ratio):\n",
        "        return ( self._create_positive_graph(graph),\n",
        "                 self._create_negative_graph(graph, ratio))\n",
        "\n",
        "    def _create_positive_graph(self, graph):\n",
        "        g = dgl.graph(graph.edges(), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = graph.edata[\"e_type\"]\n",
        "        g.ndata.update({k: graph.ndata[k] for k in [\"feat\", \"ntype\"]})\n",
        "        return g\n",
        "\n",
        "    def _create_negative_graph(self, graph, ratio):\n",
        "        neg_u, neg_v = self._generate_negative_samples(graph)\n",
        "        num_samples = int(graph.num_edges() * ratio)\n",
        "        replace = len(neg_u) < num_samples\n",
        "        sample_ids = np.random.choice(len(neg_u), num_samples, replace=replace)\n",
        "\n",
        "        g = dgl.graph((neg_u[sample_ids], neg_v[sample_ids]), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = torch.randint(0, graph.edata[\"e_type\"].max().item()+1, (g.num_edges(),))\n",
        "        g.ndata.update({\n",
        "            \"feat\": graph.ndata[\"feat\"],\n",
        "            \"ntype\": torch.ones(graph.num_nodes(), dtype=torch.int)})\n",
        "        return g\n",
        "\n",
        "    @property\n",
        "    def training_graphs(self):\n",
        "        return self.train_pos_g, self.train_neg_g\n",
        "\n",
        "    @property\n",
        "    def test_graphs(self):\n",
        "        return self.test_pos_g, self.test_neg_g\n",
        "\n",
        "# Sampling From Knowladge Graph\n",
        "sampler = GraphNegativeSampler(dataset['train'],\n",
        "                               dataset['test'],\n",
        "                               train_neg_ratio=1,\n",
        "                               test_neg_ratio=1)\n",
        "\n",
        "train_pos, train_neg = sampler.training_graphs\n",
        "test_pos, test_neg = sampler.test_graphs"
      ],
      "metadata": {
        "id": "gsFU2PjVrs5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link Prediction Model**"
      ],
      "metadata": {
        "id": "SGioSEvKr24x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedGraphSAGE(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats, out_feats, dropout=0.5):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, out_feats, \"mean\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            return g.edata[\"score\"][:, 0]"
      ],
      "metadata": {
        "id": "rZffOokcr50c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train method**"
      ],
      "metadata": {
        "id": "GRW_iO3fr7Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import dgl\n",
        "\n",
        "def train_model(model,\n",
        "                pred,\n",
        "                dataloader,\n",
        "                epochs,\n",
        "                lr=0.01):\n",
        "\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(),\n",
        "                                                 pred.parameters()),\n",
        "                                                 lr=lr)\n",
        "\n",
        "    all_losses = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch_graph = batch[\"graph\"]    # گراف اصلی\n",
        "            pos_graph = batch[\"pos_graph\"]  # گراف مثبت\n",
        "            neg_graph = batch[\"neg_graph\"]  # گراف منفی\n",
        "\n",
        "            # Forward pass\n",
        "            h = model(batch_graph, batch_graph.ndata[\"feat\"])\n",
        "            pos_score = pred(pos_graph, h)\n",
        "            neg_score = pred(neg_graph, h)\n",
        "            loss = compute_loss(pos_score,neg_score)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            e = epoch\n",
        "            loss = epoch_loss\n",
        "\n",
        "        all_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEpoch: {e}, Loss: {loss:.4f}\")\n",
        "    return h, all_losses\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ],
      "metadata": {
        "id": "sN25QhydsCYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train And Evaluation**"
      ],
      "metadata": {
        "id": "GxC8E0sWsGhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
        "from sklearn import metrics\n",
        "\n",
        "def train_and_evaluate_model(result:dict,\n",
        "                             dataset_name,\n",
        "                             ILP_dataset_paths,\n",
        "                             h_feats=16,\n",
        "                             out_feats=10,\n",
        "                             dropout=0.5,\n",
        "                             epochs=2000,\n",
        "                             lr=0.001,\n",
        "                             train_neg_ratio=10,\n",
        "                             test_neg_ratio=1):\n",
        "\n",
        "    # ===== Step 1: Dataset Preparation =====\n",
        "    graphs = PersianDGLDataset(\n",
        "        train_file=ILP_dataset_paths[dataset_name]['train'],\n",
        "        test_file=ILP_dataset_paths[dataset_name]['test']\n",
        "    )\n",
        "\n",
        "    sampler = GraphNegativeSampler(\n",
        "        graphs['train'], graphs['test'],\n",
        "        train_neg_ratio=train_neg_ratio,\n",
        "        test_neg_ratio=test_neg_ratio\n",
        "    )\n",
        "\n",
        "    train_pos_g, train_neg_g = sampler.training_graphs\n",
        "    test_pos_g, test_neg_g = sampler.test_graphs\n",
        "\n",
        "    train_dataset = GraphBatchDataset([graphs['train']], [train_pos_g], [train_neg_g])\n",
        "    train_loader = GraphDataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    test_dataset = GraphBatchDataset([graphs['test']], [test_pos_g], [test_neg_g])\n",
        "    test_loader = GraphDataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    # ===== Step 2: Training =====\n",
        "    def compute_loss(pos_score, neg_score):\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([\n",
        "            torch.ones(pos_score.shape[0]),\n",
        "            torch.zeros(neg_score.shape[0])\n",
        "        ])\n",
        "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "    in_feats = graphs['train'].ndata['feat'].shape[1]\n",
        "    model = ImprovedGraphSAGE(\n",
        "        in_feats=in_feats,\n",
        "        h_feats=h_feats,\n",
        "        out_feats=out_feats,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    pred = DotPredictor()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        itertools.chain(model.parameters(), pred.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch in train_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            pos_score = pred(batch['pos_graph'], h)\n",
        "            neg_score = pred(batch['neg_graph'], h)\n",
        "            loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ===== Step 3: Evaluation =====\n",
        "    pos_scores, pos_labels = [], []\n",
        "    neg_scores, neg_labels = [], []\n",
        "    hit1_list, hit3_list, hit10_list = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ranks = []\n",
        "        for batch in test_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            score_pos = pred(batch['pos_graph'], h).squeeze()\n",
        "            score_neg = pred(batch['neg_graph'], h).squeeze()\n",
        "\n",
        "            neg_per_pos = len(score_neg) // len(score_pos)\n",
        "            pos_scores += score_pos.tolist() if score_pos.dim() > 0 else [score_pos.item()]\n",
        "            neg_scores += score_neg.tolist() if score_neg.dim() > 0 else [score_neg.item()]\n",
        "            pos_labels += [1] * len(score_pos) if score_pos.dim() > 0 else [1]\n",
        "            neg_labels += [0] * len(score_neg) if score_neg.dim() > 0 else [0]\n",
        "\n",
        "            score_pos_exp = score_pos.view(-1, 1).repeat(1, neg_per_pos).view(-1)\n",
        "            scores = torch.stack([score_pos_exp, score_neg], dim=1)\n",
        "            scores = torch.softmax(scores, dim=1).cpu().numpy()\n",
        "            rank = np.argwhere(np.argsort(scores, axis=1)[:, ::-1] == 0)[:, 1] + 1\n",
        "\n",
        "            ranks += rank.tolist()\n",
        "            hit1_list += [1 if r <= 1 else 0 for r in rank]\n",
        "            hit3_list += [1 if r <= 3 else 0 for r in rank]\n",
        "            hit10_list += [1 if r <= 10 else 0 for r in rank]\n",
        "\n",
        "    # ===== Step 4: Result Metrics =====\n",
        "    result[dataset_name] = {\n",
        "        \"AUC\": metrics.roc_auc_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"AUC_PR\": metrics.average_precision_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"MRR\": np.mean(1.0 / np.array(ranks)).item(),\n",
        "        \"Hit1\": np.mean(hit1_list),\n",
        "        \"Hit3\": np.mean(hit3_list),\n",
        "        \"Hit10\": np.mean(hit10_list)}\n",
        "\n",
        "    return result\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tabulate import tabulate\n",
        "def display_results_table(result_dict):\n",
        "    clear_output()\n",
        "    headers = ['Dataset', 'AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']\n",
        "    rows = []\n",
        "    for name, metrics in result_dict.items():\n",
        "        row = [name] + [metrics[h] for h in headers[1:]]\n",
        "        rows.append(row)\n",
        "    print(\"\\n\" + tabulate(rows,\n",
        "                          headers=headers,\n",
        "                          tablefmt=\"fancy_grid\",\n",
        "                          floatfmt=\".4f\"))"
      ],
      "metadata": {
        "id": "N_Mp6D-8sPxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = {}\n",
        "for name, path in ILP_dataset_paths.items():\n",
        "    result = train_and_evaluate_model(result,\n",
        "                                      name,\n",
        "                                      ILP_dataset_paths,\n",
        "                                      h_feats=32,\n",
        "                                      out_feats=8,\n",
        "                                      dropout=0.5,\n",
        "                                      epochs=2000,\n",
        "                                      lr=0.001,\n",
        "                                      train_neg_ratio=1,\n",
        "                                      test_neg_ratio=1)\n",
        "    display_results_table(result)"
      ],
      "metadata": {
        "id": "rdCPemgqsWKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = {}\n",
        "for name, path in ILP_dataset_paths.items():\n",
        "    result = train_and_evaluate_model(result,\n",
        "                                      name,\n",
        "                                      ILP_dataset_paths,\n",
        "                                      h_feats=32,\n",
        "                                      out_feats=8,\n",
        "                                      dropout=0.5,\n",
        "                                      epochs=2000,\n",
        "                                      lr=0.001,\n",
        "                                      train_neg_ratio=10,\n",
        "                                      test_neg_ratio=50)\n",
        "    display_results_table(result)"
      ],
      "metadata": {
        "id": "a1xcYDfssbb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}