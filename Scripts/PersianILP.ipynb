{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZw-LPTl7tx"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SfQkfVglkHb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip uninstall -y numpy\n",
        "!pip cache purge\n",
        "!pip install numpy==1.26.4\n",
        "clear_output()\n",
        "print(\"Numpy install successful!\")\n",
        "\n",
        "import os\n",
        "import IPython\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpV8iTO6l0i_",
        "outputId": "2c28e040-830d-42d8-fd3e-89bb5ee5e494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "DGL installed!\n",
            "PyTorch Version:  2.2.0+cu121\n",
            "TorchMetrics Version:  1.2.1\n",
            "Transformers Version:  4.38.0\n",
            "DGL Version:  2.4.0\n",
            "TorchEval Is:  0.0.7\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "!pip install torchmetrics==1.2.1 transformers==4.38.0\n",
        "!pip install safetensors==0.4.1\n",
        "!pip install torcheval\n",
        "!pip install scikit-learn\n",
        "!pip install deep-translator\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import dgl\n",
        "import torch\n",
        "import torchmetrics\n",
        "import transformers\n",
        "import torcheval\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "try:\n",
        "    import dgl\n",
        "    import dgl.graphbolt as gb\n",
        "    installed = True\n",
        "except ImportError as error:\n",
        "    installed = False\n",
        "    print(error)\n",
        "\n",
        "print(\"DGL installed!\" if installed else \"DGL not found!\")\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"TorchMetrics Version: \", torchmetrics.__version__)\n",
        "print(\"Transformers Version: \", transformers.__version__)\n",
        "print(\"DGL Version: \", dgl.__version__)\n",
        "print(\"TorchEval Is: \", torcheval.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmPWFxJ0mFbJ"
      },
      "source": [
        "## 1- PersainILP Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amAxLwMynCMh"
      },
      "source": [
        "**Extrac Zip File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQj8gFpymRcR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/SPARQL.zip\"\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "excel_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "merged_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGjgfF8m10D"
      },
      "source": [
        "**Save Incomplete Triple**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfQqUUJmtbt",
        "outputId": "5af7b90e-00ea-44a5-c829-a42dc255731f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 تعداد فایل‌های CSV یافت‌شده: 206\n",
            "✅ تمام فایل‌ها با موفقیت ادغام شدند.\n",
            "\n",
            "📊 گزارش کامل دیتافریم ادغام شده:\n",
            "=================================\n",
            "➡ تعداد کل ردیف‌ها: 2,056,413\n",
            "➡ تعداد کل ستون‌ها: 3\n",
            "\n",
            "🔹 تعداد مقادیر null در هر ستون:\n",
            "subjectLabel      1920360\n",
            "predicateLabel     245845\n",
            "objectLabel        182278\n",
            "dtype: int64\n",
            "\n",
            "♻ تعداد ردیف‌ها پس از حذف موارد تکراری: 32,196\n",
            "\n",
            "💾 فایل triples کامل (9,523 ردیف) در /content/FarsiBase/complete_triples.csv ذخیره شد.\n",
            "💾 فایل triples با دو مقدار (21,492 ردیف) در /content/FarsiBase/triples_with_two_values.csv ذخیره شد.\n",
            "💾 فایل triples با یک مقدار (1,181 ردیف) در /content/FarsiBase/triples_with_one_value.csv ذخیره شد.\n",
            "\n",
            "🎉 پردازش با موفقیت به پایان رسید!\n",
            "\n",
            "📝 گزارش نهایی:\n",
            "subjectLabel      19385\n",
            "predicateLabel    23622\n",
            "objectLabel       29727\n",
            "filled_count      32196\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "extract_path = \"/content/extracted_excels\"\n",
        "output_path = \"/content/FarsiBase\"\n",
        "os.makedirs(output_path, exist_ok=True)  # ایجاد پوشه خروجی اگر وجود نداشته باشد\n",
        "\n",
        "csv_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.csv')]\n",
        "print(f\"🔎 تعداد فایل‌های CSV یافت‌شده: {len(csv_files)}\")\n",
        "\n",
        "merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "merged_df.to_csv('/content/mergeData.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"✅ تمام فایل‌ها با موفقیت ادغام شدند.\\n\")\n",
        "\n",
        "print(\"📊 گزارش کامل دیتافریم ادغام شده:\")\n",
        "print(\"=================================\")\n",
        "print(f\"➡ تعداد کل ردیف‌ها: {len(merged_df):,}\")\n",
        "print(f\"➡ تعداد کل ستون‌ها: {len(merged_df.columns)}\")\n",
        "print(\"\\n🔹 تعداد مقادیر null در هر ستون:\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "def simplify_uri(uri):\n",
        "    if isinstance(uri, str):\n",
        "        return uri.strip().split(\"/\")[-1].split(\"#\")[-1]  # بهبود برای هندل کردن URIهای مختلف\n",
        "    return uri\n",
        "\n",
        "cols_to_simplify = [\"subjectLabel\", \"predicateLabel\", \"objectLabel\"]\n",
        "simplified_df = merged_df.copy()\n",
        "for col in cols_to_simplify:\n",
        "    simplified_df[col] = simplified_df[col].apply(simplify_uri)\n",
        "\n",
        "simplified_df.drop_duplicates(inplace=True)\n",
        "simplified_df = simplified_df.dropna(how='all')\n",
        "print(f\"\\n♻ تعداد ردیف‌ها پس از حذف موارد تکراری: {len(simplified_df):,}\")\n",
        "\n",
        "simplified_df[\"filled_count\"] = simplified_df[cols_to_simplify].notna().sum(axis=1)\n",
        "\n",
        "complete_df = simplified_df[simplified_df[\"filled_count\"] == 3].drop(columns=[\"filled_count\"])\n",
        "complete_path = os.path.join(output_path, \"complete_triples.csv\")\n",
        "complete_df.to_csv(complete_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n💾 فایل triples کامل ({len(complete_df):,} ردیف) در {complete_path} ذخیره شد.\")\n",
        "\n",
        "two_filled_df = simplified_df[simplified_df[\"filled_count\"] == 2].drop(columns=[\"filled_count\"])\n",
        "two_path = os.path.join(output_path, \"triples_with_two_values.csv\")\n",
        "two_filled_df.to_csv(two_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"💾 فایل triples با دو مقدار ({len(two_filled_df):,} ردیف) در {two_path} ذخیره شد.\")\n",
        "\n",
        "one_filled_df = simplified_df[simplified_df[\"filled_count\"] == 1].drop(columns=[\"filled_count\"])\n",
        "one_path = os.path.join(output_path, \"triples_with_one_value.csv\")\n",
        "one_filled_df.to_csv(one_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"💾 فایل triples با یک مقدار ({len(one_filled_df):,} ردیف) در {one_path} ذخیره شد.\")\n",
        "\n",
        "print(\"\\n🎉 پردازش با موفقیت به پایان رسید!\")\n",
        "print(f\"\\n📝 گزارش نهایی:\\n{simplified_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaoDYMbnj5_"
      },
      "source": [
        "**FarsiBase Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orB7I9PenrDz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def convert_persian_to_english(number):\n",
        "    persian_to_english = str.maketrans('۰۱۲۳۴۵۶۷۸۹', '0123456789')\n",
        "    return str(number).translate(persian_to_english)\n",
        "\n",
        "df = pd.read_csv(\"/content/FarsiBase/complete_triples.csv\")\n",
        "for column in ['subjectLabel', 'predicateLabel', 'objectLabel']:\n",
        "    df[column] = df[column].apply(convert_persian_to_english)\n",
        "\n",
        "# Clean Relation\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^dcterms#subject','موضوع/محتوا', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^subject','موضوع', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth place','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birth_place','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^birthPlace','محل تولد', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^instanceOf','نمونه‌ای از', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^deathPlace','محل مرگ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^death place','محل مرگ', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^field','موضوع', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^genre','ژانر', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^nationality','ملیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^occupation','شغل', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^picture','تصویر', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^ActiveYears','سال‌های فعالیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^activeYears','سال‌های فعالیت', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^timezone1 dst','ناحیه زمانی ۱', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^confed_cup','جام کنفدراسیون', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^distance to London (μ)','فاصله تا لندن (میانگین)', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^fs_date','تاریخ سیستم فایل', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^państwo','کشور', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^państwo','کشور', regex=True)\n",
        "df['predicateLabel'] = df['predicateLabel'].str.replace(r'^sp_date','تاریخ طرح', regex=True)\n",
        "\n",
        "df = df.drop(df[df['predicateLabel'] == '22-rdf-syntax-ns#instanceOf'].index)\n",
        "df = df[~df['objectLabel'].str.contains('relation', case=False, na=False)]\n",
        "df = df[~df['objectLabel'].str.endswith('.JPG')] # Delete Row with .JPG Value\n",
        "df = df[~df['objectLabel'].str.endswith('.jpg')]\n",
        "df = df[~df['objectLabel'].str.endswith('.png')]\n",
        "df = df[~df['objectLabel'].str.endswith('.svg')]\n",
        "df = df[~df['objectLabel'].str.endswith('Pages_using_infobox3cols_with_multidatastyle')]\n",
        "df = df[~df['objectLabel'].str.endswith(':hy:Սյուզան_Գարագաշ')]\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Actor','بازیگر', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Ali_Daei','علی دایی', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Person','شخص', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^SoccerPlayer','بازیکن سوکر', regex=True)\n",
        "df['objectLabel'] = df['objectLabel'].str.replace(r'^Writer','نویسنده', regex=True)\n",
        "\n",
        "# Remove duplicate row\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.to_csv(\"/content/FarsiBase/complete_triples.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doaLPtD1oBbq"
      },
      "source": [
        "**Data Shuffling And Aggregation(FarsiBase + Deepseek)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CKXSVFboCLf",
        "outputId": "7e096d18-4421-480d-e5b1-eaa800d5e284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "فایل اکسل به صورت تصادفی به هم ریخته و در '/content/FarsiBase/shuffled_triple.csv' ذخیره شد.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate FarsiBase Data And DeepSeek Data\n",
        "DeepSeek_df = pd.read_excel('/content/DeepSeek_Triple.xlsx')\n",
        "input_file = '/content/FarsiBase/complete_triples.csv'\n",
        "FarsiBase_df = pd.read_csv(input_file)\n",
        "df = pd.concat([DeepSeek_df, FarsiBase_df], axis=0)\n",
        "\n",
        "# Shuffled Data\n",
        "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "output_file = '/content/FarsiBase/shuffled_triple.csv'\n",
        "shuffled_df.to_csv(output_file ,index=False , encoding='utf-8-sig')\n",
        "print(f\"فایل اکسل به صورت تصادفی به هم ریخته و در '{output_file}' ذخیره شد.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdeM5zEOoMN1"
      },
      "source": [
        "**PersianILP Normalizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16eOYAhBoH5n",
        "outputId": "0f7897d0-4c78-48fd-8e2b-0a1a3247758f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "داده‌های نرمال‌سازی شده در '/content/FarsiBase/triple.csv' ذخیره شد.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Translate Google\n",
        "def translate_relation(relation, target_lang=\"fa\"):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='auto', target=target_lang).translate(relation)\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"خطا در ترجمه '{relation}': {e}\")\n",
        "        return relation\n",
        "\n",
        "# Translate Data\n",
        "def normalize_excel(input_path, output_path, use_translation=False):\n",
        "\n",
        "    df = pd.read_csv(input_path)\n",
        "    normalized_relations = []\n",
        "    for relation in df['predicateLabel']:\n",
        "        if pd.isna(relation):\n",
        "            normalized = relation\n",
        "        else:\n",
        "            relation = str(relation)\n",
        "            if use_translation and relation.isascii():\n",
        "              normalized = translate_relation(relation)\n",
        "            else:\n",
        "              normalized = relation\n",
        "        normalized_relations.append(normalized)\n",
        "    df['predicateLabel'] = normalized_relations\n",
        "\n",
        "    # Delete Duplicate Row And Shuffling Data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"داده‌های نرمال‌سازی شده در '{output_path}' ذخیره شد.\")\n",
        "\n",
        "input_excel = \"/content/FarsiBase/shuffled_triple.csv\"\n",
        "output_excel = \"/content/FarsiBase/triple.csv\"\n",
        "normalize_excel(input_path=input_excel,\n",
        "                output_path=output_excel,\n",
        "                use_translation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5SXP1vZoY1l",
        "outputId": "372219d1-cc1c-46b8-a21a-b736e2a0ee2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ دیتاست با موفقیت خوانده شد. تعداد سه‌تایی‌ها: 16,306\n",
            "\n",
            "📊 آماره‌های پایه:\n",
            "تعداد موجودیت‌های منحصر به فرد: 10,693\n",
            "تعداد روابط منحصر به فرد: 432\n",
            "\n",
            "📈 توزیع درجه گره‌ها:\n",
            "• گره‌های با درجه ۱: 5,770 (54.0%)\n",
            "• گره‌های با درجه ۲: 2,663 (24.9%)\n",
            "• گره‌های با درجه ۳: 900 (8.4%)\n",
            "\n",
            "🔍 معیارهای ساختاری گراف:\n",
            "تعداد گره‌ها: 10,693\n",
            "تعداد یال‌ها: 16,306\n",
            "چگالی گراف: 0.000143\n",
            "اسپارس بودن: 0.9999\n",
            "میانگین درجه گره‌ها: 3.05\n",
            "\n",
            "🔗 گراف دارای 731 جزء ناهمبند است\n",
            "\n",
            "🧪 ارزیابی مناسب بودن برای پیش‌بینی پیوند:\n",
            "✓ تنوع روابط عالی (432 نوع رابطه)\n",
            "✓ اسپارس بودن ایده‌آل (بسیار مناسب برای پیش‌بینی پیوند)\n",
            "✗ توزیع درجه نامتعادل (54.0% گره‌ها درجه ۱ دارند)\n",
            "\n",
            "🎯 نتیجه‌گیری نهایی:\n",
            "✅ این دیتاست برای پیش‌بینی پیوند بسیار مناسب است\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_dataset_for_link_prediction(file_path):\n",
        "\n",
        "    # 1. خواندن داده‌ها\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, names=['subject', 'predicate', 'object'])\n",
        "        print(f\"✅ دیتاست با موفقیت خوانده شد. تعداد سه‌تایی‌ها: {len(df):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ خطا در خواندن فایل: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. محاسبه معیارهای پایه\n",
        "    num_entities = len(set(df['subject']).union(set(df['object'])))\n",
        "    num_relations = len(set(df['predicate']))\n",
        "    print(f\"\\n📊 آماره‌های پایه:\")\n",
        "    print(f\"تعداد موجودیت‌های منحصر به فرد: {num_entities:,}\")\n",
        "    print(f\"تعداد روابط منحصر به فرد: {num_relations:,}\")\n",
        "\n",
        "    # 3. ایجاد گراف\n",
        "    G = nx.MultiDiGraph()  # گراف جهت‌دار با امکان چندین یال بین گره‌ها\n",
        "    for _, row in df.iterrows():\n",
        "        G.add_edge(row['subject'], row['object'], key=row['predicate'])\n",
        "\n",
        "    # 4. تحلیل درجه گره‌ها\n",
        "    degrees = dict(G.degree())\n",
        "    degree_counts = Counter(degrees.values())\n",
        "\n",
        "    print(\"\\n📈 توزیع درجه گره‌ها:\")\n",
        "    print(f\"• گره‌های با درجه ۱: {degree_counts.get(1, 0):,} ({degree_counts.get(1, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"• گره‌های با درجه ۲: {degree_counts.get(2, 0):,} ({degree_counts.get(2, 0)/G.number_of_nodes():.1%})\")\n",
        "    print(f\"• گره‌های با درجه ۳: {degree_counts.get(3, 0):,} ({degree_counts.get(3, 0)/G.number_of_nodes():.1%})\")\n",
        "\n",
        "    # 5. محاسبه معیارهای گراف\n",
        "    density = nx.density(G)\n",
        "    sparsity = 1 - density\n",
        "    avg_degree = sum(degrees.values()) / G.number_of_nodes()\n",
        "    print(\"\\n🔍 معیارهای ساختاری گراف:\")\n",
        "    print(f\"تعداد گره‌ها: {G.number_of_nodes():,}\")\n",
        "    print(f\"تعداد یال‌ها: {G.number_of_edges():,}\")\n",
        "    print(f\"چگالی گراف: {density:.6f}\")\n",
        "    print(f\"اسپارس بودن: {sparsity:.4f}\")\n",
        "    print(f\"میانگین درجه گره‌ها: {avg_degree:.2f}\")\n",
        "\n",
        "    # 6. بررسی اتصالات\n",
        "    if nx.is_weakly_connected(G):\n",
        "        print(\"\\n🔄 گراف به صورت ضعیف متصل است\")\n",
        "    else:\n",
        "        components = nx.number_weakly_connected_components(G)\n",
        "        print(f\"\\n🔗 گراف دارای {components} جزء ناهمبند است\")\n",
        "\n",
        "    # 7. تحلیل نهایی برای پیش‌بینی پیوند\n",
        "    print(\"\\n🧪 ارزیابی مناسب بودن برای پیش‌بینی پیوند:\")\n",
        "\n",
        "    suitability_score = 0\n",
        "\n",
        "    # معیار 1: تنوع روابط\n",
        "    if num_relations > 50:\n",
        "        print(f\"✓ تنوع روابط عالی ({num_relations} نوع رابطه)\")\n",
        "        suitability_score += 2\n",
        "    elif num_relations > 10:\n",
        "        print(f\"✓ تنوع روابط قابل قبول ({num_relations} نوع رابطه)\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"✗ تنوع روابط ناکافی ({num_relations} نوع رابطه)\")\n",
        "\n",
        "    # معیار 2: اسپارس بودن\n",
        "    if sparsity > 0.99:\n",
        "        print(\"✓ اسپارس بودن ایده‌آل (بسیار مناسب برای پیش‌بینی پیوند)\")\n",
        "        suitability_score += 2\n",
        "    elif sparsity > 0.95:\n",
        "        print(\"✓ اسپارس بودن قابل قبول\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(\"✗ اسپارس بودن ناکافی\")\n",
        "\n",
        "    # معیار 3: توزیع درجه\n",
        "    if degree_counts.get(1, 0) < G.number_of_nodes() * 0.4:\n",
        "        print(\"✓ توزیع درجه متعادل\")\n",
        "        suitability_score += 1\n",
        "    else:\n",
        "        print(f\"✗ توزیع درجه نامتعادل ({degree_counts.get(1, 0)/G.number_of_nodes():.1%} گره‌ها درجه ۱ دارند)\")\n",
        "\n",
        "    # نتیجه‌گیری نهایی\n",
        "    print(\"\\n🎯 نتیجه‌گیری نهایی:\")\n",
        "    if suitability_score >= 4:\n",
        "        print(\"✅ این دیتاست برای پیش‌بینی پیوند بسیار مناسب است\")\n",
        "    elif suitability_score >= 2:\n",
        "        print(\"⚠️ این دیتاست برای پیش‌بینی پیوند نیاز به بهبودهایی دارد\")\n",
        "    else:\n",
        "        print(\"❌ این دیتاست برای پیش‌بینی پیوند مناسب نیست\")\n",
        "\n",
        "# نمونه استفاده\n",
        "analyze_dataset_for_link_prediction('/content/FarsiBase/triple.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hd_vtyosTr"
      },
      "source": [
        "**Extracting three variants of the main dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# تفکیک مجموعه‌ی آموزش و تست\n",
        "def split_full_inductive(file_path, test_size=0.2):\n",
        "    df = pd.read_csv(file_path)\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "    train_entities, test_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    train_entities = set(train_entities)\n",
        "    test_entities = set(test_entities)\n",
        "    train_mask = df.iloc[:, 0].isin(train_entities) & df.iloc[:, 2].isin(train_entities)\n",
        "    test_mask = df.iloc[:, 0].isin(test_entities) & df.iloc[:, 2].isin(test_entities)\n",
        "    train_df = df[train_mask]\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# تفکیک به صورت نیمه‌استقرایی\n",
        "def split_semi_inductive(file_path, test_size=0.2):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract all unique entities\n",
        "    subjects = set(df.iloc[:, 0].dropna().unique())\n",
        "    objects = set(df.iloc[:, 2].dropna().unique())\n",
        "    all_entities = subjects.union(objects)\n",
        "\n",
        "    seen_entities, unseen_entities = train_test_split(\n",
        "        list(all_entities),\n",
        "        test_size=test_size,\n",
        "        random_state=42)\n",
        "\n",
        "    seen_entities = set(seen_entities)\n",
        "    unseen_entities = set(unseen_entities)\n",
        "\n",
        "    train_mask = df.iloc[:, 0].isin(seen_entities) & df.iloc[:, 2].isin(seen_entities)\n",
        "    train_df = df[train_mask]\n",
        "\n",
        "    test_mask = (\n",
        "        (df.iloc[:, 0].isin(seen_entities) & df.iloc[:, 2].isin(unseen_entities)) |\n",
        "        (df.iloc[:, 0].isin(unseen_entities) & df.iloc[:, 2].isin(seen_entities)))\n",
        "\n",
        "    test_df = df[test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "# تفکیک به صورت انتقالی\n",
        "def split_transductive(file_path, test_size=0.2):\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Split triples into train and test\n",
        "    split_index = int(len(df) * (1 - test_size))\n",
        "    train_df = df.iloc[:split_index]\n",
        "    test_df = df.iloc[split_index:]\n",
        "\n",
        "    # Get entities from training set\n",
        "    train_subjects = set(train_df.iloc[:, 0].dropna().unique())\n",
        "    train_objects = set(train_df.iloc[:, 2].dropna().unique())\n",
        "    train_entities = train_subjects.union(train_objects)\n",
        "\n",
        "    # Filter test triples: both subject and object must be in training entities\n",
        "    test_mask = df.iloc[split_index:, 0].isin(train_entities) & df.iloc[split_index:, 2].isin(train_entities)\n",
        "    test_df = df.iloc[split_index:][test_mask]\n",
        "    return train_df, test_df\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "# مسیرها\n",
        "file_path = '/content/FarsiBase/triple.csv'\n",
        "output_dir = '/content/PersianILP-trainTest'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# تنظیمات نسخه‌ها و توابع مرتبط\n",
        "versions = {\n",
        "    'PersianILP_V1': split_full_inductive,      # Full Inductive\n",
        "    'PersianILP_V2': split_semi_inductive,      # Semi Inductive\n",
        "    'PersianILP_V3': split_transductive         # Transductive\n",
        "}\n",
        "\n",
        "# پردازش و ذخیره‌سازی\n",
        "for version_name, split_func in versions.items():\n",
        "    version_dir = os.path.join(output_dir, version_name)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "\n",
        "    train_data, test_data = split_func(file_path, test_size=0.3)\n",
        "\n",
        "    train_data.to_csv(os.path.join(version_dir, 'train.csv'), index=False, encoding='utf-8-sig')\n",
        "    test_data.to_csv(os.path.join(version_dir, 'test.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "# فشرده‌سازی خروجی‌ها\n",
        "zip_path = os.path.join(output_dir, 'PersianILP-data.zip')\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"✅ فایل zip با موفقیت ایجاد شد:\\n{zip_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wai2GoSp3Awq",
        "outputId": "54bb808f-9afa-447a-ad32-b6fa5dc15bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ فایل zip با موفقیت ایجاد شد:\n",
            "/content/PersianILP-trainTest/PersianILP-data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def analyze_splits(output_dir):\n",
        "    \"\"\"\n",
        "    Analyzes train/test files in each version directory and prints a summary table.\n",
        "\n",
        "    Parameters:\n",
        "        output_dir (str): Path to the directory containing version folders (e.g., PersianILP_V1, V2, V3).\n",
        "    \"\"\"\n",
        "    summary = []\n",
        "\n",
        "    for version in ['PersianILP_V1', 'PersianILP_V2', 'PersianILP_V3']:\n",
        "        version_dir = os.path.join(output_dir, version)\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            file_path = os.path.join(version_dir, f'{split}.csv')\n",
        "            if not os.path.exists(file_path):\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(file_path)\n",
        "            num_triples = len(df)\n",
        "            entities = set(df.iloc[:, 0].dropna().unique()).union(df.iloc[:, 2].dropna().unique())\n",
        "            num_entities = len(entities)\n",
        "            num_relations = len(df.iloc[:, 1].dropna().unique())\n",
        "\n",
        "            summary.append([\n",
        "                version,\n",
        "                split,\n",
        "                num_triples,\n",
        "                num_entities,\n",
        "                num_relations\n",
        "            ])\n",
        "\n",
        "    headers = ['Version', 'Split', 'Triples', 'Entities', 'Relations']\n",
        "    print(tabulate(summary, headers=headers, tablefmt='grid'))\n",
        "\n",
        "analyze_splits('/content/PersianILP-trainTest')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc007TFj-K6G",
        "outputId": "cc2671c9-e447-4dfc-b70a-2eaad5c2433c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+-----------+------------+-------------+\n",
            "| Version       | Split   |   Triples |   Entities |   Relations |\n",
            "+===============+=========+===========+============+=============+\n",
            "| PersianILP_V1 | train   |      7563 |       5951 |         324 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V1 | test    |      1684 |       1521 |         135 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V2 | train   |      7563 |       5951 |         324 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V2 | test    |      7054 |       6165 |         278 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V3 | train   |     11413 |       8677 |         362 |\n",
            "+---------------+---------+-----------+------------+-------------+\n",
            "| PersianILP_V3 | test    |      2820 |       2517 |         175 |\n",
            "+---------------+---------+-----------+------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspcHPFBpHRD"
      },
      "source": [
        "**ًImport Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN6CQqq7pLpu",
        "outputId": "e3890cad-a7cc-494e-edaa-e769bb9289ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/Data_InductiveLinkPrediction.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir:str='/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            datasets[dataset_name] = {\n",
        "                \"train\": os.path.join(dataset_path, \"train.txt\"),\n",
        "                \"valid\": os.path.join(dataset_path, \"valid.txt\"),\n",
        "                \"test\":  os.path.join(dataset_path, \"test.txt\")}\n",
        "    return datasets\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/Data_InductiveLinkPrediction')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlZymFSOqqnf"
      },
      "source": [
        "**Analysis PersianILP With English BencmarkDataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT-hkQWWqiy1",
        "outputId": "3ae8b275-2d3a-40ea-95ae-14411ad80890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| Dataset             |   Deg_1 |   Deg_2 |   Deg_3 |   Avg_Degree |   Density |   Sparsity |\n",
            "+=====================+=========+=========+=========+==============+===========+============+\n",
            "| PersianILP_V1_test  |    1029 |     283 |      51 |         2.21 |  0.000727 |   0.999273 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V1_train |    3828 |    1082 |     376 |         2.54 |  0.000213 |   0.999787 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_test  |    4221 |    1002 |     326 |         2.29 |  0.000186 |   0.999814 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V2_train |    3828 |    1082 |     376 |         2.54 |  0.000213 |   0.999787 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_test  |    1653 |     352 |     167 |         2.24 |  0.000445 |   0.999555 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| PersianILP_V3_train |    5380 |    1790 |     543 |         2.63 |  0.000152 |   0.999848 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_test  |     212 |      61 |      11 |         1.31 |  0.002306 |   0.997694 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_train |     187 |     227 |     155 |         3.51 |  0.001905 |   0.998095 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v1_ind_valid |     223 |      55 |      11 |         1.28 |  0.002207 |   0.997793 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_test  |     573 |     111 |      19 |         1.24 |  0.000876 |   0.999124 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_train |     829 |     684 |     453 |         2.91 |  0.000528 |   0.999472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v2_ind_valid |     524 |     117 |      16 |         1.24 |  0.000942 |   0.999058 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_test  |     832 |     110 |      21 |         1.24 |  0.000637 |   0.999363 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_train |    2144 |    1619 |     581 |         2.49 |  0.000245 |   0.999755 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v3_ind_valid |     758 |     106 |       4 |         1.22 |  0.000692 |   0.999308 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_test  |    1793 |     391 |      64 |         1.26 |  0.000277 |   0.999723 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_train |    1424 |    1710 |    1311 |         3.48 |  0.000246 |   0.999754 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| WN18RR_v4_ind_valid |    1731 |     386 |      70 |         1.26 |  0.000287 |   0.999713 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_test   |     226 |      55 |      14 |         1.36 |  0.00227  |   0.99773  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_train  |     376 |     227 |     152 |         3.65 |  0.00167  |   0.99833  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v1_ind_valid  |     211 |      48 |      17 |         1.44 |  0.00251  |   0.99749  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_test   |     362 |     118 |      41 |         1.7  |  0.001516 |   0.998484 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_train  |     419 |     300 |     263 |         4.99 |  0.001505 |   0.998495 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v2_ind_valid  |     359 |     116 |      38 |         1.71 |  0.001565 |   0.998435 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_test   |     609 |     213 |      98 |         1.76 |  0.0009   |   0.9991   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_train  |     531 |     354 |     330 |         5.92 |  0.001184 |   0.998816 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v3_ind_valid  |     587 |     225 |     101 |         1.78 |  0.000916 |   0.999084 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_test   |     817 |     316 |     145 |         2    |  0.0007   |   0.9993   |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_train  |     493 |     378 |     312 |         7.68 |  0.001259 |   0.998741 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| fb237_v4_ind_valid  |     820 |     314 |     129 |         2    |  0.000705 |   0.999295 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_test    |      67 |      12 |       3 |         2.38 |  0.014343 |   0.985657 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_train   |      12 |      22 |      40 |         7.4  |  0.016528 |   0.983472 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v1_ind_valid   |      66 |      13 |       3 |         2.4  |  0.014487 |   0.985513 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_test    |     316 |      67 |      32 |         1.99 |  0.002088 |   0.997912 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_train   |     968 |     415 |     196 |         4.4  |  0.001054 |   0.998946 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v2_ind_valid   |     304 |     101 |      25 |         1.89 |  0.001939 |   0.998061 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_test    |     521 |     150 |      60 |         2.03 |  0.001272 |   0.998728 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_train   |    1683 |     705 |     359 |         4.51 |  0.000633 |   0.999367 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v3_ind_valid   |     532 |     151 |      57 |         1.99 |  0.001225 |   0.998775 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_test    |     381 |      98 |      47 |         2.32 |  0.001845 |   0.998155 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_train   |    1342 |     570 |     272 |         5.06 |  0.000906 |   0.999094 |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n",
            "| nell_v4_ind_valid   |     380 |      93 |      35 |         2.32 |  0.00189  |   0.99811  |\n",
            "+---------------------+---------+---------+---------+--------------+-----------+------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "def load_data(file_path):\n",
        "    sep = \",\" if file_path.endswith('.csv') else \"\\t\"\n",
        "    return pd.read_csv(file_path, sep=sep, header=None, names=[\"head\", \"relation\", \"tail\"])\n",
        "\n",
        "def analyze_graph_metrics(file_path):\n",
        "        df = load_data(file_path)\n",
        "        G = nx.MultiDiGraph()\n",
        "        G.add_edges_from(zip(df[\"head\"], df[\"tail\"], df[\"relation\"]))\n",
        "\n",
        "        degrees = dict(G.degree())\n",
        "        counter = Counter(degrees.values())\n",
        "        avg_deg = sum(degrees.values()) / G.number_of_nodes() if G.number_of_nodes() else 0\n",
        "\n",
        "        return {\n",
        "            \"Deg_1\": counter.get(1, 0),\n",
        "            \"Deg_2\": counter.get(2, 0),\n",
        "            \"Deg_3\": counter.get(3, 0),\n",
        "            \"Avg_Degree\": round(avg_deg, 2),\n",
        "            \"Density\": round(nx.density(G), 6),\n",
        "            \"Sparsity\": round(1 - nx.density(G), 6)\n",
        "        }\n",
        "\n",
        "def process_file(file_path, label):\n",
        "    if os.path.isfile(file_path) and file_path.endswith(('.csv', '.txt')):\n",
        "        metrics = analyze_graph_metrics(file_path)\n",
        "        if metrics:\n",
        "            metrics['Dataset'] = label\n",
        "            return metrics\n",
        "    return None\n",
        "\n",
        "def analyze_all_datasets(all_dirs):\n",
        "    results = []\n",
        "    for base_dir in all_dirs:\n",
        "\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            dataset_name = os.path.basename(root)\n",
        "            for file in files:\n",
        "                path = os.path.join(root, file)\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                label_type = \"CSV\" if ext == '.csv' else \"TXT\"\n",
        "                label = f\"{dataset_name}_{os.path.splitext(file)[0]}\"\n",
        "                result = process_file(path, label)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)[[\"Dataset\", \"Deg_1\", \"Deg_2\", \"Deg_3\", \"Avg_Degree\", \"Density\", \"Sparsity\"]]\n",
        "\n",
        "# مقایسه‌ی ساختار مجموعه داده‌های فارسی و انگلیسی\n",
        "all_dirs = [ \"/content/Data_InductiveLinkPrediction\",\n",
        "             \"/content/PersianILP-trainTest\"]\n",
        "\n",
        "df_result = analyze_all_datasets(all_dirs).sort_values(\"Dataset\")\n",
        "print(tabulate(df_result, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beH7-u5KqyOq"
      },
      "source": [
        "## 2-Inductive Link Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtIzmhRrcr4"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgNVh6Q6rM6K",
        "outputId": "672d500e-c86b-4119-bff8-2c83911be19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ILP_Date_Zip_File = '/content/drive/MyDrive/DataSet/ILPDataSet.zip'\n",
        "!unzip -q {ILP_Date_Zip_File} -d {'/content'}\n",
        "\n",
        "datasets = sorted([folder for folder in os.listdir('/content') if os.path.isdir(os.path.join('/content', folder))])\n",
        "def create_dataset_dict(base_dir: str = '/content'):\n",
        "    datasets = {}\n",
        "    for dataset_name in os.listdir(base_dir):\n",
        "        dataset_path = os.path.join(base_dir, dataset_name)\n",
        "        if os.path.isdir(dataset_path):\n",
        "            dataset_files = {\n",
        "                \"train\": None,\n",
        "                \"valid\": None,\n",
        "                \"test\": None}\n",
        "\n",
        "            # Check for both .txt and .csv files\n",
        "            for split in dataset_files.keys():\n",
        "                txt_path = os.path.join(dataset_path, f\"{split}.txt\")\n",
        "                csv_path = os.path.join(dataset_path, f\"{split}.csv\")\n",
        "\n",
        "                if os.path.exists(txt_path):\n",
        "                    dataset_files[split] = txt_path\n",
        "                elif os.path.exists(csv_path):\n",
        "                    dataset_files[split] = csv_path\n",
        "\n",
        "            datasets[dataset_name] = dataset_files\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Save Path Dictionay\n",
        "ILP_dataset_paths = create_dataset_dict('/content/ILPDataSet')\n",
        "ILP_dataset_paths = dict(sorted(ILP_dataset_paths.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61E1j7rlgi0Q",
        "outputId": "c7a40052-063d-417e-8326-cc4ee5fe27ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| Dataset       |   train_triples |   test_triples |   train_relations |   test_relations |   train_entities |   test_entities |\n",
            "+===============+=================+================+===================+==================+==================+=================+\n",
            "| PersianILP_V1 |            7564 |           1685 |               325 |              136 |             5953 |            1523 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V2 |            7564 |           7055 |               325 |              279 |             5953 |            6167 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| PersianILP_V3 |           11414 |           2821 |               363 |              176 |             8679 |            2519 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v1_ind |            1618 |            188 |                 8 |                6 |              922 |             286 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v2_ind |            4011 |            441 |                10 |                9 |             2757 |             710 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v3_ind |            6327 |            605 |                11 |               10 |             5084 |             975 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| WN18RR_v4_ind |           12334 |           1429 |                 9 |                9 |             7084 |            2270 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v1_ind  |            1993 |            205 |               142 |               68 |             1093 |             301 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v2_ind  |            4145 |            478 |               172 |              107 |             1660 |             562 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v3_ind  |            7406 |            865 |               183 |              128 |             2501 |             981 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| fb237_v4_ind  |           11714 |           1424 |               200 |              166 |             3051 |            1427 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v1_ind   |             833 |            100 |                14 |                7 |              225 |              84 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v2_ind   |            4586 |            476 |                79 |               54 |             2086 |             478 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v3_ind   |            8048 |            809 |               122 |               87 |             3566 |             798 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n",
            "| nell_v4_ind   |            7073 |            731 |                61 |               47 |             2795 |             630 |\n",
            "+---------------+-----------------+----------------+-------------------+------------------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "def analyze_kg_files(base_dir):\n",
        "    \"\"\"تحلیل فایل‌های دانش‌گراف با مدیریت خطاهای بهتر\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for dataset in sorted(os.listdir(base_dir)):\n",
        "        dataset_path = os.path.join(base_dir, dataset)\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue\n",
        "\n",
        "        stats = {'Dataset': dataset}\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for ext in ['.csv', '.txt']:\n",
        "                file_path = os.path.join(dataset_path, f\"{split}{ext}\")\n",
        "                if not os.path.exists(file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # خواندن فایل با تشخیص خودکار فرمت\n",
        "                    try:\n",
        "                        # ابتدا سعی می‌کنیم با هدر بخوانیم\n",
        "                        df = pd.read_csv(file_path)\n",
        "                        # اگر ستون‌های مورد نیاز وجود ندارند، بدون هدر بخوانیم\n",
        "                        if not all(col in df.columns for col in ['head', 'relation', 'tail']):\n",
        "                            df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                           header=None, names=['head', 'relation', 'tail'])\n",
        "                    except:\n",
        "                        # اگر خطا داد، بدون هدر بخوانیم\n",
        "                        df = pd.read_csv(file_path, sep='\\t' if ext == '.txt' else ',',\n",
        "                                       header=None, names=['head', 'relation', 'tail'])\n",
        "\n",
        "                    # محاسبه آمار پایه\n",
        "                    stats.update({\n",
        "                        f'{split}_triples': int(len(df)),\n",
        "                        f'{split}_relations': int(df['relation'].nunique()),\n",
        "                        f'{split}_entities': int(pd.concat([df['head'], df['tail']]).nunique())\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"خطا در پردازش {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        results.append(stats)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def show_stats(base_dir):\n",
        "    \"\"\"نمایش جدول نتایج با فرمت‌بندی صحیح\"\"\"\n",
        "    df = analyze_kg_files(base_dir)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"هیچ دیتاست معتبری یافت نشد\")\n",
        "        return\n",
        "\n",
        "    # انتخاب و مرتب‌سازی ستون‌ها\n",
        "    columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    # حذف ردیف‌های با مقادیر NaN\n",
        "    df = df.dropna(subset=['train_triples'])[columns].sort_values('Dataset')\n",
        "\n",
        "    # نام‌گذاری ستون‌ها\n",
        "    df.columns = [\n",
        "        'Dataset',\n",
        "        'train_triples', 'test_triples',\n",
        "        'train_relations', 'test_relations',\n",
        "        'train_entities', 'test_entities'\n",
        "    ]\n",
        "\n",
        "    print(tabulate(\n",
        "        df,\n",
        "        headers='keys',\n",
        "        tablefmt='grid',\n",
        "        showindex=False,\n",
        "        numalign=\"right\",\n",
        "        floatfmt=\".0f\"  # نمایش اعداد به صورت صحیح\n",
        "    ))\n",
        "\n",
        "# اجرای نمونه\n",
        "show_stats(\"/content/ILPDataSet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reb0FyYEri_X"
      },
      "source": [
        "**Create DGL Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVMYZesrrk3m"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "class PersianDGLDataset(DGLDataset):\n",
        "    def __init__(self, train_file, test_file, seed=42):\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.seed = seed\n",
        "        self.process()\n",
        "        super().__init__(name=\"PersianLinkPrediction\")\n",
        "\n",
        "    def process(self):\n",
        "        # Initialize mappings\n",
        "        self.entity2id = {}\n",
        "        self.relation2id = {}\n",
        "        ent_id, rel_id = 0, 0\n",
        "\n",
        "        # Process training data\n",
        "        train_triples = self._load_and_process_file(self.train_file, ent_id, rel_id)\n",
        "        ent_id, rel_id = len(self.entity2id), len(self.relation2id)\n",
        "\n",
        "        # Process test data (using same mappings)\n",
        "        test_triples = self._load_and_process_file(self.test_file, ent_id, rel_id)\n",
        "\n",
        "        # Build graphs\n",
        "        self.graphs = {\n",
        "            \"train\": self._build_graph(train_triples),\n",
        "            \"test\": self._build_graph(test_triples)\n",
        "        }\n",
        "\n",
        "    def _load_file(self, file_path):\n",
        "        \"\"\"Load file based on its extension\"\"\"\n",
        "        if file_path.endswith('.csv'):\n",
        "            return pd.read_csv(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            return pd.read_csv(file_path, sep='\\t', header=None,\n",
        "                             names=['subjectLabel', 'predicateLabel', 'objectLabel'])\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Only .csv and .txt files are supported.\")\n",
        "\n",
        "    def _load_and_process_file(self, file_path, ent_id_start, rel_id_start):\n",
        "        \"\"\"Load and process a single file, updating mappings\"\"\"\n",
        "        triples = []\n",
        "        df = self._load_file(file_path)\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            h, r, t = row['subjectLabel'], row['predicateLabel'], row['objectLabel']\n",
        "\n",
        "            # Update entity mappings\n",
        "            for ent in [h, t]:\n",
        "                if ent not in self.entity2id:\n",
        "                    self.entity2id[ent] = ent_id_start\n",
        "                    ent_id_start += 1\n",
        "\n",
        "            # Update relation mappings\n",
        "            if r not in self.relation2id:\n",
        "                self.relation2id[r] = rel_id_start\n",
        "                rel_id_start += 1\n",
        "\n",
        "            triples.append((\n",
        "                self.entity2id[h],\n",
        "                self.relation2id[r],\n",
        "                self.entity2id[t]))\n",
        "\n",
        "        return triples\n",
        "\n",
        "    def _build_graph(self, triples):\n",
        "        \"\"\"Build DGL graph from triples\"\"\"\n",
        "        src, rel, dst = zip(*triples)\n",
        "        src = torch.tensor(src)\n",
        "        dst = torch.tensor(dst)\n",
        "        rel = torch.tensor(rel)\n",
        "\n",
        "        g = dgl.graph((src, dst), num_nodes=len(self.entity2id))\n",
        "        g.edata[\"e_type\"] = rel\n",
        "        g.edata[\"edge_mask\"] = torch.ones(g.num_edges(), dtype=torch.bool)\n",
        "        g.ndata[\"ntype\"] = torch.zeros(g.num_nodes(), dtype=torch.int)\n",
        "        g.ndata[\"feat\"] = torch.randn(g.num_nodes(), 64)\n",
        "        return g\n",
        "\n",
        "    def __getitem__(self, split):\n",
        "        return self.graphs[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "class GraphBatchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, graphs, pos_graphs, neg_graphs):\n",
        "        self.graphs = graphs\n",
        "        self.pos_graphs = pos_graphs\n",
        "        self.neg_graphs = neg_graphs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"graph\": self.graphs[idx],\n",
        "            \"pos_graph\": self.pos_graphs[idx],\n",
        "            \"neg_graph\": self.neg_graphs[idx]}\n",
        "\n",
        "\n",
        "dataset = PersianDGLDataset(train_file = ILP_dataset_paths['PersianILP_V1']['train'],\n",
        "                            test_file = ILP_dataset_paths['PersianILP_V1']['test'])\n",
        "train_g = dataset[\"train\"]\n",
        "test_g = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExoK1-UFrpiR"
      },
      "source": [
        "**Generate Positive Graph And Negative Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsFU2PjVrs5_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import dgl\n",
        "import scipy.sparse as sp\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "\n",
        "class GraphNegativeSampler:\n",
        "    def __init__(self, train_graph, test_graph, train_neg_ratio=1.0, test_neg_ratio=1.0):\n",
        "        self.train_graph = train_graph\n",
        "        self.test_graph = test_graph\n",
        "        self.train_neg_ratio = train_neg_ratio\n",
        "        self.test_neg_ratio = test_neg_ratio\n",
        "        self.train_pos_g, self.train_neg_g = self._prepare_graphs(train_graph, train_neg_ratio)\n",
        "        self.test_pos_g, self.test_neg_g = self._prepare_graphs(test_graph, test_neg_ratio)\n",
        "\n",
        "    def _generate_negative_samples(self, graph):\n",
        "        u, v = graph.edges()\n",
        "        adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())),\n",
        "                          shape=(graph.num_nodes(), graph.num_nodes()))\n",
        "        return np.where(1 - adj.todense() - np.eye(graph.num_nodes()) != 0)\n",
        "\n",
        "    def _prepare_graphs(self, graph, ratio):\n",
        "        return ( self._create_positive_graph(graph),\n",
        "                 self._create_negative_graph(graph, ratio))\n",
        "\n",
        "    def _create_positive_graph(self, graph):\n",
        "        g = dgl.graph(graph.edges(), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = graph.edata[\"e_type\"]\n",
        "        g.ndata.update({k: graph.ndata[k] for k in [\"feat\", \"ntype\"]})\n",
        "        return g\n",
        "\n",
        "    def _create_negative_graph(self, graph, ratio):\n",
        "        neg_u, neg_v = self._generate_negative_samples(graph)\n",
        "        num_samples = int(graph.num_edges() * ratio)\n",
        "        replace = len(neg_u) < num_samples\n",
        "        sample_ids = np.random.choice(len(neg_u), num_samples, replace=replace)\n",
        "\n",
        "        g = dgl.graph((neg_u[sample_ids], neg_v[sample_ids]), num_nodes=graph.num_nodes())\n",
        "        g.edata[\"e_type\"] = torch.randint(0, graph.edata[\"e_type\"].max().item()+1, (g.num_edges(),))\n",
        "        g.ndata.update({\n",
        "            \"feat\": graph.ndata[\"feat\"],\n",
        "            \"ntype\": torch.ones(graph.num_nodes(), dtype=torch.int)})\n",
        "        return g\n",
        "\n",
        "    @property\n",
        "    def training_graphs(self):\n",
        "        return self.train_pos_g, self.train_neg_g\n",
        "\n",
        "    @property\n",
        "    def test_graphs(self):\n",
        "        return self.test_pos_g, self.test_neg_g\n",
        "\n",
        "# Sampling From Knowladge Graph\n",
        "sampler = GraphNegativeSampler(dataset['train'],\n",
        "                               dataset['test'],\n",
        "                               train_neg_ratio=1,\n",
        "                               test_neg_ratio=1)\n",
        "\n",
        "train_pos, train_neg = sampler.training_graphs\n",
        "test_pos, test_neg = sampler.test_graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGioSEvKr24x"
      },
      "source": [
        "**Link Prediction Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZffOokcr50c"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedGraphSAGE(nn.Module):\n",
        "  def __init__(self, in_feats, h_feats, out_feats, dropout=0.5):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
        "        self.conv2 = SAGEConv(h_feats, out_feats, \"mean\")\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata[\"h\"] = h\n",
        "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
        "            return g.edata[\"score\"][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRW_iO3fr7Ek"
      },
      "source": [
        "**Train method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN25QhydsCYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import dgl\n",
        "\n",
        "def train_model(model,\n",
        "                pred,\n",
        "                dataloader,\n",
        "                epochs,\n",
        "                lr=0.01):\n",
        "\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(),\n",
        "                                                 pred.parameters()),\n",
        "                                                 lr=lr)\n",
        "\n",
        "    all_losses = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch_graph = batch[\"graph\"]    # گراف اصلی\n",
        "            pos_graph = batch[\"pos_graph\"]  # گراف مثبت\n",
        "            neg_graph = batch[\"neg_graph\"]  # گراف منفی\n",
        "\n",
        "            # Forward pass\n",
        "            h = model(batch_graph, batch_graph.ndata[\"feat\"])\n",
        "            pos_score = pred(pos_graph, h)\n",
        "            neg_score = pred(neg_graph, h)\n",
        "            loss = compute_loss(pos_score,neg_score)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            e = epoch\n",
        "            loss = epoch_loss\n",
        "\n",
        "        all_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"\\nEpoch: {e}, Loss: {loss:.4f}\")\n",
        "    return h, all_losses\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxC8E0sWsGhm"
      },
      "source": [
        "**Train And Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_Mp6D-8sPxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from tabulate import tabulate\n",
        "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
        "from sklearn import metrics\n",
        "\n",
        "def train_and_evaluate_model(result:dict,\n",
        "                             dataset_name,\n",
        "                             ILP_dataset_paths,\n",
        "                             h_feats=16,\n",
        "                             out_feats=10,\n",
        "                             dropout=0.5,\n",
        "                             epochs=2000,\n",
        "                             lr=0.001,\n",
        "                             train_neg_ratio=10,\n",
        "                             test_neg_ratio=1):\n",
        "\n",
        "    # ===== Step 1: Dataset Preparation =====\n",
        "    graphs = PersianDGLDataset(\n",
        "        train_file=ILP_dataset_paths[dataset_name]['train'],\n",
        "        test_file=ILP_dataset_paths[dataset_name]['test']\n",
        "    )\n",
        "\n",
        "    sampler = GraphNegativeSampler(\n",
        "        graphs['train'], graphs['test'],\n",
        "        train_neg_ratio=train_neg_ratio,\n",
        "        test_neg_ratio=test_neg_ratio\n",
        "    )\n",
        "\n",
        "    train_pos_g, train_neg_g = sampler.training_graphs\n",
        "    test_pos_g, test_neg_g = sampler.test_graphs\n",
        "\n",
        "    train_dataset = GraphBatchDataset([graphs['train']], [train_pos_g], [train_neg_g])\n",
        "    train_loader = GraphDataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    test_dataset = GraphBatchDataset([graphs['test']], [test_pos_g], [test_neg_g])\n",
        "    test_loader = GraphDataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0])\n",
        "\n",
        "    # ===== Step 2: Training =====\n",
        "    def compute_loss(pos_score, neg_score):\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([\n",
        "            torch.ones(pos_score.shape[0]),\n",
        "            torch.zeros(neg_score.shape[0])\n",
        "        ])\n",
        "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "    in_feats = graphs['train'].ndata['feat'].shape[1]\n",
        "    model = ImprovedGraphSAGE(\n",
        "        in_feats=in_feats,\n",
        "        h_feats=h_feats,\n",
        "        out_feats=out_feats,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    pred = DotPredictor()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        itertools.chain(model.parameters(), pred.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch in train_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            pos_score = pred(batch['pos_graph'], h)\n",
        "            neg_score = pred(batch['neg_graph'], h)\n",
        "            loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ===== Step 3: Evaluation =====\n",
        "    pos_scores, pos_labels = [], []\n",
        "    neg_scores, neg_labels = [], []\n",
        "    hit1_list, hit3_list, hit10_list = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ranks = []\n",
        "        for batch in test_loader:\n",
        "            h = model(batch['graph'], batch['graph'].ndata['feat'])\n",
        "            score_pos = pred(batch['pos_graph'], h).squeeze()\n",
        "            score_neg = pred(batch['neg_graph'], h).squeeze()\n",
        "\n",
        "            neg_per_pos = len(score_neg) // len(score_pos)\n",
        "            pos_scores += score_pos.tolist() if score_pos.dim() > 0 else [score_pos.item()]\n",
        "            neg_scores += score_neg.tolist() if score_neg.dim() > 0 else [score_neg.item()]\n",
        "            pos_labels += [1] * len(score_pos) if score_pos.dim() > 0 else [1]\n",
        "            neg_labels += [0] * len(score_neg) if score_neg.dim() > 0 else [0]\n",
        "\n",
        "            score_pos_exp = score_pos.view(-1, 1).repeat(1, neg_per_pos).view(-1)\n",
        "            scores = torch.stack([score_pos_exp, score_neg], dim=1)\n",
        "            scores = torch.softmax(scores, dim=1).cpu().numpy()\n",
        "            rank = np.argwhere(np.argsort(scores, axis=1)[:, ::-1] == 0)[:, 1] + 1\n",
        "\n",
        "            ranks += rank.tolist()\n",
        "            hit1_list += [1 if r <= 1 else 0 for r in rank]\n",
        "            hit3_list += [1 if r <= 3 else 0 for r in rank]\n",
        "            hit10_list += [1 if r <= 10 else 0 for r in rank]\n",
        "\n",
        "    # ===== Step 4: Result Metrics =====\n",
        "    result[dataset_name] = {\n",
        "        \"AUC\": metrics.roc_auc_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"AUC_PR\": metrics.average_precision_score(pos_labels + neg_labels, pos_scores + neg_scores),\n",
        "        \"MRR\": np.mean(1.0 / np.array(ranks)).item(),\n",
        "        \"Hit1\": np.mean(hit1_list),\n",
        "        \"Hit3\": np.mean(hit3_list),\n",
        "        \"Hit10\": np.mean(hit10_list)}\n",
        "\n",
        "    return result\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from tabulate import tabulate\n",
        "def display_results_table(result_dict):\n",
        "    clear_output()\n",
        "    headers = ['Dataset', 'AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']\n",
        "    rows = []\n",
        "    for name, metrics in result_dict.items():\n",
        "        row = [name] + [metrics[h] for h in headers[1:]]\n",
        "        rows.append(row)\n",
        "    print(\"\\n\" + tabulate(rows,\n",
        "                          headers=headers,\n",
        "                          tablefmt=\"fancy_grid\",\n",
        "                          floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdCPemgqsWKU",
        "outputId": "46678f34-73a5-4436-d67b-b41458e7215c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "╒═══════════════╤══════════════════╤═════════════════════╤══════════════════╤═══════════════════╤═══════════════════╤════════════════════╕\n",
            "│ Dataset       │ AUC (mean±std)   │ AUC_PR (mean±std)   │ MRR (mean±std)   │ Hit1 (mean±std)   │ Hit3 (mean±std)   │ Hit10 (mean±std)   │\n",
            "╞═══════════════╪══════════════════╪═════════════════════╪══════════════════╪═══════════════════╪═══════════════════╪════════════════════╡\n",
            "│ PersianILP_V1 │ 0.7184±0.0217    │ 0.7557±0.0181       │ 0.8578±0.0107    │ 0.7156±0.0214     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP_V2 │ 0.7122±0.0071    │ 0.7064±0.0068       │ 0.8566±0.0041    │ 0.7132±0.0082     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP_V3 │ 0.6727±0.0138    │ 0.6995±0.0140       │ 0.8359±0.0067    │ 0.6719±0.0134     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v1_ind │ 0.8270±0.0234    │ 0.8527±0.0199       │ 0.9166±0.0114    │ 0.8332±0.0228     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v2_ind │ 0.8215±0.0214    │ 0.8499±0.0177       │ 0.9123±0.0113    │ 0.8246±0.0225     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v3_ind │ 0.7828±0.0280    │ 0.8212±0.0224       │ 0.8911±0.0149    │ 0.7821±0.0298     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v4_ind │ 0.8320±0.0193    │ 0.8552±0.0162       │ 0.9160±0.0113    │ 0.8319±0.0225     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v1_ind  │ 0.7766±0.0258    │ 0.8075±0.0259       │ 0.8907±0.0153    │ 0.7815±0.0306     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v2_ind  │ 0.7620±0.0278    │ 0.7994±0.0231       │ 0.8802±0.0166    │ 0.7604±0.0332     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v3_ind  │ 0.7365±0.0258    │ 0.7788±0.0217       │ 0.8696±0.0141    │ 0.7391±0.0282     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v4_ind  │ 0.7212±0.0187    │ 0.7598±0.0167       │ 0.8612±0.0101    │ 0.7223±0.0201     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v1_ind   │ 0.6486±0.0903    │ 0.6626±0.0968       │ 0.8245±0.0470    │ 0.6490±0.0939     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v2_ind   │ 0.7278±0.0306    │ 0.7866±0.0262       │ 0.8639±0.0165    │ 0.7277±0.0329     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v3_ind   │ 0.7256±0.0183    │ 0.7877±0.0163       │ 0.8620±0.0098    │ 0.7240±0.0196     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v4_ind   │ 0.7327±0.0238    │ 0.7990±0.0195       │ 0.8655±0.0134    │ 0.7310±0.0269     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "╘═══════════════╧══════════════════╧═════════════════════╧══════════════════╧═══════════════════╧═══════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "            result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=1,\n",
        "            test_neg_ratio=1)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (mean±std)',\n",
        "    'AUC_PR (mean±std)',\n",
        "    'MRR (mean±std)',\n",
        "    'Hit1 (mean±std)',\n",
        "    'Hit3 (mean±std)',\n",
        "    'Hit10 (mean±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a1xcYDfssbb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8beb173-abf5-48a6-cc23-36e3ee32bd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Results After 10 Runs:\n",
            "╒═══════════════╤══════════════════╤═════════════════════╤══════════════════╤═══════════════════╤═══════════════════╤════════════════════╕\n",
            "│ Dataset       │ AUC (mean±std)   │ AUC_PR (mean±std)   │ MRR (mean±std)   │ Hit1 (mean±std)   │ Hit3 (mean±std)   │ Hit10 (mean±std)   │\n",
            "╞═══════════════╪══════════════════╪═════════════════════╪══════════════════╪═══════════════════╪═══════════════════╪════════════════════╡\n",
            "│ PersianILP_V1 │ 0.5608±0.0251    │ 0.0608±0.0078       │ 0.7804±0.0125    │ 0.5608±0.0251     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP_V2 │ 0.5773±0.0118    │ 0.0267±0.0018       │ 0.7887±0.0059    │ 0.5774±0.0118     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ PersianILP_V3 │ 0.5356±0.0170    │ 0.0338±0.0028       │ 0.7678±0.0084    │ 0.5356±0.0168     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v1_ind │ 0.5943±0.0380    │ 0.0731±0.0178       │ 0.7968±0.0194    │ 0.5937±0.0388     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v2_ind │ 0.6250±0.0429    │ 0.0991±0.0210       │ 0.8124±0.0214    │ 0.6249±0.0427     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v3_ind │ 0.5589±0.0350    │ 0.0680±0.0146       │ 0.7794±0.0173    │ 0.5588±0.0346     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ WN18RR_v4_ind │ 0.6098±0.0242    │ 0.0773±0.0102       │ 0.8048±0.0121    │ 0.6096±0.0243     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v1_ind  │ 0.5752±0.0350    │ 0.0597±0.0172       │ 0.7878±0.0173    │ 0.5756±0.0346     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v2_ind  │ 0.5631±0.0364    │ 0.0613±0.0164       │ 0.7817±0.0183    │ 0.5633±0.0366     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v3_ind  │ 0.5504±0.0222    │ 0.0488±0.0056       │ 0.7752±0.0114    │ 0.5505±0.0227     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ fb237_v4_ind  │ 0.5343±0.0230    │ 0.0399±0.0053       │ 0.7671±0.0115    │ 0.5343±0.0230     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v1_ind   │ 0.4788±0.0904    │ 0.0227±0.0083       │ 0.7393±0.0456    │ 0.4785±0.0912     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v2_ind   │ 0.5505±0.0232    │ 0.0689±0.0128       │ 0.7753±0.0114    │ 0.5506±0.0229     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v3_ind   │ 0.5369±0.0350    │ 0.0654±0.0171       │ 0.7684±0.0175    │ 0.5368±0.0350     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "├───────────────┼──────────────────┼─────────────────────┼──────────────────┼───────────────────┼───────────────────┼────────────────────┤\n",
            "│ nell_v4_ind   │ 0.5599±0.0291    │ 0.0756±0.0159       │ 0.7801±0.0147    │ 0.5602±0.0294     │ 1.0000±0.0000     │ 1.0000±0.0000      │\n",
            "╘═══════════════╧══════════════════╧═════════════════════╧══════════════════╧═══════════════════╧═══════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "num_runs = 20\n",
        "\n",
        "for run in range(num_runs):\n",
        "    clear_output()\n",
        "    print(f\"\\nRun {run + 1}/{num_runs}\")\n",
        "    result = {}\n",
        "\n",
        "    for name, path in ILP_dataset_paths.items():\n",
        "        result = train_and_evaluate_model(\n",
        "            result,\n",
        "            name,\n",
        "            ILP_dataset_paths,\n",
        "            h_feats=32,\n",
        "            out_feats=8,\n",
        "            dropout=0.5,\n",
        "            epochs=2000,\n",
        "            lr=0.001,\n",
        "            train_neg_ratio=10,\n",
        "            test_neg_ratio=50)\n",
        "\n",
        "    # Store results for this run\n",
        "    for dataset_name, result_metrics in result.items():\n",
        "        all_results[dataset_name].append(result_metrics)\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for dataset_name, runs in all_results.items():\n",
        "    result_metrics = runs[0].keys()  # Get metric names\n",
        "    dataset_stats = {}\n",
        "    for metric in result_metrics:\n",
        "        values = [run[metric] for run in runs]\n",
        "        dataset_stats[f\"{metric}_mean\"] = np.mean(values)\n",
        "        dataset_stats[f\"{metric}_std\"] = np.std(values)\n",
        "\n",
        "    final_results[dataset_name] = dataset_stats\n",
        "\n",
        "# Display final results\n",
        "clear_output()\n",
        "headers = [\n",
        "    'Dataset',\n",
        "    'AUC (mean±std)',\n",
        "    'AUC_PR (mean±std)',\n",
        "    'MRR (mean±std)',\n",
        "    'Hit1 (mean±std)',\n",
        "    'Hit3 (mean±std)',\n",
        "    'Hit10 (mean±std)'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, result_metrics in final_results.items():\n",
        "    row = [name]\n",
        "    for metric in ['AUC', 'AUC_PR', 'MRR', 'Hit1', 'Hit3', 'Hit10']:\n",
        "        mean = result_metrics[f\"{metric}_mean\"]\n",
        "        std = result_metrics[f\"{metric}_std\"]\n",
        "        row.append(f\"{mean:.4f}±{std:.4f}\")\n",
        "    rows.append(row)\n",
        "\n",
        "print(\"\\nFinal Results After 20 Runs:\")\n",
        "print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOzNcv9wjzDwNWmBlt4IYI"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}